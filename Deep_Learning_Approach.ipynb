{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating data using deep learning\n",
    "\n",
    "Note that for this approach we based ourselves on the Standford lecture notes on convolutional neural networks for visual recognition.\n",
    "\n",
    "This lecture is open source and can be found on http://cs231n.github.io/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "# Clean up the memory\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data/\"\n",
    "SESSION_FOLDER = \"session/\"\n",
    "\n",
    "TRAIN_SET_PERC = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_session(filename, nb_iterations = -1):\n",
    "    '''\n",
    "    Saves a seesion in a file with the given filename\n",
    "    A number of iterations can also be given \n",
    "    :param : String\n",
    "    :param : int\n",
    "    '''\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    if nb_iterations >= 0:\n",
    "        saver.save(sess, SESSION_FOLDER + filename, global_step=nb_iterations)\n",
    "    else:\n",
    "        saver.save(sess, SESSION_FOLDER + filename)\n",
    "        \n",
    "    print(\"Session saved with filename {}.\".format(filename))\n",
    "    \n",
    "def restore_session(filename):\n",
    "    '''\n",
    "    Restores a tensorflow session stored in the given filename\n",
    "    After the call of this function, the sessios's variable will be available again\n",
    "    :param : String\n",
    "    '''\n",
    "    with tf.Session() as sess:\n",
    "        new_saver = tf.train.import_meta_graph(SESSION_FOLDER + filename + '.meta')\n",
    "        new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "        \n",
    "def garbage_collection():\n",
    "    '''\n",
    "    Calls garbage collection to clean unused memory\n",
    "    '''\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred, real, loop = True):\n",
    "    '''\n",
    "    Computes RMSE between predictions and real values\n",
    "    :param : float[]\n",
    "    :param : float[]\n",
    "    :return : float\n",
    "    '''\n",
    "    if len(pred) != len(real):\n",
    "        print(\"RMSE Error : Predictions and real values arrays do not have the same length, aborting.\")\n",
    "        return None\n",
    "    \n",
    "    if loop:\n",
    "        mse = 0\n",
    "        for i in range(len(pred)):\n",
    "            mse += (pred[i] - real[i])**2\n",
    "        return math.sqrt(mse/len(pred))\n",
    "    else:\n",
    "        # The creation of the array may produce memory error\n",
    "        err = pred - real\n",
    "        mse = err.T @ err\n",
    "        return math.sqrt(2 * mse / len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    '''\n",
    "    Plots the history of the training error\n",
    "    Usefull \n",
    "    '''\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error')\n",
    "    plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),\n",
    "           label='Train Loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n",
    "           label = 'Val loss')\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use non normalized feature matrix\n",
    "# For now best results are given with this one\n",
    "#X = np.load(DATA_FOLDER + \"feature_mat_radial_compression.npy\")\n",
    "\n",
    "# Use normalized feature matrix\n",
    "################################################################################################\n",
    "# Careful                                                                                      #\n",
    "# Normally, normalisation should be done one each train/val/test matrices. It is not done here #\n",
    "################################################################################################\n",
    "X = np.load(DATA_FOLDER + \"feature_mat_radial_compression_normalized.npy\")\n",
    "y = np.load(DATA_FOLDER + \"CSD500-r_train-H_total.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test set\n",
    "\n",
    "train_set_size = int(len(X) * TRAIN_SET_PERC)\n",
    "X_train = X[: train_set_size]\n",
    "X_test = X[train_set_size:]\n",
    "y_train = y[: train_set_size]\n",
    "y_test = y[train_set_size :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (30049, 15961)\n",
      "y: (30049,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \" + str(X.shape))\n",
    "print(\"y: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single neural network model approach\n",
    "\n",
    "First we will do a single model approach, the goal is to see quickly how we can build a model using neural networks and how well it does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up as much memory as possible before starting\n",
    "garbage_collection()\n",
    "\n",
    "# Prepare model \n",
    "model = tf.keras.Sequential([\n",
    "    # Number of layers and neurons doesn't really matter, we need as much as possible.\n",
    "    # We well take care of overfitting with regularizers.\n",
    "    # We chose relu activation (relative usual choice when working on regression)\n",
    "    # We add L2 regularizers on hidden layers to avoid overfitting the data. Threshold should be tuned.\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    # Last layer represent the electromagnetic shielding, our prediction\n",
    "    layers.Dense(1, activation='relu')])\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='mse',\n",
    "              # Note that the accuracy metric is useless but it doesn't compile when trying to use RMSE.\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24339 samples, validate on 2705 samples\n",
      "Epoch 1/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 16.2263 - mean_absolute_error: 2.3938 - val_loss: 6.2262 - val_mean_absolute_error: 1.8358\n",
      "Epoch 2/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 3.2662 - mean_absolute_error: 1.3039 - val_loss: 4.0006 - val_mean_absolute_error: 1.4015\n",
      "Epoch 3/200\n",
      "24339/24339 [==============================] - 31s 1ms/step - loss: 1.8035 - mean_absolute_error: 0.9428 - val_loss: 4.2133 - val_mean_absolute_error: 1.4561\n",
      "Epoch 4/200\n",
      "24339/24339 [==============================] - 31s 1ms/step - loss: 1.7472 - mean_absolute_error: 0.9308 - val_loss: 3.5445 - val_mean_absolute_error: 1.3147\n",
      "Epoch 5/200\n",
      "24339/24339 [==============================] - 31s 1ms/step - loss: 1.4083 - mean_absolute_error: 0.8200 - val_loss: 3.1718 - val_mean_absolute_error: 1.2550\n",
      "Epoch 6/200\n",
      "24339/24339 [==============================] - 31s 1ms/step - loss: 0.9822 - mean_absolute_error: 0.6464 - val_loss: 2.8547 - val_mean_absolute_error: 1.2234\n",
      "Epoch 7/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 0.7532 - mean_absolute_error: 0.5284 - val_loss: 1.6163 - val_mean_absolute_error: 0.8387\n",
      "Epoch 8/200\n",
      "24339/24339 [==============================] - 31s 1ms/step - loss: 0.6380 - mean_absolute_error: 0.4690 - val_loss: 1.7268 - val_mean_absolute_error: 0.9506\n",
      "Epoch 9/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 0.6413 - mean_absolute_error: 0.4633 - val_loss: 1.3230 - val_mean_absolute_error: 0.7415\n",
      "Epoch 10/200\n",
      "24339/24339 [==============================] - 32s 1ms/step - loss: 1.0058 - mean_absolute_error: 0.5477 - val_loss: 1.2332 - val_mean_absolute_error: 0.7133\n",
      "Epoch 11/200\n",
      "24339/24339 [==============================] - 31s 1ms/step - loss: 0.5902 - mean_absolute_error: 0.4079 - val_loss: 1.1017 - val_mean_absolute_error: 0.6778\n",
      "Epoch 12/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 0.6531 - mean_absolute_error: 0.4508 - val_loss: 0.9402 - val_mean_absolute_error: 0.5992\n",
      "Epoch 13/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 0.5837 - mean_absolute_error: 0.3987 - val_loss: 1.1087 - val_mean_absolute_error: 0.6789\n",
      "Epoch 14/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.6055 - mean_absolute_error: 0.4121 - val_loss: 1.1925 - val_mean_absolute_error: 0.6696\n",
      "Epoch 15/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.6412 - mean_absolute_error: 0.4260 - val_loss: 0.9671 - val_mean_absolute_error: 0.5941\n",
      "Epoch 16/200\n",
      "24339/24339 [==============================] - 30s 1ms/step - loss: 0.5682 - mean_absolute_error: 0.3909 - val_loss: 0.9031 - val_mean_absolute_error: 0.5809\n",
      "Epoch 17/200\n",
      "24339/24339 [==============================] - 32s 1ms/step - loss: 0.5491 - mean_absolute_error: 0.3745 - val_loss: 0.8629 - val_mean_absolute_error: 0.5548\n",
      "Epoch 18/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.6261 - mean_absolute_error: 0.3981 - val_loss: 0.8705 - val_mean_absolute_error: 0.5569\n",
      "Epoch 19/200\n",
      "24339/24339 [==============================] - 32s 1ms/step - loss: 0.5072 - mean_absolute_error: 0.3575 - val_loss: 0.8613 - val_mean_absolute_error: 0.5617\n",
      "Epoch 20/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.5018 - mean_absolute_error: 0.3553 - val_loss: 0.7478 - val_mean_absolute_error: 0.5213\n",
      "Epoch 21/200\n",
      "24339/24339 [==============================] - 38s 2ms/step - loss: 0.4505 - mean_absolute_error: 0.3590 - val_loss: 0.7303 - val_mean_absolute_error: 0.5263\n",
      "Epoch 22/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 0.4143 - mean_absolute_error: 0.3378 - val_loss: 0.8341 - val_mean_absolute_error: 0.5996\n",
      "Epoch 23/200\n",
      "24339/24339 [==============================] - 31s 1ms/step - loss: 0.3978 - mean_absolute_error: 0.3332 - val_loss: 0.6541 - val_mean_absolute_error: 0.5093\n",
      "Epoch 24/200\n",
      "24339/24339 [==============================] - 30s 1ms/step - loss: 0.3751 - mean_absolute_error: 0.3251 - val_loss: 0.7203 - val_mean_absolute_error: 0.5365\n",
      "Epoch 25/200\n",
      "24339/24339 [==============================] - 32s 1ms/step - loss: 0.4049 - mean_absolute_error: 0.3355 - val_loss: 0.8044 - val_mean_absolute_error: 0.5754\n",
      "Epoch 26/200\n",
      "24339/24339 [==============================] - 37s 2ms/step - loss: 0.3539 - mean_absolute_error: 0.3089 - val_loss: 0.6578 - val_mean_absolute_error: 0.5158\n",
      "Epoch 27/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.3337 - mean_absolute_error: 0.3106 - val_loss: 0.7845 - val_mean_absolute_error: 0.5752\n",
      "Epoch 28/200\n",
      "24339/24339 [==============================] - 32s 1ms/step - loss: 0.3393 - mean_absolute_error: 0.3081 - val_loss: 0.6603 - val_mean_absolute_error: 0.5195\n",
      "Epoch 29/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.3245 - mean_absolute_error: 0.3094 - val_loss: 0.7355 - val_mean_absolute_error: 0.5460\n",
      "Epoch 30/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.3205 - mean_absolute_error: 0.2984 - val_loss: 0.6860 - val_mean_absolute_error: 0.5557\n",
      "Epoch 31/200\n",
      "24339/24339 [==============================] - 40s 2ms/step - loss: 0.3256 - mean_absolute_error: 0.3047 - val_loss: 0.6606 - val_mean_absolute_error: 0.5174\n",
      "Epoch 32/200\n",
      "24339/24339 [==============================] - 37s 2ms/step - loss: 0.2948 - mean_absolute_error: 0.2879 - val_loss: 0.6690 - val_mean_absolute_error: 0.5211\n",
      "Epoch 33/200\n",
      "24339/24339 [==============================] - 39s 2ms/step - loss: 0.2970 - mean_absolute_error: 0.2843 - val_loss: 0.6379 - val_mean_absolute_error: 0.5156\n",
      "Epoch 34/200\n",
      "24339/24339 [==============================] - 40s 2ms/step - loss: 0.2931 - mean_absolute_error: 0.2907 - val_loss: 0.6759 - val_mean_absolute_error: 0.5334\n",
      "Epoch 35/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.2864 - mean_absolute_error: 0.2872 - val_loss: 0.8213 - val_mean_absolute_error: 0.5945\n",
      "Epoch 36/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 0.3080 - mean_absolute_error: 0.2903 - val_loss: 0.6526 - val_mean_absolute_error: 0.5189\n",
      "Epoch 37/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.2936 - mean_absolute_error: 0.2863 - val_loss: 0.7532 - val_mean_absolute_error: 0.5499\n",
      "Epoch 38/200\n",
      "24339/24339 [==============================] - 29s 1ms/step - loss: 0.2953 - mean_absolute_error: 0.2876 - val_loss: 0.6534 - val_mean_absolute_error: 0.5130\n",
      "Epoch 39/200\n",
      "24339/24339 [==============================] - 36s 1ms/step - loss: 0.3182 - mean_absolute_error: 0.2898 - val_loss: 0.6356 - val_mean_absolute_error: 0.5121\n",
      "Epoch 40/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 0.2631 - mean_absolute_error: 0.2730 - val_loss: 0.6493 - val_mean_absolute_error: 0.5156\n",
      "Epoch 41/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 0.3078 - mean_absolute_error: 0.2886 - val_loss: 0.7755 - val_mean_absolute_error: 0.5807\n",
      "Epoch 42/200\n",
      "24339/24339 [==============================] - 37s 2ms/step - loss: 0.2728 - mean_absolute_error: 0.2687 - val_loss: 0.6954 - val_mean_absolute_error: 0.5628\n",
      "Epoch 43/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.2728 - mean_absolute_error: 0.2744 - val_loss: 0.6116 - val_mean_absolute_error: 0.5099\n",
      "Epoch 44/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.2520 - mean_absolute_error: 0.2633 - val_loss: 0.6216 - val_mean_absolute_error: 0.5055\n",
      "Epoch 45/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.2637 - mean_absolute_error: 0.2696 - val_loss: 0.7737 - val_mean_absolute_error: 0.6197\n",
      "Epoch 46/200\n",
      "24339/24339 [==============================] - 30s 1ms/step - loss: 0.2471 - mean_absolute_error: 0.2690 - val_loss: 0.6230 - val_mean_absolute_error: 0.5164\n",
      "Epoch 47/200\n",
      "24339/24339 [==============================] - 30s 1ms/step - loss: 0.3048 - mean_absolute_error: 0.2866 - val_loss: 0.6951 - val_mean_absolute_error: 0.5434\n",
      "Epoch 48/200\n",
      "24339/24339 [==============================] - 32s 1ms/step - loss: 0.2303 - mean_absolute_error: 0.2493 - val_loss: 0.6046 - val_mean_absolute_error: 0.5190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 0.2332 - mean_absolute_error: 0.2662 - val_loss: 0.6181 - val_mean_absolute_error: 0.5156\n",
      "Epoch 50/200\n",
      "24339/24339 [==============================] - 32s 1ms/step - loss: 0.2462 - mean_absolute_error: 0.2666 - val_loss: 0.5834 - val_mean_absolute_error: 0.4958\n",
      "Epoch 51/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.2626 - mean_absolute_error: 0.2700 - val_loss: 0.6383 - val_mean_absolute_error: 0.5298\n",
      "Epoch 52/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.2387 - mean_absolute_error: 0.2609 - val_loss: 0.6260 - val_mean_absolute_error: 0.5193\n",
      "Epoch 53/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 0.2534 - mean_absolute_error: 0.2679 - val_loss: 0.7373 - val_mean_absolute_error: 0.5672\n",
      "Epoch 54/200\n",
      "24339/24339 [==============================] - 30s 1ms/step - loss: 0.2392 - mean_absolute_error: 0.2541 - val_loss: 0.6180 - val_mean_absolute_error: 0.5061\n",
      "Epoch 55/200\n",
      "24339/24339 [==============================] - 31s 1ms/step - loss: 0.2167 - mean_absolute_error: 0.2547 - val_loss: 0.6057 - val_mean_absolute_error: 0.5170\n",
      "Epoch 56/200\n",
      "24339/24339 [==============================] - 32s 1ms/step - loss: 0.2207 - mean_absolute_error: 0.2554 - val_loss: 0.6085 - val_mean_absolute_error: 0.5143\n",
      "Epoch 57/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.2359 - mean_absolute_error: 0.2661 - val_loss: 0.6436 - val_mean_absolute_error: 0.5293\n",
      "Epoch 58/200\n",
      "24339/24339 [==============================] - 30s 1ms/step - loss: 0.2169 - mean_absolute_error: 0.2506 - val_loss: 0.6402 - val_mean_absolute_error: 0.5248\n",
      "Epoch 59/200\n",
      "24339/24339 [==============================] - 30s 1ms/step - loss: 0.2213 - mean_absolute_error: 0.2618 - val_loss: 0.5848 - val_mean_absolute_error: 0.5069\n",
      "Epoch 60/200\n",
      "24339/24339 [==============================] - 30s 1ms/step - loss: 0.2135 - mean_absolute_error: 0.2528 - val_loss: 0.6369 - val_mean_absolute_error: 0.5199\n",
      "Epoch 61/200\n",
      "24339/24339 [==============================] - 31s 1ms/step - loss: 0.2195 - mean_absolute_error: 0.2571 - val_loss: 0.6041 - val_mean_absolute_error: 0.5037\n",
      "Epoch 62/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.2193 - mean_absolute_error: 0.2581 - val_loss: 0.6079 - val_mean_absolute_error: 0.5177\n",
      "Epoch 63/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 0.2122 - mean_absolute_error: 0.2563 - val_loss: 0.5820 - val_mean_absolute_error: 0.5088\n",
      "Epoch 64/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.2347 - mean_absolute_error: 0.2615 - val_loss: 0.6006 - val_mean_absolute_error: 0.5042\n",
      "Epoch 65/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 0.2219 - mean_absolute_error: 0.2554 - val_loss: 0.5831 - val_mean_absolute_error: 0.5043\n",
      "Epoch 66/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.1959 - mean_absolute_error: 0.2435 - val_loss: 0.5681 - val_mean_absolute_error: 0.5126\n",
      "Epoch 67/200\n",
      "24339/24339 [==============================] - 33s 1ms/step - loss: 0.1909 - mean_absolute_error: 0.2416 - val_loss: 0.6328 - val_mean_absolute_error: 0.5310\n",
      "Epoch 68/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.2136 - mean_absolute_error: 0.2573 - val_loss: 0.5824 - val_mean_absolute_error: 0.5037\n",
      "Epoch 69/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.2052 - mean_absolute_error: 0.2529 - val_loss: 0.5619 - val_mean_absolute_error: 0.5012\n",
      "Epoch 70/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.2023 - mean_absolute_error: 0.2523 - val_loss: 0.5683 - val_mean_absolute_error: 0.4953\n",
      "Epoch 71/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.1965 - mean_absolute_error: 0.2471 - val_loss: 0.5831 - val_mean_absolute_error: 0.5194\n",
      "Epoch 72/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.2034 - mean_absolute_error: 0.2531 - val_loss: 0.6741 - val_mean_absolute_error: 0.5551\n",
      "Epoch 73/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.1982 - mean_absolute_error: 0.2460 - val_loss: 0.6421 - val_mean_absolute_error: 0.5437\n",
      "Epoch 74/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.1968 - mean_absolute_error: 0.2460 - val_loss: 0.5914 - val_mean_absolute_error: 0.5165\n",
      "Epoch 75/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.1990 - mean_absolute_error: 0.2498 - val_loss: 0.5744 - val_mean_absolute_error: 0.5016\n",
      "Epoch 76/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.1803 - mean_absolute_error: 0.2357 - val_loss: 0.5592 - val_mean_absolute_error: 0.4956\n",
      "Epoch 77/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.1927 - mean_absolute_error: 0.2504 - val_loss: 0.5857 - val_mean_absolute_error: 0.5103\n",
      "Epoch 78/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.1916 - mean_absolute_error: 0.2483 - val_loss: 0.5946 - val_mean_absolute_error: 0.5198\n",
      "Epoch 79/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.1980 - mean_absolute_error: 0.2526 - val_loss: 0.6112 - val_mean_absolute_error: 0.5236\n",
      "Epoch 80/200\n",
      "24339/24339 [==============================] - 34s 1ms/step - loss: 0.2006 - mean_absolute_error: 0.2514 - val_loss: 0.5573 - val_mean_absolute_error: 0.4972\n",
      "Epoch 81/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.1844 - mean_absolute_error: 0.2380 - val_loss: 0.5463 - val_mean_absolute_error: 0.4918\n",
      "Epoch 82/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.1821 - mean_absolute_error: 0.2413 - val_loss: 0.5964 - val_mean_absolute_error: 0.5232\n",
      "Epoch 83/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.1865 - mean_absolute_error: 0.2433 - val_loss: 0.5576 - val_mean_absolute_error: 0.4971\n",
      "Epoch 84/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.1820 - mean_absolute_error: 0.2384 - val_loss: 0.5561 - val_mean_absolute_error: 0.5038\n",
      "Epoch 85/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.1853 - mean_absolute_error: 0.2459 - val_loss: 0.6114 - val_mean_absolute_error: 0.5239\n",
      "Epoch 86/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.1956 - mean_absolute_error: 0.2508 - val_loss: 0.5809 - val_mean_absolute_error: 0.5108\n",
      "Epoch 87/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.1939 - mean_absolute_error: 0.2511 - val_loss: 0.5912 - val_mean_absolute_error: 0.5154\n",
      "Epoch 88/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.1784 - mean_absolute_error: 0.2376 - val_loss: 0.5700 - val_mean_absolute_error: 0.5078\n",
      "Epoch 89/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.1898 - mean_absolute_error: 0.2457 - val_loss: 0.5670 - val_mean_absolute_error: 0.5093\n",
      "Epoch 90/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.1803 - mean_absolute_error: 0.2396 - val_loss: 0.5564 - val_mean_absolute_error: 0.5028\n",
      "Epoch 91/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.1761 - mean_absolute_error: 0.2375 - val_loss: 0.5721 - val_mean_absolute_error: 0.5104\n",
      "Epoch 92/200\n",
      "24339/24339 [==============================] - 36s 1ms/step - loss: 0.1812 - mean_absolute_error: 0.2425 - val_loss: 0.5921 - val_mean_absolute_error: 0.5146\n",
      "Epoch 93/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.2036 - mean_absolute_error: 0.2597 - val_loss: 0.5486 - val_mean_absolute_error: 0.4927\n",
      "Epoch 94/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.1754 - mean_absolute_error: 0.2297 - val_loss: 0.5646 - val_mean_absolute_error: 0.4966\n",
      "Epoch 95/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.1738 - mean_absolute_error: 0.2328 - val_loss: 0.5893 - val_mean_absolute_error: 0.5087\n",
      "Epoch 96/200\n",
      "24339/24339 [==============================] - 35s 1ms/step - loss: 0.1862 - mean_absolute_error: 0.2442 - val_loss: 0.6873 - val_mean_absolute_error: 0.5700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/200\n",
      "24339/24339 [==============================] - 31s 1ms/step - loss: 0.1789 - mean_absolute_error: 0.2405 - val_loss: 0.6396 - val_mean_absolute_error: 0.5494\n",
      "Epoch 98/200\n",
      "24339/24339 [==============================] - 31s 1ms/step - loss: 0.1754 - mean_absolute_error: 0.2384 - val_loss: 0.5826 - val_mean_absolute_error: 0.5109\n",
      "Epoch 99/200\n",
      "24339/24339 [==============================] - 31s 1ms/step - loss: 0.1846 - mean_absolute_error: 0.2464 - val_loss: 0.6068 - val_mean_absolute_error: 0.5251\n",
      "Epoch 100/200\n",
      "24339/24339 [==============================] - 31s 1ms/step - loss: 0.1732 - mean_absolute_error: 0.2346 - val_loss: 0.5752 - val_mean_absolute_error: 0.4983\n",
      "Epoch 101/200\n",
      "24339/24339 [==============================] - 31s 1ms/step - loss: 0.1945 - mean_absolute_error: 0.2554 - val_loss: 0.5877 - val_mean_absolute_error: 0.5083\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  4086272   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  65792     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  65792     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  65792     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  65792     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  257       \n",
      "=================================================================\n",
      "Total params: 4,547,073\n",
      "Trainable params: 4,547,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We train the model on our data\n",
    "# Number of epochs the network should run through\n",
    "EPOCHS = 200\n",
    "# Size of the batch for optimization\n",
    "BATCH_SIZE = 32\n",
    "# Set up validation split\n",
    "VALIDATION_SPLIT = 0.1\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "# This will avoid overfitting\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split = VALIDATION_SPLIT, \\\n",
    "          callbacks=[early_stop])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6604633181447641"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.predict(X_test, batch_size=32)\n",
    "rmse(result, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "For the following model :\n",
    "    - 4 hidden layers of 64 neurons - relu activation.\n",
    "    - A result layer of 1 neuron - relu activation.\n",
    "    - Early stopping callback\n",
    "    - Test split of 0.2\n",
    "    - Using AdamOptimizer gives best results.\n",
    "    - No cross validation.\n",
    "    \n",
    "Network size :\n",
    "- $4*64 + 1 = 257$ neurons (biases)\n",
    "- $30049*64 + 64*64 + 64*64 + 64 = 1931392$ weights\n",
    "- Total of $1931649$ learnable parameters (almost 2 millions)\n",
    "\n",
    "With the above, we reached a RMSE of 0.5. But this is without separating the data into train and test set. This can lead to overfitting. Now we need to ensure that we are not overfitting.\n",
    "\n",
    "For this we will test 2 solutions :\n",
    "\n",
    "### Change our implementation to add regulizer that will avoid this overfitting\n",
    "\n",
    "### Cross validation over multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi neural network approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_models(X, y, k):\n",
    "    split = len(X) // k\n",
    "    \n",
    "    # Create subsets of X\n",
    "    X_subsets = []\n",
    "    y_subsets = []\n",
    "    for i in range(k - 1):\n",
    "        X_subsets.append(X[i*split : (i + 1)*split])\n",
    "        y_subsets.append(y[i*split : (i + 1)*split])\n",
    "    X_subsets.append(X[(k-1)*split :])\n",
    "    y_subsets.append(y[(k - 1)*split :])\n",
    "    \n",
    "    # Create models\n",
    "    models = []\n",
    "    for i in range(k):\n",
    "        model = tf.keras.Sequential([\n",
    "        # Assuming each layer represent a link between particules, we begin with 4 layers\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        # Last layer represent the electromagnetic shielding, our prediction\n",
    "        layers.Dense(1, activation='relu')])\n",
    "\n",
    "        model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "                  loss='mse',\n",
    "                  # Note that the accuracy metric is useless but it doesn't compile when trying to use RMSE.\n",
    "                  metrics=['mae'])\n",
    "        \n",
    "        EPOCHS = 200\n",
    "        BATCH_SIZE = 32\n",
    "        VALIDATION_SPLIT = 0.2\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "        \n",
    "        flatten = lambda l: np.array([item for sublist in l for item in sublist])\n",
    "        X_model = flatten(X_subsets[0 : i] + X_subsets[(i + 1) :])\n",
    "        y_model = flatten(y_subsets[0 : i] + y_subsets[(i + 1) :])\n",
    "        print(X_model.shape, y_model.shape)\n",
    "        model.fit(X_model, y_model, \\\n",
    "                  epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split = VALIDATION_SPLIT, \\\n",
    "                  callbacks=[early_stop])\n",
    "        models.append(model)\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_predictions(X_test, models):\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        predictions.append(model.predict(X_test, batch_size=32))\n",
    "    return np.mean(predictions, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20283, 2000) (20283,)\n",
      "Train on 16226 samples, validate on 4057 samples\n",
      "Epoch 1/200\n",
      "16226/16226 [==============================] - 3s 168us/step - loss: 30.0769 - mean_absolute_error: 3.0378 - val_loss: 6.0630 - val_mean_absolute_error: 1.8248\n",
      "Epoch 2/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 2.3231 - mean_absolute_error: 1.1593 - val_loss: 4.8266 - val_mean_absolute_error: 1.6400\n",
      "Epoch 3/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 1.6784 - mean_absolute_error: 0.9793 - val_loss: 4.4746 - val_mean_absolute_error: 1.5627\n",
      "Epoch 4/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 1.4042 - mean_absolute_error: 0.9045 - val_loss: 4.4405 - val_mean_absolute_error: 1.5434\n",
      "Epoch 5/200\n",
      "16226/16226 [==============================] - 1s 68us/step - loss: 1.3661 - mean_absolute_error: 0.8996 - val_loss: 4.7473 - val_mean_absolute_error: 1.6319\n",
      "Epoch 6/200\n",
      "16226/16226 [==============================] - 1s 64us/step - loss: 1.2049 - mean_absolute_error: 0.8436 - val_loss: 3.7568 - val_mean_absolute_error: 1.4764\n",
      "Epoch 7/200\n",
      "16226/16226 [==============================] - 1s 65us/step - loss: 1.1649 - mean_absolute_error: 0.8280 - val_loss: 3.8633 - val_mean_absolute_error: 1.4199\n",
      "Epoch 8/200\n",
      "16226/16226 [==============================] - 1s 66us/step - loss: 0.9657 - mean_absolute_error: 0.7535 - val_loss: 4.0384 - val_mean_absolute_error: 1.5142\n",
      "Epoch 9/200\n",
      "16226/16226 [==============================] - 1s 67us/step - loss: 0.9486 - mean_absolute_error: 0.7492 - val_loss: 3.4781 - val_mean_absolute_error: 1.3999\n",
      "Epoch 10/200\n",
      "16226/16226 [==============================] - 1s 69us/step - loss: 0.9024 - mean_absolute_error: 0.7318 - val_loss: 4.3517 - val_mean_absolute_error: 1.5876\n",
      "Epoch 11/200\n",
      "16226/16226 [==============================] - 1s 66us/step - loss: 1.0758 - mean_absolute_error: 0.7936 - val_loss: 3.0741 - val_mean_absolute_error: 1.2810\n",
      "Epoch 12/200\n",
      "16226/16226 [==============================] - 1s 65us/step - loss: 0.7774 - mean_absolute_error: 0.6747 - val_loss: 3.2516 - val_mean_absolute_error: 1.3110\n",
      "Epoch 13/200\n",
      "16226/16226 [==============================] - 1s 64us/step - loss: 0.7196 - mean_absolute_error: 0.6550 - val_loss: 5.7794 - val_mean_absolute_error: 1.9516\n",
      "Epoch 14/200\n",
      "16226/16226 [==============================] - 1s 65us/step - loss: 0.7421 - mean_absolute_error: 0.6588 - val_loss: 2.8389 - val_mean_absolute_error: 1.2287\n",
      "Epoch 15/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.6090 - mean_absolute_error: 0.5971 - val_loss: 3.2586 - val_mean_absolute_error: 1.3123\n",
      "Epoch 16/200\n",
      "16226/16226 [==============================] - 1s 64us/step - loss: 0.7824 - mean_absolute_error: 0.6743 - val_loss: 3.0997 - val_mean_absolute_error: 1.2862\n",
      "Epoch 17/200\n",
      "16226/16226 [==============================] - 1s 64us/step - loss: 0.5721 - mean_absolute_error: 0.5789 - val_loss: 2.9043 - val_mean_absolute_error: 1.2291\n",
      "Epoch 18/200\n",
      "16226/16226 [==============================] - 1s 65us/step - loss: 0.5926 - mean_absolute_error: 0.5882 - val_loss: 3.0458 - val_mean_absolute_error: 1.2857\n",
      "Epoch 19/200\n",
      "16226/16226 [==============================] - 1s 66us/step - loss: 0.5538 - mean_absolute_error: 0.5733 - val_loss: 2.7272 - val_mean_absolute_error: 1.1935\n",
      "Epoch 20/200\n",
      "16226/16226 [==============================] - 1s 69us/step - loss: 0.6713 - mean_absolute_error: 0.6114 - val_loss: 4.4155 - val_mean_absolute_error: 1.6708\n",
      "Epoch 21/200\n",
      "16226/16226 [==============================] - 1s 69us/step - loss: 0.6078 - mean_absolute_error: 0.5876 - val_loss: 2.4884 - val_mean_absolute_error: 1.1482\n",
      "Epoch 22/200\n",
      "16226/16226 [==============================] - 1s 65us/step - loss: 0.4234 - mean_absolute_error: 0.4970 - val_loss: 2.7615 - val_mean_absolute_error: 1.1893\n",
      "Epoch 23/200\n",
      "16226/16226 [==============================] - 1s 64us/step - loss: 0.4730 - mean_absolute_error: 0.5289 - val_loss: 2.6080 - val_mean_absolute_error: 1.1546\n",
      "Epoch 24/200\n",
      "16226/16226 [==============================] - 1s 65us/step - loss: 0.4647 - mean_absolute_error: 0.5219 - val_loss: 2.7333 - val_mean_absolute_error: 1.2100\n",
      "Epoch 25/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.4481 - mean_absolute_error: 0.5088 - val_loss: 3.4369 - val_mean_absolute_error: 1.4619\n",
      "Epoch 26/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.5189 - mean_absolute_error: 0.5435 - val_loss: 2.7112 - val_mean_absolute_error: 1.1885\n",
      "Epoch 27/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.4172 - mean_absolute_error: 0.4958 - val_loss: 2.3383 - val_mean_absolute_error: 1.0864\n",
      "Epoch 28/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.4263 - mean_absolute_error: 0.4995 - val_loss: 3.2279 - val_mean_absolute_error: 1.3776\n",
      "Epoch 29/200\n",
      "16226/16226 [==============================] - 1s 64us/step - loss: 0.4227 - mean_absolute_error: 0.4970 - val_loss: 2.4776 - val_mean_absolute_error: 1.1314\n",
      "Epoch 30/200\n",
      "16226/16226 [==============================] - 1s 64us/step - loss: 0.3471 - mean_absolute_error: 0.4484 - val_loss: 2.3789 - val_mean_absolute_error: 1.0965\n",
      "Epoch 31/200\n",
      "16226/16226 [==============================] - 1s 66us/step - loss: 0.3397 - mean_absolute_error: 0.4452 - val_loss: 3.0970 - val_mean_absolute_error: 1.2991\n",
      "Epoch 32/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.4197 - mean_absolute_error: 0.4887 - val_loss: 2.3290 - val_mean_absolute_error: 1.0752\n",
      "Epoch 33/200\n",
      "16226/16226 [==============================] - 1s 64us/step - loss: 0.3205 - mean_absolute_error: 0.4340 - val_loss: 2.7493 - val_mean_absolute_error: 1.2177\n",
      "Epoch 34/200\n",
      "16226/16226 [==============================] - 1s 64us/step - loss: 0.3586 - mean_absolute_error: 0.4581 - val_loss: 2.7744 - val_mean_absolute_error: 1.2845\n",
      "Epoch 35/200\n",
      "16226/16226 [==============================] - 1s 64us/step - loss: 0.3310 - mean_absolute_error: 0.4386 - val_loss: 2.2471 - val_mean_absolute_error: 1.0779\n",
      "Epoch 36/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.2817 - mean_absolute_error: 0.4047 - val_loss: 2.3283 - val_mean_absolute_error: 1.1067\n",
      "Epoch 37/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.3134 - mean_absolute_error: 0.4280 - val_loss: 2.2350 - val_mean_absolute_error: 1.0844\n",
      "Epoch 38/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.3237 - mean_absolute_error: 0.4359 - val_loss: 2.1851 - val_mean_absolute_error: 1.0618\n",
      "Epoch 39/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.2985 - mean_absolute_error: 0.4151 - val_loss: 2.3945 - val_mean_absolute_error: 1.1094\n",
      "Epoch 40/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.2660 - mean_absolute_error: 0.3910 - val_loss: 2.1525 - val_mean_absolute_error: 1.0672\n",
      "Epoch 41/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.3310 - mean_absolute_error: 0.4298 - val_loss: 2.2247 - val_mean_absolute_error: 1.0647\n",
      "Epoch 42/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.2527 - mean_absolute_error: 0.3822 - val_loss: 2.0060 - val_mean_absolute_error: 1.0056\n",
      "Epoch 43/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.2594 - mean_absolute_error: 0.3910 - val_loss: 2.3423 - val_mean_absolute_error: 1.1344\n",
      "Epoch 44/200\n",
      "16226/16226 [==============================] - 1s 68us/step - loss: 0.2652 - mean_absolute_error: 0.3894 - val_loss: 2.0698 - val_mean_absolute_error: 1.0388\n",
      "Epoch 45/200\n",
      "16226/16226 [==============================] - 1s 64us/step - loss: 0.2332 - mean_absolute_error: 0.3666 - val_loss: 2.0249 - val_mean_absolute_error: 1.0256\n",
      "Epoch 46/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.3149 - mean_absolute_error: 0.4231 - val_loss: 2.0723 - val_mean_absolute_error: 1.0190\n",
      "Epoch 47/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.2072 - mean_absolute_error: 0.3441 - val_loss: 2.0783 - val_mean_absolute_error: 1.0189\n",
      "Epoch 48/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16226/16226 [==============================] - 1s 64us/step - loss: 0.2303 - mean_absolute_error: 0.3658 - val_loss: 2.1982 - val_mean_absolute_error: 1.0572\n",
      "Epoch 49/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.2609 - mean_absolute_error: 0.3846 - val_loss: 2.0411 - val_mean_absolute_error: 1.0099\n",
      "Epoch 50/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.2206 - mean_absolute_error: 0.3556 - val_loss: 2.0147 - val_mean_absolute_error: 1.0038\n",
      "Epoch 51/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.2194 - mean_absolute_error: 0.3570 - val_loss: 1.9604 - val_mean_absolute_error: 1.0091\n",
      "Epoch 52/200\n",
      "16226/16226 [==============================] - 1s 65us/step - loss: 0.2239 - mean_absolute_error: 0.3569 - val_loss: 2.1639 - val_mean_absolute_error: 1.0726\n",
      "Epoch 53/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.2114 - mean_absolute_error: 0.3502 - val_loss: 2.2018 - val_mean_absolute_error: 1.0767\n",
      "Epoch 54/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.2309 - mean_absolute_error: 0.3674 - val_loss: 1.9444 - val_mean_absolute_error: 0.9731\n",
      "Epoch 55/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.1722 - mean_absolute_error: 0.3136 - val_loss: 1.9177 - val_mean_absolute_error: 0.9649\n",
      "Epoch 56/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.1998 - mean_absolute_error: 0.3394 - val_loss: 1.9196 - val_mean_absolute_error: 0.9658\n",
      "Epoch 57/200\n",
      "16226/16226 [==============================] - 1s 66us/step - loss: 0.1948 - mean_absolute_error: 0.3364 - val_loss: 1.8410 - val_mean_absolute_error: 0.9490\n",
      "Epoch 58/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.2249 - mean_absolute_error: 0.3633 - val_loss: 1.8937 - val_mean_absolute_error: 0.9719\n",
      "Epoch 59/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.1829 - mean_absolute_error: 0.3244 - val_loss: 1.9355 - val_mean_absolute_error: 0.9818\n",
      "Epoch 60/200\n",
      "16226/16226 [==============================] - 1s 67us/step - loss: 0.1890 - mean_absolute_error: 0.3297 - val_loss: 1.8864 - val_mean_absolute_error: 0.9678\n",
      "Epoch 61/200\n",
      "16226/16226 [==============================] - 1s 68us/step - loss: 0.1638 - mean_absolute_error: 0.3079 - val_loss: 1.8796 - val_mean_absolute_error: 0.9760\n",
      "Epoch 62/200\n",
      "16226/16226 [==============================] - 1s 69us/step - loss: 0.1947 - mean_absolute_error: 0.3330 - val_loss: 1.9075 - val_mean_absolute_error: 0.9699\n",
      "Epoch 63/200\n",
      "16226/16226 [==============================] - 1s 65us/step - loss: 0.1573 - mean_absolute_error: 0.2990 - val_loss: 1.8439 - val_mean_absolute_error: 0.9576\n",
      "Epoch 64/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.1990 - mean_absolute_error: 0.3344 - val_loss: 1.7891 - val_mean_absolute_error: 0.9369\n",
      "Epoch 65/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.1600 - mean_absolute_error: 0.3064 - val_loss: 1.9432 - val_mean_absolute_error: 0.9839\n",
      "Epoch 66/200\n",
      "16226/16226 [==============================] - 1s 68us/step - loss: 0.1749 - mean_absolute_error: 0.3147 - val_loss: 1.8080 - val_mean_absolute_error: 0.9455\n",
      "Epoch 67/200\n",
      "16226/16226 [==============================] - 1s 65us/step - loss: 0.1417 - mean_absolute_error: 0.2841 - val_loss: 1.8338 - val_mean_absolute_error: 0.9489\n",
      "Epoch 68/200\n",
      "16226/16226 [==============================] - 1s 66us/step - loss: 0.1492 - mean_absolute_error: 0.2932 - val_loss: 1.7489 - val_mean_absolute_error: 0.9234\n",
      "Epoch 69/200\n",
      "16226/16226 [==============================] - 1s 64us/step - loss: 0.1543 - mean_absolute_error: 0.2973 - val_loss: 1.7466 - val_mean_absolute_error: 0.9295\n",
      "Epoch 70/200\n",
      "16226/16226 [==============================] - 1s 68us/step - loss: 0.1744 - mean_absolute_error: 0.3166 - val_loss: 1.7696 - val_mean_absolute_error: 0.9347\n",
      "Epoch 71/200\n",
      "16226/16226 [==============================] - 1s 66us/step - loss: 0.1473 - mean_absolute_error: 0.2894 - val_loss: 1.8675 - val_mean_absolute_error: 0.9645\n",
      "Epoch 72/200\n",
      "16226/16226 [==============================] - 1s 65us/step - loss: 0.1704 - mean_absolute_error: 0.3100 - val_loss: 1.7671 - val_mean_absolute_error: 0.9272\n",
      "Epoch 73/200\n",
      "16226/16226 [==============================] - 1s 66us/step - loss: 0.1448 - mean_absolute_error: 0.2842 - val_loss: 1.8739 - val_mean_absolute_error: 0.9754\n",
      "Epoch 74/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.1501 - mean_absolute_error: 0.2922 - val_loss: 1.7345 - val_mean_absolute_error: 0.9210\n",
      "Epoch 75/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1448 - mean_absolute_error: 0.2895 - val_loss: 1.7061 - val_mean_absolute_error: 0.9071\n",
      "Epoch 76/200\n",
      "16226/16226 [==============================] - 1s 69us/step - loss: 0.1247 - mean_absolute_error: 0.2676 - val_loss: 1.7789 - val_mean_absolute_error: 0.9344\n",
      "Epoch 77/200\n",
      "16226/16226 [==============================] - 1s 65us/step - loss: 0.1446 - mean_absolute_error: 0.2906 - val_loss: 1.6858 - val_mean_absolute_error: 0.8966\n",
      "Epoch 78/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.1411 - mean_absolute_error: 0.2837 - val_loss: 1.6225 - val_mean_absolute_error: 0.8779\n",
      "Epoch 79/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.1188 - mean_absolute_error: 0.2592 - val_loss: 1.6907 - val_mean_absolute_error: 0.8991\n",
      "Epoch 80/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.1516 - mean_absolute_error: 0.2971 - val_loss: 2.2697 - val_mean_absolute_error: 1.1126\n",
      "Epoch 81/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.1446 - mean_absolute_error: 0.2846 - val_loss: 1.7417 - val_mean_absolute_error: 0.9338\n",
      "Epoch 82/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.1118 - mean_absolute_error: 0.2506 - val_loss: 1.6520 - val_mean_absolute_error: 0.8887\n",
      "Epoch 83/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.1188 - mean_absolute_error: 0.2625 - val_loss: 1.7928 - val_mean_absolute_error: 0.9447\n",
      "Epoch 84/200\n",
      "16226/16226 [==============================] - 1s 63us/step - loss: 0.1400 - mean_absolute_error: 0.2817 - val_loss: 1.7736 - val_mean_absolute_error: 0.9626\n",
      "Epoch 85/200\n",
      "16226/16226 [==============================] - 1s 67us/step - loss: 0.1285 - mean_absolute_error: 0.2657 - val_loss: 1.7984 - val_mean_absolute_error: 0.9539\n",
      "Epoch 86/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.1113 - mean_absolute_error: 0.2504 - val_loss: 1.6629 - val_mean_absolute_error: 0.8907\n",
      "Epoch 87/200\n",
      "16226/16226 [==============================] - 1s 66us/step - loss: 0.1305 - mean_absolute_error: 0.2724 - val_loss: 1.6128 - val_mean_absolute_error: 0.8793\n",
      "Epoch 88/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.1128 - mean_absolute_error: 0.2506 - val_loss: 1.6367 - val_mean_absolute_error: 0.8880\n",
      "Epoch 89/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1052 - mean_absolute_error: 0.2419 - val_loss: 1.6374 - val_mean_absolute_error: 0.8799\n",
      "Epoch 90/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.1168 - mean_absolute_error: 0.2594 - val_loss: 1.6231 - val_mean_absolute_error: 0.8728\n",
      "Epoch 91/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.1247 - mean_absolute_error: 0.2634 - val_loss: 1.5924 - val_mean_absolute_error: 0.8692\n",
      "Epoch 92/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.1071 - mean_absolute_error: 0.2465 - val_loss: 1.5961 - val_mean_absolute_error: 0.8691\n",
      "Epoch 93/200\n",
      "16226/16226 [==============================] - 1s 67us/step - loss: 0.0943 - mean_absolute_error: 0.2303 - val_loss: 1.5896 - val_mean_absolute_error: 0.8599\n",
      "Epoch 94/200\n",
      "16226/16226 [==============================] - 1s 68us/step - loss: 0.1096 - mean_absolute_error: 0.2477 - val_loss: 1.5855 - val_mean_absolute_error: 0.8669\n",
      "Epoch 95/200\n",
      "16226/16226 [==============================] - 1s 67us/step - loss: 0.1031 - mean_absolute_error: 0.2432 - val_loss: 1.6918 - val_mean_absolute_error: 0.9033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/200\n",
      "16226/16226 [==============================] - 1s 69us/step - loss: 0.1073 - mean_absolute_error: 0.2435 - val_loss: 1.6788 - val_mean_absolute_error: 0.9028\n",
      "Epoch 97/200\n",
      "16226/16226 [==============================] - 1s 67us/step - loss: 0.1213 - mean_absolute_error: 0.2559 - val_loss: 1.5806 - val_mean_absolute_error: 0.8689\n",
      "Epoch 98/200\n",
      "16226/16226 [==============================] - 1s 69us/step - loss: 0.1088 - mean_absolute_error: 0.2484 - val_loss: 1.6441 - val_mean_absolute_error: 0.9162\n",
      "Epoch 99/200\n",
      "16226/16226 [==============================] - 1s 69us/step - loss: 0.0958 - mean_absolute_error: 0.2321 - val_loss: 1.5612 - val_mean_absolute_error: 0.8652\n",
      "Epoch 100/200\n",
      "16226/16226 [==============================] - 1s 67us/step - loss: 0.1012 - mean_absolute_error: 0.2363 - val_loss: 1.6797 - val_mean_absolute_error: 0.9168\n",
      "Epoch 101/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.1250 - mean_absolute_error: 0.2626 - val_loss: 1.5289 - val_mean_absolute_error: 0.8554\n",
      "Epoch 102/200\n",
      "16226/16226 [==============================] - 1s 69us/step - loss: 0.0893 - mean_absolute_error: 0.2239 - val_loss: 1.5431 - val_mean_absolute_error: 0.8608\n",
      "Epoch 103/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0877 - mean_absolute_error: 0.2216 - val_loss: 1.5188 - val_mean_absolute_error: 0.8543\n",
      "Epoch 104/200\n",
      "16226/16226 [==============================] - 1s 69us/step - loss: 0.0904 - mean_absolute_error: 0.2262 - val_loss: 1.6415 - val_mean_absolute_error: 0.9038\n",
      "Epoch 105/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0976 - mean_absolute_error: 0.2339 - val_loss: 1.5476 - val_mean_absolute_error: 0.8745\n",
      "Epoch 106/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.0952 - mean_absolute_error: 0.2316 - val_loss: 1.5059 - val_mean_absolute_error: 0.8425\n",
      "Epoch 107/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.1033 - mean_absolute_error: 0.2339 - val_loss: 1.5146 - val_mean_absolute_error: 0.8423\n",
      "Epoch 108/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0845 - mean_absolute_error: 0.2184 - val_loss: 1.5183 - val_mean_absolute_error: 0.8407\n",
      "Epoch 109/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0824 - mean_absolute_error: 0.2113 - val_loss: 1.4757 - val_mean_absolute_error: 0.8346\n",
      "Epoch 110/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0989 - mean_absolute_error: 0.2370 - val_loss: 1.5260 - val_mean_absolute_error: 0.8503\n",
      "Epoch 111/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0892 - mean_absolute_error: 0.2236 - val_loss: 1.5453 - val_mean_absolute_error: 0.8553\n",
      "Epoch 112/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0797 - mean_absolute_error: 0.2081 - val_loss: 1.4524 - val_mean_absolute_error: 0.8257\n",
      "Epoch 113/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0928 - mean_absolute_error: 0.2297 - val_loss: 1.5874 - val_mean_absolute_error: 0.8852\n",
      "Epoch 114/200\n",
      "16226/16226 [==============================] - 1s 85us/step - loss: 0.0822 - mean_absolute_error: 0.2153 - val_loss: 1.5328 - val_mean_absolute_error: 0.8585\n",
      "Epoch 115/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0905 - mean_absolute_error: 0.2252 - val_loss: 1.5081 - val_mean_absolute_error: 0.8479\n",
      "Epoch 116/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0797 - mean_absolute_error: 0.2094 - val_loss: 1.5298 - val_mean_absolute_error: 0.8613\n",
      "Epoch 117/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0788 - mean_absolute_error: 0.2063 - val_loss: 1.4727 - val_mean_absolute_error: 0.8328\n",
      "Epoch 118/200\n",
      "16226/16226 [==============================] - 1s 86us/step - loss: 0.0782 - mean_absolute_error: 0.2080 - val_loss: 1.5342 - val_mean_absolute_error: 0.8723\n",
      "Epoch 119/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0872 - mean_absolute_error: 0.2156 - val_loss: 1.5322 - val_mean_absolute_error: 0.8639\n",
      "Epoch 120/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.0766 - mean_absolute_error: 0.2067 - val_loss: 1.4909 - val_mean_absolute_error: 0.8405\n",
      "Epoch 121/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.0776 - mean_absolute_error: 0.2081 - val_loss: 1.4848 - val_mean_absolute_error: 0.8417\n",
      "Epoch 122/200\n",
      "16226/16226 [==============================] - 1s 67us/step - loss: 0.0729 - mean_absolute_error: 0.2013 - val_loss: 1.6116 - val_mean_absolute_error: 0.9040\n",
      "Epoch 123/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0821 - mean_absolute_error: 0.2146 - val_loss: 1.5003 - val_mean_absolute_error: 0.8465\n",
      "Epoch 124/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0884 - mean_absolute_error: 0.2238 - val_loss: 1.4591 - val_mean_absolute_error: 0.8265\n",
      "Epoch 125/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0702 - mean_absolute_error: 0.1978 - val_loss: 1.4414 - val_mean_absolute_error: 0.8236\n",
      "Epoch 126/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0779 - mean_absolute_error: 0.2061 - val_loss: 1.5488 - val_mean_absolute_error: 0.8734\n",
      "Epoch 127/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0765 - mean_absolute_error: 0.2047 - val_loss: 1.4790 - val_mean_absolute_error: 0.8346\n",
      "Epoch 128/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0673 - mean_absolute_error: 0.1927 - val_loss: 1.4430 - val_mean_absolute_error: 0.8288\n",
      "Epoch 129/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0723 - mean_absolute_error: 0.1999 - val_loss: 1.5013 - val_mean_absolute_error: 0.8426\n",
      "Epoch 130/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0681 - mean_absolute_error: 0.1917 - val_loss: 1.5244 - val_mean_absolute_error: 0.8648\n",
      "Epoch 131/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0674 - mean_absolute_error: 0.1915 - val_loss: 1.4967 - val_mean_absolute_error: 0.8479\n",
      "Epoch 132/200\n",
      "16226/16226 [==============================] - 1s 64us/step - loss: 0.0682 - mean_absolute_error: 0.1946 - val_loss: 1.4518 - val_mean_absolute_error: 0.8215\n",
      "Epoch 133/200\n",
      "16226/16226 [==============================] - 1s 69us/step - loss: 0.0802 - mean_absolute_error: 0.2076 - val_loss: 1.4372 - val_mean_absolute_error: 0.8196\n",
      "Epoch 134/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0587 - mean_absolute_error: 0.1792 - val_loss: 1.5620 - val_mean_absolute_error: 0.8955\n",
      "Epoch 135/200\n",
      "16226/16226 [==============================] - 1s 86us/step - loss: 0.0868 - mean_absolute_error: 0.2120 - val_loss: 1.4467 - val_mean_absolute_error: 0.8199\n",
      "Epoch 136/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0623 - mean_absolute_error: 0.1847 - val_loss: 1.4510 - val_mean_absolute_error: 0.8142\n",
      "Epoch 137/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0608 - mean_absolute_error: 0.1834 - val_loss: 1.5575 - val_mean_absolute_error: 0.8733\n",
      "Epoch 138/200\n",
      "16226/16226 [==============================] - 1s 86us/step - loss: 0.0739 - mean_absolute_error: 0.2023 - val_loss: 1.4277 - val_mean_absolute_error: 0.8184\n",
      "Epoch 139/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0668 - mean_absolute_error: 0.1908 - val_loss: 1.4331 - val_mean_absolute_error: 0.8137\n",
      "Epoch 140/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0587 - mean_absolute_error: 0.1779 - val_loss: 1.4433 - val_mean_absolute_error: 0.8198\n",
      "Epoch 141/200\n",
      "16226/16226 [==============================] - 1s 89us/step - loss: 0.0612 - mean_absolute_error: 0.1837 - val_loss: 1.4702 - val_mean_absolute_error: 0.8275\n",
      "Epoch 142/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0676 - mean_absolute_error: 0.1937 - val_loss: 1.5229 - val_mean_absolute_error: 0.8506\n",
      "Epoch 143/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0662 - mean_absolute_error: 0.1910 - val_loss: 1.4649 - val_mean_absolute_error: 0.8292\n",
      "Epoch 144/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0629 - mean_absolute_error: 0.1868 - val_loss: 1.4607 - val_mean_absolute_error: 0.8385\n",
      "Epoch 145/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0560 - mean_absolute_error: 0.1760 - val_loss: 1.4247 - val_mean_absolute_error: 0.8212\n",
      "Epoch 146/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0598 - mean_absolute_error: 0.1803 - val_loss: 1.4429 - val_mean_absolute_error: 0.8227\n",
      "Epoch 147/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0714 - mean_absolute_error: 0.1980 - val_loss: 1.4001 - val_mean_absolute_error: 0.8157\n",
      "Epoch 148/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0707 - mean_absolute_error: 0.1945 - val_loss: 1.4112 - val_mean_absolute_error: 0.8089\n",
      "Epoch 149/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0600 - mean_absolute_error: 0.1790 - val_loss: 1.4219 - val_mean_absolute_error: 0.8171\n",
      "Epoch 150/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0539 - mean_absolute_error: 0.1717 - val_loss: 1.4345 - val_mean_absolute_error: 0.8197\n",
      "Epoch 151/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0618 - mean_absolute_error: 0.1835 - val_loss: 1.5365 - val_mean_absolute_error: 0.8615\n",
      "Epoch 152/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0679 - mean_absolute_error: 0.1888 - val_loss: 1.4397 - val_mean_absolute_error: 0.8214\n",
      "Epoch 153/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0545 - mean_absolute_error: 0.1739 - val_loss: 1.4185 - val_mean_absolute_error: 0.8093\n",
      "Epoch 154/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0587 - mean_absolute_error: 0.1791 - val_loss: 1.3917 - val_mean_absolute_error: 0.8015\n",
      "Epoch 155/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0529 - mean_absolute_error: 0.1697 - val_loss: 1.4020 - val_mean_absolute_error: 0.8111\n",
      "Epoch 156/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0552 - mean_absolute_error: 0.1732 - val_loss: 1.3916 - val_mean_absolute_error: 0.8016\n",
      "Epoch 157/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0520 - mean_absolute_error: 0.1674 - val_loss: 1.3719 - val_mean_absolute_error: 0.7959\n",
      "Epoch 158/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0526 - mean_absolute_error: 0.1688 - val_loss: 1.3934 - val_mean_absolute_error: 0.8160\n",
      "Epoch 159/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0616 - mean_absolute_error: 0.1828 - val_loss: 1.5194 - val_mean_absolute_error: 0.8867\n",
      "Epoch 160/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0788 - mean_absolute_error: 0.2041 - val_loss: 1.3670 - val_mean_absolute_error: 0.7906\n",
      "Epoch 161/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0465 - mean_absolute_error: 0.1591 - val_loss: 1.4428 - val_mean_absolute_error: 0.8237\n",
      "Epoch 162/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0526 - mean_absolute_error: 0.1692 - val_loss: 1.3754 - val_mean_absolute_error: 0.7963\n",
      "Epoch 163/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0483 - mean_absolute_error: 0.1618 - val_loss: 1.3919 - val_mean_absolute_error: 0.8052\n",
      "Epoch 164/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0468 - mean_absolute_error: 0.1619 - val_loss: 1.3903 - val_mean_absolute_error: 0.8118\n",
      "Epoch 165/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0601 - mean_absolute_error: 0.1766 - val_loss: 1.5505 - val_mean_absolute_error: 0.8895\n",
      "Epoch 166/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0608 - mean_absolute_error: 0.1809 - val_loss: 1.3920 - val_mean_absolute_error: 0.8064\n",
      "Epoch 167/200\n",
      "16226/16226 [==============================] - 1s 85us/step - loss: 0.0438 - mean_absolute_error: 0.1525 - val_loss: 1.4079 - val_mean_absolute_error: 0.8051\n",
      "Epoch 168/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0558 - mean_absolute_error: 0.1765 - val_loss: 1.3634 - val_mean_absolute_error: 0.7891\n",
      "Epoch 169/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0501 - mean_absolute_error: 0.1630 - val_loss: 1.3590 - val_mean_absolute_error: 0.7881\n",
      "Epoch 170/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0484 - mean_absolute_error: 0.1643 - val_loss: 1.3362 - val_mean_absolute_error: 0.7817\n",
      "Epoch 171/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0511 - mean_absolute_error: 0.1672 - val_loss: 1.3571 - val_mean_absolute_error: 0.7914\n",
      "Epoch 172/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0491 - mean_absolute_error: 0.1611 - val_loss: 1.3761 - val_mean_absolute_error: 0.8001\n",
      "Epoch 173/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0512 - mean_absolute_error: 0.1629 - val_loss: 1.3851 - val_mean_absolute_error: 0.8140\n",
      "Epoch 174/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0455 - mean_absolute_error: 0.1582 - val_loss: 1.3685 - val_mean_absolute_error: 0.7887\n",
      "Epoch 175/200\n",
      "16226/16226 [==============================] - 1s 70us/step - loss: 0.0516 - mean_absolute_error: 0.1679 - val_loss: 1.3578 - val_mean_absolute_error: 0.7863\n",
      "Epoch 176/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0494 - mean_absolute_error: 0.1627 - val_loss: 1.4254 - val_mean_absolute_error: 0.8210\n",
      "Epoch 177/200\n",
      "16226/16226 [==============================] - 1s 70us/step - loss: 0.0459 - mean_absolute_error: 0.1569 - val_loss: 1.3604 - val_mean_absolute_error: 0.7905\n",
      "Epoch 178/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0459 - mean_absolute_error: 0.1555 - val_loss: 1.3796 - val_mean_absolute_error: 0.8164\n",
      "Epoch 179/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0542 - mean_absolute_error: 0.1679 - val_loss: 1.3415 - val_mean_absolute_error: 0.7876\n",
      "Epoch 180/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0401 - mean_absolute_error: 0.1456 - val_loss: 1.3357 - val_mean_absolute_error: 0.7802\n",
      "Epoch 181/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0432 - mean_absolute_error: 0.1541 - val_loss: 1.3305 - val_mean_absolute_error: 0.7879\n",
      "Epoch 182/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0437 - mean_absolute_error: 0.1536 - val_loss: 1.4013 - val_mean_absolute_error: 0.8212\n",
      "Epoch 183/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0495 - mean_absolute_error: 0.1639 - val_loss: 1.3533 - val_mean_absolute_error: 0.7874\n",
      "Epoch 184/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0432 - mean_absolute_error: 0.1529 - val_loss: 1.3423 - val_mean_absolute_error: 0.7778\n",
      "Epoch 185/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0418 - mean_absolute_error: 0.1490 - val_loss: 1.3856 - val_mean_absolute_error: 0.8107\n",
      "Epoch 186/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0458 - mean_absolute_error: 0.1576 - val_loss: 1.3171 - val_mean_absolute_error: 0.7730\n",
      "Epoch 187/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0397 - mean_absolute_error: 0.1475 - val_loss: 1.3465 - val_mean_absolute_error: 0.7765\n",
      "Epoch 188/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0434 - mean_absolute_error: 0.1541 - val_loss: 1.3733 - val_mean_absolute_error: 0.7902\n",
      "Epoch 189/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0534 - mean_absolute_error: 0.1688 - val_loss: 1.3767 - val_mean_absolute_error: 0.7994\n",
      "Epoch 190/200\n",
      "16226/16226 [==============================] - 1s 70us/step - loss: 0.0432 - mean_absolute_error: 0.1515 - val_loss: 1.4244 - val_mean_absolute_error: 0.8163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/200\n",
      "16226/16226 [==============================] - 1s 70us/step - loss: 0.0414 - mean_absolute_error: 0.1460 - val_loss: 1.3925 - val_mean_absolute_error: 0.8071\n",
      "Epoch 192/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.0362 - mean_absolute_error: 0.1393 - val_loss: 1.3519 - val_mean_absolute_error: 0.7878\n",
      "Epoch 193/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0414 - mean_absolute_error: 0.1490 - val_loss: 1.3455 - val_mean_absolute_error: 0.7776\n",
      "Epoch 194/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0461 - mean_absolute_error: 0.1550 - val_loss: 1.3404 - val_mean_absolute_error: 0.7873\n",
      "Epoch 195/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0490 - mean_absolute_error: 0.1638 - val_loss: 1.3321 - val_mean_absolute_error: 0.7794\n",
      "Epoch 196/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0506 - mean_absolute_error: 0.1632 - val_loss: 1.3255 - val_mean_absolute_error: 0.7724\n",
      "Epoch 197/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0387 - mean_absolute_error: 0.1434 - val_loss: 1.3360 - val_mean_absolute_error: 0.7769\n",
      "Epoch 198/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0361 - mean_absolute_error: 0.1390 - val_loss: 1.4497 - val_mean_absolute_error: 0.8392\n",
      "Epoch 199/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0374 - mean_absolute_error: 0.1424 - val_loss: 1.3407 - val_mean_absolute_error: 0.7790\n",
      "Epoch 200/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0424 - mean_absolute_error: 0.1498 - val_loss: 1.3365 - val_mean_absolute_error: 0.7735\n",
      "(20283, 2000) (20283,)\n",
      "Train on 16226 samples, validate on 4057 samples\n",
      "Epoch 1/200\n",
      "16226/16226 [==============================] - 1s 87us/step - loss: 27.6219 - mean_absolute_error: 2.8950 - val_loss: 5.4802 - val_mean_absolute_error: 1.7222\n",
      "Epoch 2/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 2.3275 - mean_absolute_error: 1.1706 - val_loss: 4.6078 - val_mean_absolute_error: 1.5930\n",
      "Epoch 3/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 1.6706 - mean_absolute_error: 0.9905 - val_loss: 4.3066 - val_mean_absolute_error: 1.5339\n",
      "Epoch 4/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 1.4804 - mean_absolute_error: 0.9340 - val_loss: 4.1748 - val_mean_absolute_error: 1.4589\n",
      "Epoch 5/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 1.2974 - mean_absolute_error: 0.8748 - val_loss: 3.8799 - val_mean_absolute_error: 1.4125\n",
      "Epoch 6/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 1.1115 - mean_absolute_error: 0.8079 - val_loss: 4.2467 - val_mean_absolute_error: 1.4761\n",
      "Epoch 7/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 1.0842 - mean_absolute_error: 0.7972 - val_loss: 3.8539 - val_mean_absolute_error: 1.4215\n",
      "Epoch 8/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.9794 - mean_absolute_error: 0.7628 - val_loss: 5.5995 - val_mean_absolute_error: 1.7479\n",
      "Epoch 9/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 1.2033 - mean_absolute_error: 0.8399 - val_loss: 4.1212 - val_mean_absolute_error: 1.4549\n",
      "Epoch 10/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.8284 - mean_absolute_error: 0.6945 - val_loss: 3.6281 - val_mean_absolute_error: 1.3483\n",
      "Epoch 11/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.7565 - mean_absolute_error: 0.6700 - val_loss: 3.3867 - val_mean_absolute_error: 1.3425\n",
      "Epoch 12/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.8263 - mean_absolute_error: 0.6899 - val_loss: 3.4992 - val_mean_absolute_error: 1.3339\n",
      "Epoch 13/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.7170 - mean_absolute_error: 0.6443 - val_loss: 3.2637 - val_mean_absolute_error: 1.2534\n",
      "Epoch 14/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.7946 - mean_absolute_error: 0.6774 - val_loss: 3.1000 - val_mean_absolute_error: 1.2340\n",
      "Epoch 15/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.6306 - mean_absolute_error: 0.6048 - val_loss: 2.9898 - val_mean_absolute_error: 1.2288\n",
      "Epoch 16/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.6198 - mean_absolute_error: 0.5992 - val_loss: 3.2185 - val_mean_absolute_error: 1.2461\n",
      "Epoch 17/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.6113 - mean_absolute_error: 0.5999 - val_loss: 3.4498 - val_mean_absolute_error: 1.3507\n",
      "Epoch 18/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.6177 - mean_absolute_error: 0.5943 - val_loss: 3.0239 - val_mean_absolute_error: 1.2316\n",
      "Epoch 19/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.6554 - mean_absolute_error: 0.6030 - val_loss: 2.8713 - val_mean_absolute_error: 1.2277\n",
      "Epoch 20/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.5404 - mean_absolute_error: 0.5500 - val_loss: 2.6670 - val_mean_absolute_error: 1.1436\n",
      "Epoch 21/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.4071 - mean_absolute_error: 0.4842 - val_loss: 2.7995 - val_mean_absolute_error: 1.1855\n",
      "Epoch 22/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.4570 - mean_absolute_error: 0.5200 - val_loss: 3.5001 - val_mean_absolute_error: 1.3941\n",
      "Epoch 23/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.5363 - mean_absolute_error: 0.5601 - val_loss: 3.0918 - val_mean_absolute_error: 1.2483\n",
      "Epoch 24/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.4434 - mean_absolute_error: 0.5077 - val_loss: 2.7518 - val_mean_absolute_error: 1.1832\n",
      "Epoch 25/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.4332 - mean_absolute_error: 0.5041 - val_loss: 2.4690 - val_mean_absolute_error: 1.1033\n",
      "Epoch 26/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.4792 - mean_absolute_error: 0.5209 - val_loss: 2.5485 - val_mean_absolute_error: 1.1370\n",
      "Epoch 27/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.3617 - mean_absolute_error: 0.4584 - val_loss: 2.5916 - val_mean_absolute_error: 1.1227\n",
      "Epoch 28/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.3580 - mean_absolute_error: 0.4581 - val_loss: 2.6324 - val_mean_absolute_error: 1.1509\n",
      "Epoch 29/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.3823 - mean_absolute_error: 0.4742 - val_loss: 2.6125 - val_mean_absolute_error: 1.1341\n",
      "Epoch 30/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.3445 - mean_absolute_error: 0.4494 - val_loss: 2.4650 - val_mean_absolute_error: 1.0926\n",
      "Epoch 31/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.3533 - mean_absolute_error: 0.4493 - val_loss: 2.6858 - val_mean_absolute_error: 1.1486\n",
      "Epoch 32/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.3065 - mean_absolute_error: 0.4217 - val_loss: 2.5749 - val_mean_absolute_error: 1.1760\n",
      "Epoch 33/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.3477 - mean_absolute_error: 0.4479 - val_loss: 2.3238 - val_mean_absolute_error: 1.0458\n",
      "Epoch 34/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.3258 - mean_absolute_error: 0.4350 - val_loss: 2.3846 - val_mean_absolute_error: 1.0608\n",
      "Epoch 35/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.2863 - mean_absolute_error: 0.4084 - val_loss: 2.9392 - val_mean_absolute_error: 1.2385\n",
      "Epoch 36/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.2911 - mean_absolute_error: 0.4136 - val_loss: 2.3710 - val_mean_absolute_error: 1.1114\n",
      "Epoch 37/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.2856 - mean_absolute_error: 0.4069 - val_loss: 2.4104 - val_mean_absolute_error: 1.0718\n",
      "Epoch 38/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.2916 - mean_absolute_error: 0.4132 - val_loss: 2.3456 - val_mean_absolute_error: 1.0817\n",
      "Epoch 39/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.2659 - mean_absolute_error: 0.3951 - val_loss: 2.2633 - val_mean_absolute_error: 1.0307\n",
      "Epoch 40/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.2967 - mean_absolute_error: 0.4098 - val_loss: 2.1889 - val_mean_absolute_error: 1.0291\n",
      "Epoch 41/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.2362 - mean_absolute_error: 0.3703 - val_loss: 2.4578 - val_mean_absolute_error: 1.1203\n",
      "Epoch 42/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.2581 - mean_absolute_error: 0.3858 - val_loss: 2.1140 - val_mean_absolute_error: 0.9974\n",
      "Epoch 43/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.2369 - mean_absolute_error: 0.3712 - val_loss: 2.2308 - val_mean_absolute_error: 1.0450\n",
      "Epoch 44/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.2249 - mean_absolute_error: 0.3609 - val_loss: 2.1694 - val_mean_absolute_error: 1.0255\n",
      "Epoch 45/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.2204 - mean_absolute_error: 0.3566 - val_loss: 2.3402 - val_mean_absolute_error: 1.0652\n",
      "Epoch 46/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.2257 - mean_absolute_error: 0.3632 - val_loss: 2.5974 - val_mean_absolute_error: 1.1622\n",
      "Epoch 47/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.2615 - mean_absolute_error: 0.3948 - val_loss: 2.3852 - val_mean_absolute_error: 1.0926\n",
      "Epoch 48/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.2226 - mean_absolute_error: 0.3601 - val_loss: 2.1981 - val_mean_absolute_error: 1.0331\n",
      "Epoch 49/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.2113 - mean_absolute_error: 0.3495 - val_loss: 2.1182 - val_mean_absolute_error: 1.0019\n",
      "Epoch 50/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.1977 - mean_absolute_error: 0.3367 - val_loss: 2.0490 - val_mean_absolute_error: 0.9918\n",
      "Epoch 51/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.1833 - mean_absolute_error: 0.3260 - val_loss: 2.0252 - val_mean_absolute_error: 0.9729\n",
      "Epoch 52/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.2190 - mean_absolute_error: 0.3534 - val_loss: 2.1645 - val_mean_absolute_error: 1.0184\n",
      "Epoch 53/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.1982 - mean_absolute_error: 0.3361 - val_loss: 2.0942 - val_mean_absolute_error: 0.9921\n",
      "Epoch 54/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.1901 - mean_absolute_error: 0.3331 - val_loss: 1.9706 - val_mean_absolute_error: 0.9666\n",
      "Epoch 55/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.1930 - mean_absolute_error: 0.3345 - val_loss: 1.9826 - val_mean_absolute_error: 0.9651\n",
      "Epoch 56/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.1914 - mean_absolute_error: 0.3331 - val_loss: 1.9836 - val_mean_absolute_error: 0.9787\n",
      "Epoch 57/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.1746 - mean_absolute_error: 0.3183 - val_loss: 2.1894 - val_mean_absolute_error: 1.0369\n",
      "Epoch 58/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1920 - mean_absolute_error: 0.3325 - val_loss: 1.9171 - val_mean_absolute_error: 0.9560\n",
      "Epoch 59/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.1956 - mean_absolute_error: 0.3304 - val_loss: 1.9637 - val_mean_absolute_error: 0.9721\n",
      "Epoch 60/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.1532 - mean_absolute_error: 0.2964 - val_loss: 2.0576 - val_mean_absolute_error: 0.9898\n",
      "Epoch 61/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.1603 - mean_absolute_error: 0.3041 - val_loss: 1.8387 - val_mean_absolute_error: 0.9399\n",
      "Epoch 62/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.1544 - mean_absolute_error: 0.2993 - val_loss: 1.8928 - val_mean_absolute_error: 0.9654\n",
      "Epoch 63/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.1674 - mean_absolute_error: 0.3094 - val_loss: 1.9003 - val_mean_absolute_error: 0.9797\n",
      "Epoch 64/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.1600 - mean_absolute_error: 0.3033 - val_loss: 1.8757 - val_mean_absolute_error: 0.9614\n",
      "Epoch 65/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.1663 - mean_absolute_error: 0.3100 - val_loss: 1.7972 - val_mean_absolute_error: 0.9162\n",
      "Epoch 66/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.1403 - mean_absolute_error: 0.2821 - val_loss: 1.9826 - val_mean_absolute_error: 0.9784\n",
      "Epoch 67/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1522 - mean_absolute_error: 0.2946 - val_loss: 1.7804 - val_mean_absolute_error: 0.9054\n",
      "Epoch 68/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.1405 - mean_absolute_error: 0.2840 - val_loss: 1.9715 - val_mean_absolute_error: 0.9728\n",
      "Epoch 69/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.1311 - mean_absolute_error: 0.2739 - val_loss: 1.9342 - val_mean_absolute_error: 0.9652\n",
      "Epoch 70/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1350 - mean_absolute_error: 0.2787 - val_loss: 1.9723 - val_mean_absolute_error: 0.9723\n",
      "Epoch 71/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.1404 - mean_absolute_error: 0.2858 - val_loss: 1.9478 - val_mean_absolute_error: 0.9866\n",
      "Epoch 72/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1369 - mean_absolute_error: 0.2782 - val_loss: 2.0797 - val_mean_absolute_error: 1.0398\n",
      "Epoch 73/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1541 - mean_absolute_error: 0.2983 - val_loss: 1.8011 - val_mean_absolute_error: 0.9197\n",
      "Epoch 74/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.1453 - mean_absolute_error: 0.2894 - val_loss: 1.8495 - val_mean_absolute_error: 0.9411\n",
      "Epoch 75/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1233 - mean_absolute_error: 0.2677 - val_loss: 1.7232 - val_mean_absolute_error: 0.8980\n",
      "Epoch 76/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1384 - mean_absolute_error: 0.2779 - val_loss: 1.7775 - val_mean_absolute_error: 0.9214\n",
      "Epoch 77/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1166 - mean_absolute_error: 0.2594 - val_loss: 1.6980 - val_mean_absolute_error: 0.8853\n",
      "Epoch 78/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1157 - mean_absolute_error: 0.2564 - val_loss: 1.6816 - val_mean_absolute_error: 0.8983\n",
      "Epoch 79/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1206 - mean_absolute_error: 0.2617 - val_loss: 1.7601 - val_mean_absolute_error: 0.9324\n",
      "Epoch 80/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1249 - mean_absolute_error: 0.2618 - val_loss: 1.8732 - val_mean_absolute_error: 0.9684\n",
      "Epoch 81/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1230 - mean_absolute_error: 0.2678 - val_loss: 1.7605 - val_mean_absolute_error: 0.9287\n",
      "Epoch 82/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1091 - mean_absolute_error: 0.2482 - val_loss: 1.6233 - val_mean_absolute_error: 0.8611\n",
      "Epoch 83/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.1113 - mean_absolute_error: 0.2520 - val_loss: 1.7026 - val_mean_absolute_error: 0.8877\n",
      "Epoch 84/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.1124 - mean_absolute_error: 0.2512 - val_loss: 1.7356 - val_mean_absolute_error: 0.9100\n",
      "Epoch 85/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.1231 - mean_absolute_error: 0.2635 - val_loss: 1.6321 - val_mean_absolute_error: 0.8773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.1020 - mean_absolute_error: 0.2413 - val_loss: 1.9437 - val_mean_absolute_error: 0.9942\n",
      "Epoch 87/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.1133 - mean_absolute_error: 0.2556 - val_loss: 1.6537 - val_mean_absolute_error: 0.8741\n",
      "Epoch 88/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0983 - mean_absolute_error: 0.2361 - val_loss: 1.6524 - val_mean_absolute_error: 0.8946\n",
      "Epoch 89/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.1103 - mean_absolute_error: 0.2500 - val_loss: 1.5823 - val_mean_absolute_error: 0.8543\n",
      "Epoch 90/200\n",
      "16226/16226 [==============================] - 1s 86us/step - loss: 0.0986 - mean_absolute_error: 0.2356 - val_loss: 1.6008 - val_mean_absolute_error: 0.8602\n",
      "Epoch 91/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.1068 - mean_absolute_error: 0.2435 - val_loss: 1.6667 - val_mean_absolute_error: 0.9092\n",
      "Epoch 92/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.1142 - mean_absolute_error: 0.2460 - val_loss: 1.5862 - val_mean_absolute_error: 0.8479\n",
      "Epoch 93/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0883 - mean_absolute_error: 0.2236 - val_loss: 1.6491 - val_mean_absolute_error: 0.8730\n",
      "Epoch 94/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0822 - mean_absolute_error: 0.2173 - val_loss: 1.5966 - val_mean_absolute_error: 0.8599\n",
      "Epoch 95/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.1017 - mean_absolute_error: 0.2395 - val_loss: 1.5535 - val_mean_absolute_error: 0.8478\n",
      "Epoch 96/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0987 - mean_absolute_error: 0.2362 - val_loss: 1.6071 - val_mean_absolute_error: 0.8597\n",
      "Epoch 97/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0863 - mean_absolute_error: 0.2203 - val_loss: 1.6099 - val_mean_absolute_error: 0.8586\n",
      "Epoch 98/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0815 - mean_absolute_error: 0.2134 - val_loss: 1.5759 - val_mean_absolute_error: 0.8500\n",
      "Epoch 99/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.1046 - mean_absolute_error: 0.2455 - val_loss: 1.9243 - val_mean_absolute_error: 0.9964\n",
      "Epoch 100/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.1279 - mean_absolute_error: 0.2633 - val_loss: 1.5733 - val_mean_absolute_error: 0.8468\n",
      "Epoch 101/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0848 - mean_absolute_error: 0.2181 - val_loss: 1.5806 - val_mean_absolute_error: 0.8446\n",
      "Epoch 102/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0778 - mean_absolute_error: 0.2080 - val_loss: 1.5079 - val_mean_absolute_error: 0.8467\n",
      "Epoch 103/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0847 - mean_absolute_error: 0.2174 - val_loss: 1.5264 - val_mean_absolute_error: 0.8414\n",
      "Epoch 104/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0875 - mean_absolute_error: 0.2216 - val_loss: 1.6642 - val_mean_absolute_error: 0.8895\n",
      "Epoch 105/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0783 - mean_absolute_error: 0.2102 - val_loss: 1.6102 - val_mean_absolute_error: 0.8664\n",
      "Epoch 106/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0852 - mean_absolute_error: 0.2195 - val_loss: 1.5200 - val_mean_absolute_error: 0.8302\n",
      "Epoch 107/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0783 - mean_absolute_error: 0.2100 - val_loss: 1.6410 - val_mean_absolute_error: 0.8777\n",
      "Epoch 108/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0818 - mean_absolute_error: 0.2149 - val_loss: 1.5377 - val_mean_absolute_error: 0.8425\n",
      "Epoch 109/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0818 - mean_absolute_error: 0.2129 - val_loss: 1.5010 - val_mean_absolute_error: 0.8221\n",
      "Epoch 110/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0802 - mean_absolute_error: 0.2106 - val_loss: 1.5316 - val_mean_absolute_error: 0.8602\n",
      "Epoch 111/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0873 - mean_absolute_error: 0.2177 - val_loss: 1.5044 - val_mean_absolute_error: 0.8234\n",
      "Epoch 112/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0707 - mean_absolute_error: 0.1981 - val_loss: 1.5326 - val_mean_absolute_error: 0.8330\n",
      "Epoch 113/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0732 - mean_absolute_error: 0.2026 - val_loss: 1.4969 - val_mean_absolute_error: 0.8318\n",
      "Epoch 114/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0875 - mean_absolute_error: 0.2202 - val_loss: 1.5661 - val_mean_absolute_error: 0.8567\n",
      "Epoch 115/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0753 - mean_absolute_error: 0.2043 - val_loss: 1.6729 - val_mean_absolute_error: 0.8938\n",
      "Epoch 116/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0877 - mean_absolute_error: 0.2121 - val_loss: 1.5702 - val_mean_absolute_error: 0.8984\n",
      "Epoch 117/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0931 - mean_absolute_error: 0.2226 - val_loss: 1.4653 - val_mean_absolute_error: 0.8154\n",
      "Epoch 118/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0618 - mean_absolute_error: 0.1854 - val_loss: 1.5038 - val_mean_absolute_error: 0.8418\n",
      "Epoch 119/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0754 - mean_absolute_error: 0.2047 - val_loss: 1.5693 - val_mean_absolute_error: 0.8423\n",
      "Epoch 120/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0755 - mean_absolute_error: 0.2047 - val_loss: 1.5089 - val_mean_absolute_error: 0.8261\n",
      "Epoch 121/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0723 - mean_absolute_error: 0.2011 - val_loss: 1.5923 - val_mean_absolute_error: 0.8739\n",
      "Epoch 122/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0721 - mean_absolute_error: 0.1997 - val_loss: 1.5307 - val_mean_absolute_error: 0.8411\n",
      "Epoch 123/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0809 - mean_absolute_error: 0.2129 - val_loss: 1.5200 - val_mean_absolute_error: 0.8327\n",
      "Epoch 124/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0694 - mean_absolute_error: 0.1981 - val_loss: 1.5363 - val_mean_absolute_error: 0.8441\n",
      "Epoch 125/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0657 - mean_absolute_error: 0.1913 - val_loss: 1.4497 - val_mean_absolute_error: 0.8156\n",
      "Epoch 126/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0737 - mean_absolute_error: 0.2035 - val_loss: 1.4455 - val_mean_absolute_error: 0.8157\n",
      "Epoch 127/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0636 - mean_absolute_error: 0.1878 - val_loss: 1.4481 - val_mean_absolute_error: 0.7998\n",
      "Epoch 128/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0616 - mean_absolute_error: 0.1814 - val_loss: 1.5337 - val_mean_absolute_error: 0.8630\n",
      "Epoch 129/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0860 - mean_absolute_error: 0.2148 - val_loss: 1.4444 - val_mean_absolute_error: 0.8020\n",
      "Epoch 130/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0606 - mean_absolute_error: 0.1834 - val_loss: 1.4660 - val_mean_absolute_error: 0.8120\n",
      "Epoch 131/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0554 - mean_absolute_error: 0.1761 - val_loss: 1.5807 - val_mean_absolute_error: 0.8636\n",
      "Epoch 132/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0760 - mean_absolute_error: 0.2033 - val_loss: 1.4267 - val_mean_absolute_error: 0.7954\n",
      "Epoch 133/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0603 - mean_absolute_error: 0.1831 - val_loss: 1.4533 - val_mean_absolute_error: 0.8071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0557 - mean_absolute_error: 0.1758 - val_loss: 1.4677 - val_mean_absolute_error: 0.8330\n",
      "Epoch 135/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0571 - mean_absolute_error: 0.1782 - val_loss: 1.4606 - val_mean_absolute_error: 0.8108\n",
      "Epoch 136/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0642 - mean_absolute_error: 0.1864 - val_loss: 1.4839 - val_mean_absolute_error: 0.8549\n",
      "Epoch 137/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0733 - mean_absolute_error: 0.1975 - val_loss: 1.4402 - val_mean_absolute_error: 0.8038\n",
      "Epoch 138/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0672 - mean_absolute_error: 0.1958 - val_loss: 1.4582 - val_mean_absolute_error: 0.8124\n",
      "Epoch 139/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0533 - mean_absolute_error: 0.1708 - val_loss: 1.4389 - val_mean_absolute_error: 0.7963\n",
      "Epoch 140/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0541 - mean_absolute_error: 0.1723 - val_loss: 1.4278 - val_mean_absolute_error: 0.8021\n",
      "Epoch 141/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0631 - mean_absolute_error: 0.1848 - val_loss: 1.4558 - val_mean_absolute_error: 0.8104\n",
      "Epoch 142/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0581 - mean_absolute_error: 0.1815 - val_loss: 1.4213 - val_mean_absolute_error: 0.8067\n",
      "Epoch 143/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0566 - mean_absolute_error: 0.1763 - val_loss: 1.5004 - val_mean_absolute_error: 0.8265\n",
      "Epoch 144/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0534 - mean_absolute_error: 0.1684 - val_loss: 1.4410 - val_mean_absolute_error: 0.7951\n",
      "Epoch 145/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0626 - mean_absolute_error: 0.1852 - val_loss: 1.4304 - val_mean_absolute_error: 0.7972\n",
      "Epoch 146/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0659 - mean_absolute_error: 0.1872 - val_loss: 1.8027 - val_mean_absolute_error: 0.9788\n",
      "Epoch 147/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0787 - mean_absolute_error: 0.1969 - val_loss: 1.4299 - val_mean_absolute_error: 0.7958\n",
      "Epoch 148/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0519 - mean_absolute_error: 0.1672 - val_loss: 1.4328 - val_mean_absolute_error: 0.7940\n",
      "Epoch 149/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0505 - mean_absolute_error: 0.1650 - val_loss: 1.4143 - val_mean_absolute_error: 0.7866\n",
      "Epoch 150/200\n",
      "16226/16226 [==============================] - 1s 70us/step - loss: 0.0486 - mean_absolute_error: 0.1631 - val_loss: 1.4371 - val_mean_absolute_error: 0.8043\n",
      "Epoch 151/200\n",
      "16226/16226 [==============================] - ETA: 0s - loss: 0.0574 - mean_absolute_error: 0.177 - 1s 72us/step - loss: 0.0574 - mean_absolute_error: 0.1770 - val_loss: 1.4202 - val_mean_absolute_error: 0.7874\n",
      "Epoch 152/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0584 - mean_absolute_error: 0.1774 - val_loss: 1.3990 - val_mean_absolute_error: 0.7913\n",
      "Epoch 153/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0489 - mean_absolute_error: 0.1619 - val_loss: 1.4207 - val_mean_absolute_error: 0.7949\n",
      "Epoch 154/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0507 - mean_absolute_error: 0.1677 - val_loss: 1.4901 - val_mean_absolute_error: 0.8222\n",
      "Epoch 155/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0583 - mean_absolute_error: 0.1774 - val_loss: 1.4098 - val_mean_absolute_error: 0.7899\n",
      "Epoch 156/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0550 - mean_absolute_error: 0.1722 - val_loss: 1.6161 - val_mean_absolute_error: 0.8849\n",
      "Epoch 157/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0540 - mean_absolute_error: 0.1696 - val_loss: 1.4286 - val_mean_absolute_error: 0.7940\n",
      "Epoch 158/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0606 - mean_absolute_error: 0.1805 - val_loss: 1.4145 - val_mean_absolute_error: 0.7867\n",
      "Epoch 159/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0511 - mean_absolute_error: 0.1658 - val_loss: 1.4936 - val_mean_absolute_error: 0.8252\n",
      "Epoch 160/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0514 - mean_absolute_error: 0.1655 - val_loss: 1.4131 - val_mean_absolute_error: 0.7916\n",
      "Epoch 161/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0458 - mean_absolute_error: 0.1567 - val_loss: 1.3722 - val_mean_absolute_error: 0.7807\n",
      "Epoch 162/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0499 - mean_absolute_error: 0.1657 - val_loss: 1.4229 - val_mean_absolute_error: 0.7897\n",
      "Epoch 163/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0536 - mean_absolute_error: 0.1706 - val_loss: 1.4468 - val_mean_absolute_error: 0.8060\n",
      "Epoch 164/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0483 - mean_absolute_error: 0.1625 - val_loss: 1.3993 - val_mean_absolute_error: 0.7801\n",
      "Epoch 165/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0530 - mean_absolute_error: 0.1673 - val_loss: 1.3851 - val_mean_absolute_error: 0.7755\n",
      "Epoch 166/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0498 - mean_absolute_error: 0.1629 - val_loss: 1.3795 - val_mean_absolute_error: 0.7758\n",
      "Epoch 167/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0415 - mean_absolute_error: 0.1503 - val_loss: 1.4300 - val_mean_absolute_error: 0.7967\n",
      "Epoch 168/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0464 - mean_absolute_error: 0.1568 - val_loss: 1.4000 - val_mean_absolute_error: 0.7805\n",
      "Epoch 169/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0394 - mean_absolute_error: 0.1465 - val_loss: 1.3545 - val_mean_absolute_error: 0.7654\n",
      "Epoch 170/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0547 - mean_absolute_error: 0.1741 - val_loss: 1.3759 - val_mean_absolute_error: 0.7790\n",
      "Epoch 171/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0482 - mean_absolute_error: 0.1622 - val_loss: 1.3555 - val_mean_absolute_error: 0.7690\n",
      "Epoch 172/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0461 - mean_absolute_error: 0.1581 - val_loss: 1.3631 - val_mean_absolute_error: 0.7772\n",
      "Epoch 173/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0467 - mean_absolute_error: 0.1579 - val_loss: 1.4117 - val_mean_absolute_error: 0.7912\n",
      "Epoch 174/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0396 - mean_absolute_error: 0.1453 - val_loss: 1.3963 - val_mean_absolute_error: 0.7839\n",
      "Epoch 175/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0458 - mean_absolute_error: 0.1581 - val_loss: 1.3638 - val_mean_absolute_error: 0.7785\n",
      "Epoch 176/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0551 - mean_absolute_error: 0.1746 - val_loss: 1.3546 - val_mean_absolute_error: 0.7790\n",
      "Epoch 177/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0420 - mean_absolute_error: 0.1516 - val_loss: 1.3828 - val_mean_absolute_error: 0.7740\n",
      "Epoch 178/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0406 - mean_absolute_error: 0.1472 - val_loss: 1.3849 - val_mean_absolute_error: 0.7780\n",
      "Epoch 179/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0478 - mean_absolute_error: 0.1622 - val_loss: 1.4200 - val_mean_absolute_error: 0.7947\n",
      "Epoch 180/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0426 - mean_absolute_error: 0.1482 - val_loss: 1.3455 - val_mean_absolute_error: 0.7629\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0430 - mean_absolute_error: 0.1510 - val_loss: 1.3540 - val_mean_absolute_error: 0.7639\n",
      "Epoch 182/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0427 - mean_absolute_error: 0.1476 - val_loss: 1.4043 - val_mean_absolute_error: 0.7838\n",
      "Epoch 183/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0421 - mean_absolute_error: 0.1507 - val_loss: 1.3995 - val_mean_absolute_error: 0.7845\n",
      "Epoch 184/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0426 - mean_absolute_error: 0.1541 - val_loss: 1.3490 - val_mean_absolute_error: 0.7696\n",
      "Epoch 185/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0392 - mean_absolute_error: 0.1459 - val_loss: 1.3590 - val_mean_absolute_error: 0.7672\n",
      "Epoch 186/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0408 - mean_absolute_error: 0.1470 - val_loss: 1.4229 - val_mean_absolute_error: 0.7972\n",
      "Epoch 187/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0433 - mean_absolute_error: 0.1537 - val_loss: 1.4165 - val_mean_absolute_error: 0.7946\n",
      "Epoch 188/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0424 - mean_absolute_error: 0.1495 - val_loss: 1.4096 - val_mean_absolute_error: 0.7893\n",
      "Epoch 189/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0357 - mean_absolute_error: 0.1375 - val_loss: 1.3441 - val_mean_absolute_error: 0.7632\n",
      "Epoch 190/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0409 - mean_absolute_error: 0.1465 - val_loss: 1.3428 - val_mean_absolute_error: 0.7603\n",
      "Epoch 191/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0394 - mean_absolute_error: 0.1448 - val_loss: 1.3923 - val_mean_absolute_error: 0.7754\n",
      "Epoch 192/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0516 - mean_absolute_error: 0.1583 - val_loss: 1.3913 - val_mean_absolute_error: 0.7839\n",
      "Epoch 193/200\n",
      "16226/16226 [==============================] - 1s 74us/step - loss: 0.0400 - mean_absolute_error: 0.1449 - val_loss: 1.3278 - val_mean_absolute_error: 0.7636\n",
      "Epoch 194/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0375 - mean_absolute_error: 0.1396 - val_loss: 1.3328 - val_mean_absolute_error: 0.7588\n",
      "Epoch 195/200\n",
      "16226/16226 [==============================] - 1s 71us/step - loss: 0.0362 - mean_absolute_error: 0.1401 - val_loss: 1.3757 - val_mean_absolute_error: 0.7780\n",
      "Epoch 196/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0374 - mean_absolute_error: 0.1437 - val_loss: 1.3524 - val_mean_absolute_error: 0.7692\n",
      "Epoch 197/200\n",
      "16226/16226 [==============================] - 1s 72us/step - loss: 0.0374 - mean_absolute_error: 0.1425 - val_loss: 1.4403 - val_mean_absolute_error: 0.8114\n",
      "Epoch 198/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0358 - mean_absolute_error: 0.1372 - val_loss: 1.3265 - val_mean_absolute_error: 0.7548\n",
      "Epoch 199/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0427 - mean_absolute_error: 0.1492 - val_loss: 1.3697 - val_mean_absolute_error: 0.7808\n",
      "Epoch 200/200\n",
      "16226/16226 [==============================] - 1s 73us/step - loss: 0.0441 - mean_absolute_error: 0.1552 - val_loss: 1.3555 - val_mean_absolute_error: 0.7649\n",
      "(20283, 2000) (20283,)\n",
      "Train on 16226 samples, validate on 4057 samples\n",
      "Epoch 1/200\n",
      "16226/16226 [==============================] - 1s 91us/step - loss: 25.4332 - mean_absolute_error: 2.8815 - val_loss: 6.0249 - val_mean_absolute_error: 1.8746\n",
      "Epoch 2/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 2.4973 - mean_absolute_error: 1.2023 - val_loss: 4.6303 - val_mean_absolute_error: 1.5894\n",
      "Epoch 3/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 1.7347 - mean_absolute_error: 1.0132 - val_loss: 4.1363 - val_mean_absolute_error: 1.4917\n",
      "Epoch 4/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 1.4991 - mean_absolute_error: 0.9388 - val_loss: 4.0260 - val_mean_absolute_error: 1.4778\n",
      "Epoch 5/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 1.2914 - mean_absolute_error: 0.8726 - val_loss: 4.1263 - val_mean_absolute_error: 1.4893\n",
      "Epoch 6/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 1.3305 - mean_absolute_error: 0.8911 - val_loss: 4.0962 - val_mean_absolute_error: 1.5157\n",
      "Epoch 7/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 1.2977 - mean_absolute_error: 0.8633 - val_loss: 3.4301 - val_mean_absolute_error: 1.3501\n",
      "Epoch 8/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 1.0684 - mean_absolute_error: 0.7900 - val_loss: 3.2606 - val_mean_absolute_error: 1.2969\n",
      "Epoch 9/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 1.1335 - mean_absolute_error: 0.7966 - val_loss: 3.7208 - val_mean_absolute_error: 1.3783\n",
      "Epoch 10/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.8228 - mean_absolute_error: 0.6932 - val_loss: 2.9076 - val_mean_absolute_error: 1.2347\n",
      "Epoch 11/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.8465 - mean_absolute_error: 0.7034 - val_loss: 3.8167 - val_mean_absolute_error: 1.3912\n",
      "Epoch 12/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.9326 - mean_absolute_error: 0.7411 - val_loss: 3.6420 - val_mean_absolute_error: 1.3576\n",
      "Epoch 13/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.8289 - mean_absolute_error: 0.6892 - val_loss: 3.9380 - val_mean_absolute_error: 1.4565\n",
      "Epoch 14/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.7937 - mean_absolute_error: 0.6782 - val_loss: 3.0274 - val_mean_absolute_error: 1.2783\n",
      "Epoch 15/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.6852 - mean_absolute_error: 0.6283 - val_loss: 2.8535 - val_mean_absolute_error: 1.2242\n",
      "Epoch 16/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.6338 - mean_absolute_error: 0.6076 - val_loss: 2.9435 - val_mean_absolute_error: 1.1942\n",
      "Epoch 17/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.6430 - mean_absolute_error: 0.6109 - val_loss: 2.7529 - val_mean_absolute_error: 1.1532\n",
      "Epoch 18/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.5693 - mean_absolute_error: 0.5702 - val_loss: 3.1438 - val_mean_absolute_error: 1.2487\n",
      "Epoch 19/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.6482 - mean_absolute_error: 0.6133 - val_loss: 3.4459 - val_mean_absolute_error: 1.3534\n",
      "Epoch 20/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.5975 - mean_absolute_error: 0.5805 - val_loss: 2.9213 - val_mean_absolute_error: 1.2706\n",
      "Epoch 21/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.5013 - mean_absolute_error: 0.5428 - val_loss: 2.6491 - val_mean_absolute_error: 1.1675\n",
      "Epoch 22/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.5657 - mean_absolute_error: 0.5701 - val_loss: 2.3937 - val_mean_absolute_error: 1.0685\n",
      "Epoch 23/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.4920 - mean_absolute_error: 0.5336 - val_loss: 2.4275 - val_mean_absolute_error: 1.1245\n",
      "Epoch 24/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.4462 - mean_absolute_error: 0.5033 - val_loss: 2.2422 - val_mean_absolute_error: 1.0481\n",
      "Epoch 25/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.4879 - mean_absolute_error: 0.5322 - val_loss: 2.5101 - val_mean_absolute_error: 1.1038\n",
      "Epoch 26/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.3893 - mean_absolute_error: 0.4750 - val_loss: 2.5094 - val_mean_absolute_error: 1.0927\n",
      "Epoch 27/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.4290 - mean_absolute_error: 0.4998 - val_loss: 2.3620 - val_mean_absolute_error: 1.0946\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.3915 - mean_absolute_error: 0.4766 - val_loss: 2.2416 - val_mean_absolute_error: 1.0463\n",
      "Epoch 29/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.4028 - mean_absolute_error: 0.4873 - val_loss: 2.3341 - val_mean_absolute_error: 1.0681\n",
      "Epoch 30/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.3476 - mean_absolute_error: 0.4478 - val_loss: 2.1634 - val_mean_absolute_error: 1.0212\n",
      "Epoch 31/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.3787 - mean_absolute_error: 0.4697 - val_loss: 2.7296 - val_mean_absolute_error: 1.1760\n",
      "Epoch 32/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.3677 - mean_absolute_error: 0.4619 - val_loss: 2.4846 - val_mean_absolute_error: 1.0983\n",
      "Epoch 33/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.3519 - mean_absolute_error: 0.4538 - val_loss: 2.3936 - val_mean_absolute_error: 1.0830\n",
      "Epoch 34/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.3480 - mean_absolute_error: 0.4450 - val_loss: 2.2293 - val_mean_absolute_error: 1.0993\n",
      "Epoch 35/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.3780 - mean_absolute_error: 0.4701 - val_loss: 2.1436 - val_mean_absolute_error: 1.0011\n",
      "Epoch 36/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.2714 - mean_absolute_error: 0.3922 - val_loss: 2.0664 - val_mean_absolute_error: 1.0189\n",
      "Epoch 37/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.2693 - mean_absolute_error: 0.3912 - val_loss: 2.0428 - val_mean_absolute_error: 1.0137\n",
      "Epoch 38/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.2970 - mean_absolute_error: 0.4135 - val_loss: 2.9356 - val_mean_absolute_error: 1.2718\n",
      "Epoch 39/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.4050 - mean_absolute_error: 0.4720 - val_loss: 1.9512 - val_mean_absolute_error: 0.9618\n",
      "Epoch 40/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.2374 - mean_absolute_error: 0.3661 - val_loss: 2.1547 - val_mean_absolute_error: 1.0259\n",
      "Epoch 41/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.2820 - mean_absolute_error: 0.4023 - val_loss: 2.0481 - val_mean_absolute_error: 0.9855\n",
      "Epoch 42/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.2348 - mean_absolute_error: 0.3643 - val_loss: 1.8871 - val_mean_absolute_error: 0.9392\n",
      "Epoch 43/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.2571 - mean_absolute_error: 0.3853 - val_loss: 2.0182 - val_mean_absolute_error: 0.9720\n",
      "Epoch 44/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.2534 - mean_absolute_error: 0.3809 - val_loss: 2.0849 - val_mean_absolute_error: 1.0715\n",
      "Epoch 45/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.2803 - mean_absolute_error: 0.3958 - val_loss: 1.8770 - val_mean_absolute_error: 0.9362\n",
      "Epoch 46/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.2084 - mean_absolute_error: 0.3451 - val_loss: 1.8940 - val_mean_absolute_error: 0.9387\n",
      "Epoch 47/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.2274 - mean_absolute_error: 0.3619 - val_loss: 2.2401 - val_mean_absolute_error: 1.0601\n",
      "Epoch 48/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.2006 - mean_absolute_error: 0.3349 - val_loss: 1.8434 - val_mean_absolute_error: 0.9347\n",
      "Epoch 49/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.2383 - mean_absolute_error: 0.3705 - val_loss: 2.1552 - val_mean_absolute_error: 1.0293\n",
      "Epoch 50/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.2125 - mean_absolute_error: 0.3463 - val_loss: 2.2651 - val_mean_absolute_error: 1.0762\n",
      "Epoch 51/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.2384 - mean_absolute_error: 0.3669 - val_loss: 1.7926 - val_mean_absolute_error: 0.9058\n",
      "Epoch 52/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.1901 - mean_absolute_error: 0.3244 - val_loss: 1.7212 - val_mean_absolute_error: 0.9050\n",
      "Epoch 53/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.2019 - mean_absolute_error: 0.3389 - val_loss: 1.6805 - val_mean_absolute_error: 0.8863\n",
      "Epoch 54/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.1998 - mean_absolute_error: 0.3327 - val_loss: 1.7557 - val_mean_absolute_error: 0.9026\n",
      "Epoch 55/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.1631 - mean_absolute_error: 0.3005 - val_loss: 1.6915 - val_mean_absolute_error: 0.8785\n",
      "Epoch 56/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.1861 - mean_absolute_error: 0.3239 - val_loss: 1.8990 - val_mean_absolute_error: 0.9542\n",
      "Epoch 57/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.1950 - mean_absolute_error: 0.3294 - val_loss: 1.7466 - val_mean_absolute_error: 0.8879\n",
      "Epoch 58/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.1768 - mean_absolute_error: 0.3135 - val_loss: 1.6427 - val_mean_absolute_error: 0.8626\n",
      "Epoch 59/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.1732 - mean_absolute_error: 0.3087 - val_loss: 1.7630 - val_mean_absolute_error: 0.9526\n",
      "Epoch 60/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.1809 - mean_absolute_error: 0.3206 - val_loss: 1.5897 - val_mean_absolute_error: 0.8591\n",
      "Epoch 61/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.1739 - mean_absolute_error: 0.3042 - val_loss: 2.3520 - val_mean_absolute_error: 1.1234\n",
      "Epoch 62/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.1864 - mean_absolute_error: 0.3152 - val_loss: 2.6996 - val_mean_absolute_error: 1.2906\n",
      "Epoch 63/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.1717 - mean_absolute_error: 0.3026 - val_loss: 1.6702 - val_mean_absolute_error: 0.8751\n",
      "Epoch 64/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.1518 - mean_absolute_error: 0.2900 - val_loss: 1.6753 - val_mean_absolute_error: 0.8685\n",
      "Epoch 65/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.1385 - mean_absolute_error: 0.2746 - val_loss: 1.6399 - val_mean_absolute_error: 0.8557\n",
      "Epoch 66/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.1334 - mean_absolute_error: 0.2712 - val_loss: 1.6487 - val_mean_absolute_error: 0.8572\n",
      "Epoch 67/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.1564 - mean_absolute_error: 0.2957 - val_loss: 1.6044 - val_mean_absolute_error: 0.8448\n",
      "Epoch 68/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.1488 - mean_absolute_error: 0.2862 - val_loss: 1.6882 - val_mean_absolute_error: 0.8666\n",
      "Epoch 69/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.1440 - mean_absolute_error: 0.2813 - val_loss: 1.7754 - val_mean_absolute_error: 0.9072\n",
      "Epoch 70/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.1291 - mean_absolute_error: 0.2671 - val_loss: 1.7974 - val_mean_absolute_error: 0.9196\n",
      "Epoch 71/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.1346 - mean_absolute_error: 0.2723 - val_loss: 1.7062 - val_mean_absolute_error: 0.8917\n",
      "Epoch 72/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.1402 - mean_absolute_error: 0.2775 - val_loss: 1.6078 - val_mean_absolute_error: 0.8582\n",
      "Epoch 73/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.1367 - mean_absolute_error: 0.2762 - val_loss: 1.5934 - val_mean_absolute_error: 0.8561\n",
      "Epoch 74/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.1373 - mean_absolute_error: 0.2758 - val_loss: 1.5491 - val_mean_absolute_error: 0.8258\n",
      "Epoch 75/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.1156 - mean_absolute_error: 0.2531 - val_loss: 1.5080 - val_mean_absolute_error: 0.8290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.1369 - mean_absolute_error: 0.2708 - val_loss: 1.5509 - val_mean_absolute_error: 0.8269\n",
      "Epoch 77/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.1228 - mean_absolute_error: 0.2588 - val_loss: 1.5859 - val_mean_absolute_error: 0.8953\n",
      "Epoch 78/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.1250 - mean_absolute_error: 0.2622 - val_loss: 1.5241 - val_mean_absolute_error: 0.8202\n",
      "Epoch 79/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.1386 - mean_absolute_error: 0.2670 - val_loss: 1.4753 - val_mean_absolute_error: 0.8018\n",
      "Epoch 80/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.1068 - mean_absolute_error: 0.2437 - val_loss: 1.5626 - val_mean_absolute_error: 0.8464\n",
      "Epoch 81/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.1125 - mean_absolute_error: 0.2489 - val_loss: 1.4320 - val_mean_absolute_error: 0.8061\n",
      "Epoch 82/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.1107 - mean_absolute_error: 0.2463 - val_loss: 1.7401 - val_mean_absolute_error: 0.9098\n",
      "Epoch 83/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.1227 - mean_absolute_error: 0.2586 - val_loss: 1.5163 - val_mean_absolute_error: 0.8165\n",
      "Epoch 84/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.1095 - mean_absolute_error: 0.2467 - val_loss: 1.5874 - val_mean_absolute_error: 0.8414\n",
      "Epoch 85/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.1007 - mean_absolute_error: 0.2346 - val_loss: 1.4389 - val_mean_absolute_error: 0.7942\n",
      "Epoch 86/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.0994 - mean_absolute_error: 0.2307 - val_loss: 1.4822 - val_mean_absolute_error: 0.8055\n",
      "Epoch 87/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.1119 - mean_absolute_error: 0.2458 - val_loss: 1.4439 - val_mean_absolute_error: 0.7996\n",
      "Epoch 88/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.1008 - mean_absolute_error: 0.2379 - val_loss: 1.6541 - val_mean_absolute_error: 0.8780\n",
      "Epoch 89/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0996 - mean_absolute_error: 0.2358 - val_loss: 1.4986 - val_mean_absolute_error: 0.8128\n",
      "Epoch 90/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.1068 - mean_absolute_error: 0.2428 - val_loss: 1.4507 - val_mean_absolute_error: 0.7877\n",
      "Epoch 91/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0939 - mean_absolute_error: 0.2226 - val_loss: 1.4868 - val_mean_absolute_error: 0.8082\n",
      "Epoch 92/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0982 - mean_absolute_error: 0.2307 - val_loss: 1.5972 - val_mean_absolute_error: 0.8575\n",
      "Epoch 93/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0964 - mean_absolute_error: 0.2286 - val_loss: 1.4617 - val_mean_absolute_error: 0.8222\n",
      "Epoch 94/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0906 - mean_absolute_error: 0.2202 - val_loss: 1.4776 - val_mean_absolute_error: 0.8221\n",
      "Epoch 95/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0978 - mean_absolute_error: 0.2342 - val_loss: 1.4928 - val_mean_absolute_error: 0.7994\n",
      "Epoch 96/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0956 - mean_absolute_error: 0.2286 - val_loss: 1.4682 - val_mean_absolute_error: 0.7935\n",
      "Epoch 97/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0935 - mean_absolute_error: 0.2253 - val_loss: 1.6236 - val_mean_absolute_error: 0.8641\n",
      "Epoch 98/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0831 - mean_absolute_error: 0.2121 - val_loss: 1.4023 - val_mean_absolute_error: 0.7945\n",
      "Epoch 99/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0834 - mean_absolute_error: 0.2119 - val_loss: 1.4166 - val_mean_absolute_error: 0.7907\n",
      "Epoch 100/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.1015 - mean_absolute_error: 0.2361 - val_loss: 1.4069 - val_mean_absolute_error: 0.7781\n",
      "Epoch 101/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0864 - mean_absolute_error: 0.2169 - val_loss: 1.4306 - val_mean_absolute_error: 0.8197\n",
      "Epoch 102/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0852 - mean_absolute_error: 0.2127 - val_loss: 1.4879 - val_mean_absolute_error: 0.8185\n",
      "Epoch 103/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0775 - mean_absolute_error: 0.2041 - val_loss: 1.3824 - val_mean_absolute_error: 0.7686\n",
      "Epoch 104/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0785 - mean_absolute_error: 0.2070 - val_loss: 1.4116 - val_mean_absolute_error: 0.7753\n",
      "Epoch 105/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0792 - mean_absolute_error: 0.2073 - val_loss: 1.4464 - val_mean_absolute_error: 0.7918\n",
      "Epoch 106/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0886 - mean_absolute_error: 0.2120 - val_loss: 1.4952 - val_mean_absolute_error: 0.8204\n",
      "Epoch 107/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0794 - mean_absolute_error: 0.2050 - val_loss: 1.5331 - val_mean_absolute_error: 0.8328\n",
      "Epoch 108/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0800 - mean_absolute_error: 0.2080 - val_loss: 1.4124 - val_mean_absolute_error: 0.8042\n",
      "Epoch 109/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0748 - mean_absolute_error: 0.2017 - val_loss: 1.4036 - val_mean_absolute_error: 0.7759\n",
      "Epoch 110/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0654 - mean_absolute_error: 0.1869 - val_loss: 1.4379 - val_mean_absolute_error: 0.7900\n",
      "Epoch 111/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0712 - mean_absolute_error: 0.1950 - val_loss: 1.4407 - val_mean_absolute_error: 0.7968\n",
      "Epoch 112/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0695 - mean_absolute_error: 0.1942 - val_loss: 1.3785 - val_mean_absolute_error: 0.7698\n",
      "Epoch 113/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0797 - mean_absolute_error: 0.2050 - val_loss: 1.3911 - val_mean_absolute_error: 0.7689\n",
      "Epoch 114/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0699 - mean_absolute_error: 0.1922 - val_loss: 1.3587 - val_mean_absolute_error: 0.7644\n",
      "Epoch 115/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0657 - mean_absolute_error: 0.1850 - val_loss: 1.3589 - val_mean_absolute_error: 0.7678\n",
      "Epoch 116/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0732 - mean_absolute_error: 0.1944 - val_loss: 1.3937 - val_mean_absolute_error: 0.7772\n",
      "Epoch 117/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0715 - mean_absolute_error: 0.1948 - val_loss: 1.3334 - val_mean_absolute_error: 0.7642\n",
      "Epoch 118/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0721 - mean_absolute_error: 0.1961 - val_loss: 1.5090 - val_mean_absolute_error: 0.8182\n",
      "Epoch 119/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0732 - mean_absolute_error: 0.1950 - val_loss: 1.3719 - val_mean_absolute_error: 0.7588\n",
      "Epoch 120/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0685 - mean_absolute_error: 0.1908 - val_loss: 1.3562 - val_mean_absolute_error: 0.7602\n",
      "Epoch 121/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0632 - mean_absolute_error: 0.1810 - val_loss: 1.4285 - val_mean_absolute_error: 0.7857\n",
      "Epoch 122/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0594 - mean_absolute_error: 0.1773 - val_loss: 1.3541 - val_mean_absolute_error: 0.7634\n",
      "Epoch 123/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0611 - mean_absolute_error: 0.1779 - val_loss: 1.3990 - val_mean_absolute_error: 0.7942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0726 - mean_absolute_error: 0.1949 - val_loss: 1.3634 - val_mean_absolute_error: 0.7763\n",
      "Epoch 125/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0643 - mean_absolute_error: 0.1854 - val_loss: 1.3644 - val_mean_absolute_error: 0.7555\n",
      "Epoch 126/200\n",
      "16226/16226 [==============================] - 1s 78us/step - loss: 0.0700 - mean_absolute_error: 0.1932 - val_loss: 1.4060 - val_mean_absolute_error: 0.7751\n",
      "Epoch 127/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0625 - mean_absolute_error: 0.1809 - val_loss: 1.4006 - val_mean_absolute_error: 0.7803\n",
      "Epoch 128/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0565 - mean_absolute_error: 0.1719 - val_loss: 1.3563 - val_mean_absolute_error: 0.7621\n",
      "Epoch 129/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0563 - mean_absolute_error: 0.1713 - val_loss: 1.3654 - val_mean_absolute_error: 0.7734\n",
      "Epoch 130/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0634 - mean_absolute_error: 0.1852 - val_loss: 1.3708 - val_mean_absolute_error: 0.7619\n",
      "Epoch 131/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0586 - mean_absolute_error: 0.1763 - val_loss: 1.3400 - val_mean_absolute_error: 0.7667\n",
      "Epoch 132/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0605 - mean_absolute_error: 0.1787 - val_loss: 1.4739 - val_mean_absolute_error: 0.8097\n",
      "Epoch 133/200\n",
      "16226/16226 [==============================] - 1s 75us/step - loss: 0.0510 - mean_absolute_error: 0.1653 - val_loss: 1.3639 - val_mean_absolute_error: 0.7554\n",
      "Epoch 134/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0523 - mean_absolute_error: 0.1660 - val_loss: 1.3764 - val_mean_absolute_error: 0.7610\n",
      "Epoch 135/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0611 - mean_absolute_error: 0.1781 - val_loss: 1.4148 - val_mean_absolute_error: 0.7922\n",
      "Epoch 136/200\n",
      "16226/16226 [==============================] - 1s 77us/step - loss: 0.0592 - mean_absolute_error: 0.1772 - val_loss: 1.3619 - val_mean_absolute_error: 0.7820\n",
      "Epoch 137/200\n",
      "16226/16226 [==============================] - 1s 76us/step - loss: 0.0582 - mean_absolute_error: 0.1769 - val_loss: 1.3391 - val_mean_absolute_error: 0.7501\n",
      "(20283, 2000) (20283,)\n",
      "Train on 16226 samples, validate on 4057 samples\n",
      "Epoch 1/200\n",
      "16226/16226 [==============================] - 2s 96us/step - loss: 34.5366 - mean_absolute_error: 3.1741 - val_loss: 8.8065 - val_mean_absolute_error: 2.2980\n",
      "Epoch 2/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 2.8047 - mean_absolute_error: 1.2805 - val_loss: 4.5369 - val_mean_absolute_error: 1.5473\n",
      "Epoch 3/200\n",
      "16226/16226 [==============================] - 1s 88us/step - loss: 1.7027 - mean_absolute_error: 1.0086 - val_loss: 3.9874 - val_mean_absolute_error: 1.4468\n",
      "Epoch 4/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 1.4884 - mean_absolute_error: 0.9407 - val_loss: 4.2770 - val_mean_absolute_error: 1.5081\n",
      "Epoch 5/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 1.4937 - mean_absolute_error: 0.9340 - val_loss: 3.4451 - val_mean_absolute_error: 1.3095\n",
      "Epoch 6/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 1.0811 - mean_absolute_error: 0.8004 - val_loss: 3.3430 - val_mean_absolute_error: 1.2994\n",
      "Epoch 7/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 1.1780 - mean_absolute_error: 0.8408 - val_loss: 4.1259 - val_mean_absolute_error: 1.5472\n",
      "Epoch 8/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 1.1344 - mean_absolute_error: 0.8161 - val_loss: 3.8069 - val_mean_absolute_error: 1.4109\n",
      "Epoch 9/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 1.0452 - mean_absolute_error: 0.7835 - val_loss: 3.5790 - val_mean_absolute_error: 1.3741\n",
      "Epoch 10/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.9134 - mean_absolute_error: 0.7295 - val_loss: 3.0564 - val_mean_absolute_error: 1.2420\n",
      "Epoch 11/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.7939 - mean_absolute_error: 0.6858 - val_loss: 2.8532 - val_mean_absolute_error: 1.1767\n",
      "Epoch 12/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.7885 - mean_absolute_error: 0.6836 - val_loss: 2.9982 - val_mean_absolute_error: 1.2206\n",
      "Epoch 13/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.7553 - mean_absolute_error: 0.6663 - val_loss: 3.6181 - val_mean_absolute_error: 1.4019\n",
      "Epoch 14/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.7084 - mean_absolute_error: 0.6478 - val_loss: 3.4544 - val_mean_absolute_error: 1.4277\n",
      "Epoch 15/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.8506 - mean_absolute_error: 0.6874 - val_loss: 3.4117 - val_mean_absolute_error: 1.3381\n",
      "Epoch 16/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.7192 - mean_absolute_error: 0.6422 - val_loss: 2.9426 - val_mean_absolute_error: 1.2051\n",
      "Epoch 17/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.5355 - mean_absolute_error: 0.5614 - val_loss: 2.6103 - val_mean_absolute_error: 1.1227\n",
      "Epoch 18/200\n",
      "16226/16226 [==============================] - 1s 85us/step - loss: 0.5683 - mean_absolute_error: 0.5776 - val_loss: 2.4375 - val_mean_absolute_error: 1.0707\n",
      "Epoch 19/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.5255 - mean_absolute_error: 0.5545 - val_loss: 2.4417 - val_mean_absolute_error: 1.0969\n",
      "Epoch 20/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.5434 - mean_absolute_error: 0.5648 - val_loss: 2.5350 - val_mean_absolute_error: 1.1169\n",
      "Epoch 21/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.5223 - mean_absolute_error: 0.5541 - val_loss: 2.6160 - val_mean_absolute_error: 1.1240\n",
      "Epoch 22/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.4557 - mean_absolute_error: 0.5137 - val_loss: 2.6233 - val_mean_absolute_error: 1.1438\n",
      "Epoch 23/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.4880 - mean_absolute_error: 0.5329 - val_loss: 2.4246 - val_mean_absolute_error: 1.0778\n",
      "Epoch 24/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.4507 - mean_absolute_error: 0.5140 - val_loss: 2.3918 - val_mean_absolute_error: 1.1023\n",
      "Epoch 25/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.4552 - mean_absolute_error: 0.5116 - val_loss: 2.3374 - val_mean_absolute_error: 1.0986\n",
      "Epoch 26/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.4355 - mean_absolute_error: 0.5060 - val_loss: 2.1750 - val_mean_absolute_error: 1.0222\n",
      "Epoch 27/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.3854 - mean_absolute_error: 0.4750 - val_loss: 2.2062 - val_mean_absolute_error: 1.0389\n",
      "Epoch 28/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.3785 - mean_absolute_error: 0.4688 - val_loss: 2.2292 - val_mean_absolute_error: 1.0580\n",
      "Epoch 29/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.3657 - mean_absolute_error: 0.4595 - val_loss: 2.1570 - val_mean_absolute_error: 0.9866\n",
      "Epoch 30/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.3551 - mean_absolute_error: 0.4534 - val_loss: 2.6361 - val_mean_absolute_error: 1.1581\n",
      "Epoch 31/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.3792 - mean_absolute_error: 0.4692 - val_loss: 2.4326 - val_mean_absolute_error: 1.0807\n",
      "Epoch 32/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.3515 - mean_absolute_error: 0.4521 - val_loss: 2.4744 - val_mean_absolute_error: 1.1133\n",
      "Epoch 33/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.3426 - mean_absolute_error: 0.4488 - val_loss: 2.4720 - val_mean_absolute_error: 1.0921\n",
      "Epoch 34/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.3059 - mean_absolute_error: 0.4207 - val_loss: 2.0837 - val_mean_absolute_error: 0.9974\n",
      "Epoch 35/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.3361 - mean_absolute_error: 0.4386 - val_loss: 1.9924 - val_mean_absolute_error: 0.9617\n",
      "Epoch 36/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.2766 - mean_absolute_error: 0.4017 - val_loss: 1.9873 - val_mean_absolute_error: 1.0007\n",
      "Epoch 37/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.2800 - mean_absolute_error: 0.4009 - val_loss: 2.1857 - val_mean_absolute_error: 1.0318\n",
      "Epoch 38/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.3339 - mean_absolute_error: 0.4376 - val_loss: 2.1386 - val_mean_absolute_error: 0.9957\n",
      "Epoch 39/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.2719 - mean_absolute_error: 0.3959 - val_loss: 1.9616 - val_mean_absolute_error: 0.9484\n",
      "Epoch 40/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.2657 - mean_absolute_error: 0.3926 - val_loss: 2.1726 - val_mean_absolute_error: 1.0509\n",
      "Epoch 41/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.2740 - mean_absolute_error: 0.3984 - val_loss: 2.2845 - val_mean_absolute_error: 1.0567\n",
      "Epoch 42/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.2652 - mean_absolute_error: 0.3897 - val_loss: 2.0758 - val_mean_absolute_error: 0.9821\n",
      "Epoch 43/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.2781 - mean_absolute_error: 0.3976 - val_loss: 2.0629 - val_mean_absolute_error: 0.9796\n",
      "Epoch 44/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.2313 - mean_absolute_error: 0.3648 - val_loss: 1.8611 - val_mean_absolute_error: 0.9608\n",
      "Epoch 45/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.2556 - mean_absolute_error: 0.3817 - val_loss: 2.0972 - val_mean_absolute_error: 1.0561\n",
      "Epoch 46/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.3320 - mean_absolute_error: 0.4183 - val_loss: 1.8870 - val_mean_absolute_error: 0.9733\n",
      "Epoch 47/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.1787 - mean_absolute_error: 0.3188 - val_loss: 2.0942 - val_mean_absolute_error: 0.9868\n",
      "Epoch 48/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.2012 - mean_absolute_error: 0.3382 - val_loss: 1.9250 - val_mean_absolute_error: 0.9443\n",
      "Epoch 49/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.2080 - mean_absolute_error: 0.3475 - val_loss: 1.7794 - val_mean_absolute_error: 0.9021\n",
      "Epoch 50/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.2041 - mean_absolute_error: 0.3437 - val_loss: 1.8836 - val_mean_absolute_error: 0.9330\n",
      "Epoch 51/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.2072 - mean_absolute_error: 0.3432 - val_loss: 1.7687 - val_mean_absolute_error: 0.8848\n",
      "Epoch 52/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.2055 - mean_absolute_error: 0.3414 - val_loss: 1.8131 - val_mean_absolute_error: 0.9067\n",
      "Epoch 53/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.2013 - mean_absolute_error: 0.3426 - val_loss: 1.8178 - val_mean_absolute_error: 0.9141\n",
      "Epoch 54/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.1920 - mean_absolute_error: 0.3294 - val_loss: 1.9208 - val_mean_absolute_error: 0.9988\n",
      "Epoch 55/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1853 - mean_absolute_error: 0.3229 - val_loss: 1.8150 - val_mean_absolute_error: 0.9131\n",
      "Epoch 56/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1725 - mean_absolute_error: 0.3150 - val_loss: 1.6904 - val_mean_absolute_error: 0.8649\n",
      "Epoch 57/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.1983 - mean_absolute_error: 0.3393 - val_loss: 1.8744 - val_mean_absolute_error: 0.9324\n",
      "Epoch 58/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1851 - mean_absolute_error: 0.3242 - val_loss: 1.8094 - val_mean_absolute_error: 0.9139\n",
      "Epoch 59/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1683 - mean_absolute_error: 0.3067 - val_loss: 1.7371 - val_mean_absolute_error: 0.9027\n",
      "Epoch 60/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1743 - mean_absolute_error: 0.3157 - val_loss: 1.7986 - val_mean_absolute_error: 0.9345\n",
      "Epoch 61/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1706 - mean_absolute_error: 0.3099 - val_loss: 1.6943 - val_mean_absolute_error: 0.8702\n",
      "Epoch 62/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1576 - mean_absolute_error: 0.3018 - val_loss: 1.6620 - val_mean_absolute_error: 0.8662\n",
      "Epoch 63/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.1619 - mean_absolute_error: 0.3015 - val_loss: 1.5823 - val_mean_absolute_error: 0.8388\n",
      "Epoch 64/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.1372 - mean_absolute_error: 0.2773 - val_loss: 1.7006 - val_mean_absolute_error: 0.8861\n",
      "Epoch 65/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1649 - mean_absolute_error: 0.3066 - val_loss: 1.7564 - val_mean_absolute_error: 0.8905\n",
      "Epoch 66/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1624 - mean_absolute_error: 0.3017 - val_loss: 1.7165 - val_mean_absolute_error: 0.8999\n",
      "Epoch 67/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.1332 - mean_absolute_error: 0.2744 - val_loss: 1.6580 - val_mean_absolute_error: 0.8772\n",
      "Epoch 68/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.1356 - mean_absolute_error: 0.2750 - val_loss: 1.7238 - val_mean_absolute_error: 0.9028\n",
      "Epoch 69/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1382 - mean_absolute_error: 0.2786 - val_loss: 1.5532 - val_mean_absolute_error: 0.8272\n",
      "Epoch 70/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1315 - mean_absolute_error: 0.2718 - val_loss: 1.6642 - val_mean_absolute_error: 0.9079\n",
      "Epoch 71/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1684 - mean_absolute_error: 0.3095 - val_loss: 1.6303 - val_mean_absolute_error: 0.8763\n",
      "Epoch 72/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1336 - mean_absolute_error: 0.2734 - val_loss: 1.5516 - val_mean_absolute_error: 0.8274\n",
      "Epoch 73/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1229 - mean_absolute_error: 0.2617 - val_loss: 1.7293 - val_mean_absolute_error: 0.9014\n",
      "Epoch 74/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1413 - mean_absolute_error: 0.2802 - val_loss: 1.6233 - val_mean_absolute_error: 0.8495\n",
      "Epoch 75/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.1258 - mean_absolute_error: 0.2651 - val_loss: 1.5324 - val_mean_absolute_error: 0.8218\n",
      "Epoch 76/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1218 - mean_absolute_error: 0.2617 - val_loss: 1.6521 - val_mean_absolute_error: 0.8974\n",
      "Epoch 77/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.1167 - mean_absolute_error: 0.2575 - val_loss: 1.7991 - val_mean_absolute_error: 0.9566\n",
      "Epoch 78/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.1215 - mean_absolute_error: 0.2608 - val_loss: 1.5373 - val_mean_absolute_error: 0.8255\n",
      "Epoch 79/200\n",
      "16226/16226 [==============================] - 1s 85us/step - loss: 0.1114 - mean_absolute_error: 0.2475 - val_loss: 1.4905 - val_mean_absolute_error: 0.8208\n",
      "Epoch 80/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.1232 - mean_absolute_error: 0.2620 - val_loss: 1.5441 - val_mean_absolute_error: 0.8275\n",
      "Epoch 81/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1168 - mean_absolute_error: 0.2554 - val_loss: 1.5506 - val_mean_absolute_error: 0.8452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.1214 - mean_absolute_error: 0.2605 - val_loss: 1.5477 - val_mean_absolute_error: 0.8428\n",
      "Epoch 83/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.1211 - mean_absolute_error: 0.2571 - val_loss: 1.5814 - val_mean_absolute_error: 0.8583\n",
      "Epoch 84/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.1051 - mean_absolute_error: 0.2347 - val_loss: 1.4559 - val_mean_absolute_error: 0.7992\n",
      "Epoch 85/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0978 - mean_absolute_error: 0.2350 - val_loss: 1.5786 - val_mean_absolute_error: 0.8433\n",
      "Epoch 86/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.1082 - mean_absolute_error: 0.2464 - val_loss: 1.5009 - val_mean_absolute_error: 0.8143\n",
      "Epoch 87/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0951 - mean_absolute_error: 0.2302 - val_loss: 1.5272 - val_mean_absolute_error: 0.8380\n",
      "Epoch 88/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.1143 - mean_absolute_error: 0.2514 - val_loss: 1.4385 - val_mean_absolute_error: 0.8009\n",
      "Epoch 89/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0980 - mean_absolute_error: 0.2336 - val_loss: 1.4342 - val_mean_absolute_error: 0.7987\n",
      "Epoch 90/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.1028 - mean_absolute_error: 0.2355 - val_loss: 1.5244 - val_mean_absolute_error: 0.8273\n",
      "Epoch 91/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.1004 - mean_absolute_error: 0.2383 - val_loss: 1.4901 - val_mean_absolute_error: 0.8274\n",
      "Epoch 92/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0984 - mean_absolute_error: 0.2318 - val_loss: 1.4486 - val_mean_absolute_error: 0.7981\n",
      "Epoch 93/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.1015 - mean_absolute_error: 0.2389 - val_loss: 1.5087 - val_mean_absolute_error: 0.8281\n",
      "Epoch 94/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0896 - mean_absolute_error: 0.2216 - val_loss: 1.4531 - val_mean_absolute_error: 0.8003\n",
      "Epoch 95/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0880 - mean_absolute_error: 0.2215 - val_loss: 1.4180 - val_mean_absolute_error: 0.7868\n",
      "Epoch 96/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0885 - mean_absolute_error: 0.2177 - val_loss: 1.4249 - val_mean_absolute_error: 0.7946\n",
      "Epoch 97/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.1006 - mean_absolute_error: 0.2368 - val_loss: 1.4363 - val_mean_absolute_error: 0.7838\n",
      "Epoch 98/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0848 - mean_absolute_error: 0.2167 - val_loss: 1.4104 - val_mean_absolute_error: 0.7817\n",
      "Epoch 99/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0809 - mean_absolute_error: 0.2116 - val_loss: 1.4502 - val_mean_absolute_error: 0.8098\n",
      "Epoch 100/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.1044 - mean_absolute_error: 0.2415 - val_loss: 1.3782 - val_mean_absolute_error: 0.7829\n",
      "Epoch 101/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0812 - mean_absolute_error: 0.2114 - val_loss: 1.4359 - val_mean_absolute_error: 0.7911\n",
      "Epoch 102/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0763 - mean_absolute_error: 0.2065 - val_loss: 1.4719 - val_mean_absolute_error: 0.8109\n",
      "Epoch 103/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0777 - mean_absolute_error: 0.2087 - val_loss: 1.4032 - val_mean_absolute_error: 0.7816\n",
      "Epoch 104/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0915 - mean_absolute_error: 0.2260 - val_loss: 1.3941 - val_mean_absolute_error: 0.7867\n",
      "Epoch 105/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0843 - mean_absolute_error: 0.2139 - val_loss: 1.4562 - val_mean_absolute_error: 0.8126\n",
      "Epoch 106/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0859 - mean_absolute_error: 0.2194 - val_loss: 1.3958 - val_mean_absolute_error: 0.7852\n",
      "Epoch 107/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0695 - mean_absolute_error: 0.1950 - val_loss: 1.4404 - val_mean_absolute_error: 0.7940\n",
      "Epoch 108/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0829 - mean_absolute_error: 0.2137 - val_loss: 1.4997 - val_mean_absolute_error: 0.8455\n",
      "Epoch 109/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0761 - mean_absolute_error: 0.2062 - val_loss: 1.4054 - val_mean_absolute_error: 0.8134\n",
      "Epoch 110/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0801 - mean_absolute_error: 0.2093 - val_loss: 1.4874 - val_mean_absolute_error: 0.8228\n",
      "Epoch 111/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.0775 - mean_absolute_error: 0.2066 - val_loss: 1.4067 - val_mean_absolute_error: 0.7834\n",
      "Epoch 112/200\n",
      "16226/16226 [==============================] - 1s 85us/step - loss: 0.0679 - mean_absolute_error: 0.1944 - val_loss: 1.4016 - val_mean_absolute_error: 0.7725\n",
      "Epoch 113/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0690 - mean_absolute_error: 0.1942 - val_loss: 1.3428 - val_mean_absolute_error: 0.7617\n",
      "Epoch 114/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.0761 - mean_absolute_error: 0.2045 - val_loss: 1.3392 - val_mean_absolute_error: 0.7598\n",
      "Epoch 115/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0701 - mean_absolute_error: 0.1960 - val_loss: 1.4097 - val_mean_absolute_error: 0.7682\n",
      "Epoch 116/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0743 - mean_absolute_error: 0.2030 - val_loss: 1.4065 - val_mean_absolute_error: 0.7966\n",
      "Epoch 117/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.0779 - mean_absolute_error: 0.2045 - val_loss: 1.3227 - val_mean_absolute_error: 0.7569\n",
      "Epoch 118/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.0680 - mean_absolute_error: 0.1912 - val_loss: 1.3358 - val_mean_absolute_error: 0.7573\n",
      "Epoch 119/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0630 - mean_absolute_error: 0.1855 - val_loss: 1.3755 - val_mean_absolute_error: 0.7834\n",
      "Epoch 120/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0641 - mean_absolute_error: 0.1872 - val_loss: 1.3423 - val_mean_absolute_error: 0.7580\n",
      "Epoch 121/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.0735 - mean_absolute_error: 0.2012 - val_loss: 1.3778 - val_mean_absolute_error: 0.7762\n",
      "Epoch 122/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0624 - mean_absolute_error: 0.1833 - val_loss: 1.3663 - val_mean_absolute_error: 0.7708\n",
      "Epoch 123/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.0641 - mean_absolute_error: 0.1845 - val_loss: 1.3217 - val_mean_absolute_error: 0.7654\n",
      "Epoch 124/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.0609 - mean_absolute_error: 0.1825 - val_loss: 1.3785 - val_mean_absolute_error: 0.7690\n",
      "Epoch 125/200\n",
      "16226/16226 [==============================] - ETA: 0s - loss: 0.0578 - mean_absolute_error: 0.175 - 1s 85us/step - loss: 0.0580 - mean_absolute_error: 0.1754 - val_loss: 1.3465 - val_mean_absolute_error: 0.7557\n",
      "Epoch 126/200\n",
      "16226/16226 [==============================] - 2s 96us/step - loss: 0.0661 - mean_absolute_error: 0.1897 - val_loss: 1.3982 - val_mean_absolute_error: 0.7923\n",
      "Epoch 127/200\n",
      "16226/16226 [==============================] - 1s 90us/step - loss: 0.0650 - mean_absolute_error: 0.1881 - val_loss: 1.3364 - val_mean_absolute_error: 0.7653\n",
      "Epoch 128/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0687 - mean_absolute_error: 0.1937 - val_loss: 1.3257 - val_mean_absolute_error: 0.7559\n",
      "Epoch 129/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0619 - mean_absolute_error: 0.1831 - val_loss: 1.3161 - val_mean_absolute_error: 0.7483\n",
      "Epoch 130/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0571 - mean_absolute_error: 0.1766 - val_loss: 1.2970 - val_mean_absolute_error: 0.7401\n",
      "Epoch 131/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0600 - mean_absolute_error: 0.1817 - val_loss: 1.3634 - val_mean_absolute_error: 0.7641\n",
      "Epoch 132/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0704 - mean_absolute_error: 0.1898 - val_loss: 1.3437 - val_mean_absolute_error: 0.7653\n",
      "Epoch 133/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0543 - mean_absolute_error: 0.1711 - val_loss: 1.3879 - val_mean_absolute_error: 0.7821\n",
      "Epoch 134/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0547 - mean_absolute_error: 0.1727 - val_loss: 1.3237 - val_mean_absolute_error: 0.7497\n",
      "Epoch 135/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0610 - mean_absolute_error: 0.1818 - val_loss: 1.3050 - val_mean_absolute_error: 0.7421\n",
      "Epoch 136/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0559 - mean_absolute_error: 0.1736 - val_loss: 1.3963 - val_mean_absolute_error: 0.7955\n",
      "Epoch 137/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0635 - mean_absolute_error: 0.1822 - val_loss: 1.2833 - val_mean_absolute_error: 0.7306\n",
      "Epoch 138/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0539 - mean_absolute_error: 0.1708 - val_loss: 1.3255 - val_mean_absolute_error: 0.7627\n",
      "Epoch 139/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0551 - mean_absolute_error: 0.1725 - val_loss: 1.3504 - val_mean_absolute_error: 0.7521\n",
      "Epoch 140/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.0567 - mean_absolute_error: 0.1756 - val_loss: 1.3690 - val_mean_absolute_error: 0.7896\n",
      "Epoch 141/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0528 - mean_absolute_error: 0.1688 - val_loss: 1.2753 - val_mean_absolute_error: 0.7383\n",
      "Epoch 142/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0504 - mean_absolute_error: 0.1634 - val_loss: 1.2985 - val_mean_absolute_error: 0.7507\n",
      "Epoch 143/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0551 - mean_absolute_error: 0.1728 - val_loss: 1.2792 - val_mean_absolute_error: 0.7286\n",
      "Epoch 144/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.0472 - mean_absolute_error: 0.1590 - val_loss: 1.2886 - val_mean_absolute_error: 0.7432\n",
      "Epoch 145/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0525 - mean_absolute_error: 0.1695 - val_loss: 1.3110 - val_mean_absolute_error: 0.7507\n",
      "Epoch 146/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0586 - mean_absolute_error: 0.1738 - val_loss: 1.3050 - val_mean_absolute_error: 0.7407\n",
      "Epoch 147/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0510 - mean_absolute_error: 0.1659 - val_loss: 1.3404 - val_mean_absolute_error: 0.7677\n",
      "Epoch 148/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.0590 - mean_absolute_error: 0.1731 - val_loss: 1.3117 - val_mean_absolute_error: 0.7440\n",
      "Epoch 149/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0498 - mean_absolute_error: 0.1624 - val_loss: 1.2755 - val_mean_absolute_error: 0.7318\n",
      "Epoch 150/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.0495 - mean_absolute_error: 0.1618 - val_loss: 1.2893 - val_mean_absolute_error: 0.7318\n",
      "Epoch 151/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0514 - mean_absolute_error: 0.1626 - val_loss: 1.3451 - val_mean_absolute_error: 0.7695\n",
      "Epoch 152/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.0533 - mean_absolute_error: 0.1718 - val_loss: 1.3049 - val_mean_absolute_error: 0.7483\n",
      "Epoch 153/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0493 - mean_absolute_error: 0.1646 - val_loss: 1.2828 - val_mean_absolute_error: 0.7351\n",
      "Epoch 154/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0435 - mean_absolute_error: 0.1545 - val_loss: 1.2896 - val_mean_absolute_error: 0.7375\n",
      "Epoch 155/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0451 - mean_absolute_error: 0.1552 - val_loss: 1.3351 - val_mean_absolute_error: 0.7717\n",
      "Epoch 156/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0494 - mean_absolute_error: 0.1609 - val_loss: 1.2906 - val_mean_absolute_error: 0.7350\n",
      "Epoch 157/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0479 - mean_absolute_error: 0.1592 - val_loss: 1.3020 - val_mean_absolute_error: 0.7434\n",
      "Epoch 158/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0433 - mean_absolute_error: 0.1535 - val_loss: 1.3011 - val_mean_absolute_error: 0.7490\n",
      "Epoch 159/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.0466 - mean_absolute_error: 0.1565 - val_loss: 1.2984 - val_mean_absolute_error: 0.7383\n",
      "Epoch 160/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0591 - mean_absolute_error: 0.1755 - val_loss: 1.3120 - val_mean_absolute_error: 0.7440\n",
      "Epoch 161/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0462 - mean_absolute_error: 0.1552 - val_loss: 1.2518 - val_mean_absolute_error: 0.7239\n",
      "Epoch 162/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0450 - mean_absolute_error: 0.1548 - val_loss: 1.3104 - val_mean_absolute_error: 0.7574\n",
      "Epoch 163/200\n",
      "16226/16226 [==============================] - 1s 85us/step - loss: 0.0472 - mean_absolute_error: 0.1582 - val_loss: 1.3179 - val_mean_absolute_error: 0.7580\n",
      "Epoch 164/200\n",
      "16226/16226 [==============================] - 1s 81us/step - loss: 0.0513 - mean_absolute_error: 0.1648 - val_loss: 1.2750 - val_mean_absolute_error: 0.7312\n",
      "Epoch 165/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0405 - mean_absolute_error: 0.1479 - val_loss: 1.2728 - val_mean_absolute_error: 0.7308\n",
      "Epoch 166/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0446 - mean_absolute_error: 0.1503 - val_loss: 1.3002 - val_mean_absolute_error: 0.7503\n",
      "Epoch 167/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.0492 - mean_absolute_error: 0.1614 - val_loss: 1.3002 - val_mean_absolute_error: 0.7536\n",
      "Epoch 168/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.0418 - mean_absolute_error: 0.1487 - val_loss: 1.2731 - val_mean_absolute_error: 0.7391\n",
      "Epoch 169/200\n",
      "16226/16226 [==============================] - 1s 85us/step - loss: 0.0453 - mean_absolute_error: 0.1542 - val_loss: 1.2616 - val_mean_absolute_error: 0.7241\n",
      "Epoch 170/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.0430 - mean_absolute_error: 0.1502 - val_loss: 1.3238 - val_mean_absolute_error: 0.7681\n",
      "Epoch 171/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.0406 - mean_absolute_error: 0.1463 - val_loss: 1.2787 - val_mean_absolute_error: 0.7236\n",
      "Epoch 172/200\n",
      "16226/16226 [==============================] - 1s 83us/step - loss: 0.0455 - mean_absolute_error: 0.1576 - val_loss: 1.3025 - val_mean_absolute_error: 0.7323\n",
      "Epoch 173/200\n",
      "16226/16226 [==============================] - 1s 86us/step - loss: 0.0412 - mean_absolute_error: 0.1453 - val_loss: 1.2999 - val_mean_absolute_error: 0.7321\n",
      "Epoch 174/200\n",
      "16226/16226 [==============================] - 1s 85us/step - loss: 0.0540 - mean_absolute_error: 0.1681 - val_loss: 1.2742 - val_mean_absolute_error: 0.7351\n",
      "Epoch 175/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.0396 - mean_absolute_error: 0.1446 - val_loss: 1.2817 - val_mean_absolute_error: 0.7277\n",
      "Epoch 176/200\n",
      "16226/16226 [==============================] - 1s 84us/step - loss: 0.0396 - mean_absolute_error: 0.1444 - val_loss: 1.2741 - val_mean_absolute_error: 0.7222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/200\n",
      "16226/16226 [==============================] - 1s 82us/step - loss: 0.0375 - mean_absolute_error: 0.1423 - val_loss: 1.3043 - val_mean_absolute_error: 0.7509\n",
      "Epoch 178/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0390 - mean_absolute_error: 0.1436 - val_loss: 1.2672 - val_mean_absolute_error: 0.7177\n",
      "Epoch 179/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0415 - mean_absolute_error: 0.1466 - val_loss: 1.2655 - val_mean_absolute_error: 0.7185\n",
      "Epoch 180/200\n",
      "16226/16226 [==============================] - 1s 79us/step - loss: 0.0420 - mean_absolute_error: 0.1506 - val_loss: 1.3539 - val_mean_absolute_error: 0.7742\n",
      "Epoch 181/200\n",
      "16226/16226 [==============================] - 1s 80us/step - loss: 0.0415 - mean_absolute_error: 0.1513 - val_loss: 1.2706 - val_mean_absolute_error: 0.7231\n"
     ]
    }
   ],
   "source": [
    "# For now training on 2000 features, memory errors otherwise. Change when feature selection on point\n",
    "models = create_k_models(X_train[:, : 2000], y_train, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = k_predictions(X_test[:, :2000], models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6512500940246324"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The results are not concluent, we propose several reasons :\n",
    "- We used only 2000 features for memory usage reasons.\n",
    "- Training on the full set when using deep neural networks may be more interesting than training several models and ask for the average of the predictions. When using less data, the models are more probe to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Dropout layers\n",
    "Dropout layers have been reported in the litterature to bring good results to avoid overfitting in combination with L2 regularizers http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf (Srivastava et al.). It will train the model while keeping only a neuron active with probability p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dropout() got an unexpected keyword argument 'activation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-89512d890d4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m model = tf.keras.Sequential([\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Assuming each layer represent a link between particules, we begin with 4 layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: dropout() got an unexpected keyword argument 'activation'"
     ]
    }
   ],
   "source": [
    "# Clean up as much memory as possible before starting\n",
    "garbage_collection()\n",
    "\n",
    "# Prepare model \n",
    "model = tf.keras.Sequential([\n",
    "    # Assuming each layer represent a link between particules, we begin with 4 layers\n",
    "    tf.layers.dropout(64, rate=0.1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.5)),\n",
    "    tf.layers.dropout(64, rate=0.1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.5)),\n",
    "    tf.layers.dropout(64, rate=0.1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.5)),\n",
    "    tf.layers.dropout(64, rate=0.1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.5)),\n",
    "    # Last layer represent the electromagnetic shielding, our prediction\n",
    "    layers.Dense(1, activation='relu')])\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='mse',\n",
    "              # Note that the accuracy metric is useless but it doesn't compile when trying to use RMSE.\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train the model on our data\n",
    "# Number of epochs the network should run through\n",
    "EPOCHS = 100\n",
    "# Size of the batch for optimization\n",
    "BATCH_SIZE = 32\n",
    "# Set up validation split\n",
    "VALIDATION_SPLIT = 0.2\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "# This will avoid overfitting\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split = VALIDATION_SPLIT, \\\n",
    "          callbacks=[early_stop])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of feature selection matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_red = np.load(DATA_FOLDER + \"feature_mat_radial_compression_normalized_red.npy\")\n",
    "y = np.load(DATA_FOLDER + \"CSD500-r_train-H_total.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_size = int(len(X_red) * TRAIN_SET_PERC)\n",
    "X_train_red = X_red[: train_set_size]\n",
    "X_test_red = X_red[train_set_size:]\n",
    "y_train = y[: train_set_size]\n",
    "y_test = y[train_set_size :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_red: \" + str(X_red.shape))\n",
    "print(\"y: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up as much memory as possible before starting\n",
    "garbage_collection()\n",
    "\n",
    "# Prepare model \n",
    "model = tf.keras.Sequential([\n",
    "    # Number of layers and neurons doesn't really matter, we need as much as possible.\n",
    "    # We well take care of overfitting with regularizers.\n",
    "    # We chose relu activation (relative usual choice when working on regression)\n",
    "    # We add L2 regularizers on hidden layers to avoid overfitting the data. Threshold should be tuned.\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    # Last layer represent the electromagnetic shielding, our prediction\n",
    "    layers.Dense(1, activation='relu')])\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='mse',\n",
    "              # Note that the accuracy metric is useless but it doesn't compile when trying to use RMSE.\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.1\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "history = model.fit(X_train_red, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split = VALIDATION_SPLIT, \\\n",
    "          callbacks=[early_stop])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
