{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating data using deep learning\n",
    "\n",
    "Note that for this approach we based ourselves on the Standford lecture notes on convolutional neural networks for visual recognition.\n",
    "\n",
    "This lecture is open source and can be found on http://cs231n.github.io/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "# Clean up the memory\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data/\"\n",
    "SESSION_FOLDER = \"session/\"\n",
    "\n",
    "TRAIN_SET_PERC = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_session(filename, nb_iterations = -1):\n",
    "    '''\n",
    "    Saves a seesion in a file with the given filename\n",
    "    A number of iterations can also be given \n",
    "    :param : String\n",
    "    :param : int\n",
    "    '''\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    if nb_iterations >= 0:\n",
    "        saver.save(sess, SESSION_FOLDER + filename, global_step=nb_iterations)\n",
    "    else:\n",
    "        saver.save(sess, SESSION_FOLDER + filename)\n",
    "        \n",
    "    print(\"Session saved with filename {}.\".format(filename))\n",
    "    \n",
    "def restore_session(filename):\n",
    "    '''\n",
    "    Restores a tensorflow session stored in the given filename\n",
    "    After the call of this function, the sessios's variable will be available again\n",
    "    :param : String\n",
    "    '''\n",
    "    with tf.Session() as sess:\n",
    "        new_saver = tf.train.import_meta_graph(SESSION_FOLDER + filename + '.meta')\n",
    "        new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "        \n",
    "def garbage_collection():\n",
    "    '''\n",
    "    Calls garbage collection to clean unused memory\n",
    "    '''\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred, real, loop = True):\n",
    "    '''\n",
    "    Computes RMSE between predictions and real values\n",
    "    :param : float[]\n",
    "    :param : float[]\n",
    "    :return : float\n",
    "    '''\n",
    "    if len(pred) != len(real):\n",
    "        print(\"RMSE Error : Predictions and real values arrays do not have the same length, aborting.\")\n",
    "        return None\n",
    "    \n",
    "    if loop:\n",
    "        mse = 0\n",
    "        for i in range(len(pred)):\n",
    "            mse += (pred[i] - real[i])**2\n",
    "        return math.sqrt(2*mse/len(pred))\n",
    "    else:\n",
    "        # The creation of the array may produce memory error\n",
    "        err = pred - real\n",
    "        mse = err.T @ err\n",
    "        return math.sqrt(2 * mse / len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    '''\n",
    "    Plots the history of the training error\n",
    "    Usefull \n",
    "    '''\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [1000$]')\n",
    "    plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),\n",
    "           label='Train Loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n",
    "           label = 'Val loss')\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use non normalized feature matrix\n",
    "# For now best results are given with this one\n",
    "#X = np.load(DATA_FOLDER + \"feature_mat_radial_compression.npy\")\n",
    "\n",
    "# Use normalized feature matrix\n",
    "################################################################################################\n",
    "# Careful                                                                                      #\n",
    "# Normally, normalisation should be done one each train/val/test matrices. It is not done here #\n",
    "################################################################################################\n",
    "X = np.load(DATA_FOLDER + \"feature_mat_radial_compression_normalized.npy\")\n",
    "y = np.load(DATA_FOLDER + \"CSD500-r_train-H_total.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test set\n",
    "\n",
    "train_set_size = int(len(X) * TRAIN_SET_PERC)\n",
    "X_train = X[: train_set_size]\n",
    "X_test = X[train_set_size:]\n",
    "y_train = y[: train_set_size]\n",
    "y_test = y[train_set_size :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (30049, 15961)\n",
      "y: (30049,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \" + str(X.shape))\n",
    "print(\"y: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single neural network model approach\n",
    "\n",
    "First we will do a single model approach, the goal is to see quickly how we can build a model using neural networks and how well it does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up as much memory as possible before starting\n",
    "garbage_collection()\n",
    "\n",
    "# Prepare model \n",
    "model = tf.keras.Sequential([\n",
    "    # Assuming each layer represent a link between particules, we begin with 4 layers\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    # Last layer represent the electromagnetic shielding, our prediction\n",
    "    layers.Dense(1, activation='relu')])\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='mse',\n",
    "              # Note that the accuracy metric is useless but it doesn't compile when trying to use RMSE.\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21635 samples, validate on 5409 samples\n",
      "Epoch 1/200\n",
      "21635/21635 [==============================] - 7s 345us/step - loss: 19.1277 - mean_absolute_error: 2.5579 - val_loss: 5.3781 - val_mean_absolute_error: 1.6841\n",
      "Epoch 2/200\n",
      "21635/21635 [==============================] - 6s 289us/step - loss: 2.8471 - mean_absolute_error: 1.1796 - val_loss: 4.3331 - val_mean_absolute_error: 1.4901\n",
      "Epoch 3/200\n",
      "21635/21635 [==============================] - 6s 278us/step - loss: 1.8293 - mean_absolute_error: 0.9594 - val_loss: 3.9719 - val_mean_absolute_error: 1.4241\n",
      "Epoch 4/200\n",
      "21635/21635 [==============================] - 6s 289us/step - loss: 1.5047 - mean_absolute_error: 0.8830 - val_loss: 3.3000 - val_mean_absolute_error: 1.2985\n",
      "Epoch 5/200\n",
      "21635/21635 [==============================] - 6s 293us/step - loss: 1.3501 - mean_absolute_error: 0.8496 - val_loss: 3.4294 - val_mean_absolute_error: 1.3413\n",
      "Epoch 6/200\n",
      "21635/21635 [==============================] - 6s 294us/step - loss: 1.2400 - mean_absolute_error: 0.8098 - val_loss: 3.5140 - val_mean_absolute_error: 1.4062\n",
      "Epoch 7/200\n",
      "21635/21635 [==============================] - 6s 296us/step - loss: 1.1653 - mean_absolute_error: 0.7983 - val_loss: 2.7546 - val_mean_absolute_error: 1.1687\n",
      "Epoch 8/200\n",
      "21635/21635 [==============================] - 6s 297us/step - loss: 1.0437 - mean_absolute_error: 0.7541 - val_loss: 2.8221 - val_mean_absolute_error: 1.1915\n",
      "Epoch 9/200\n",
      "21635/21635 [==============================] - 6s 291us/step - loss: 0.8735 - mean_absolute_error: 0.6934 - val_loss: 2.4222 - val_mean_absolute_error: 1.1060\n",
      "Epoch 10/200\n",
      "21635/21635 [==============================] - 6s 295us/step - loss: 0.8121 - mean_absolute_error: 0.6692 - val_loss: 2.3589 - val_mean_absolute_error: 1.0968\n",
      "Epoch 11/200\n",
      "21635/21635 [==============================] - 7s 312us/step - loss: 0.7983 - mean_absolute_error: 0.6678 - val_loss: 2.4159 - val_mean_absolute_error: 1.1088\n",
      "Epoch 12/200\n",
      "21635/21635 [==============================] - 7s 338us/step - loss: 0.6692 - mean_absolute_error: 0.6059 - val_loss: 2.6330 - val_mean_absolute_error: 1.2111\n",
      "Epoch 13/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.7320 - mean_absolute_error: 0.6181 - val_loss: 2.1769 - val_mean_absolute_error: 1.0544\n",
      "Epoch 14/200\n",
      "21635/21635 [==============================] - 6s 269us/step - loss: 0.4956 - mean_absolute_error: 0.5328 - val_loss: 2.0948 - val_mean_absolute_error: 1.0104\n",
      "Epoch 15/200\n",
      "21635/21635 [==============================] - 6s 262us/step - loss: 0.4887 - mean_absolute_error: 0.5264 - val_loss: 1.8948 - val_mean_absolute_error: 0.9561\n",
      "Epoch 16/200\n",
      "21635/21635 [==============================] - 6s 290us/step - loss: 0.5510 - mean_absolute_error: 0.5605 - val_loss: 2.0937 - val_mean_absolute_error: 1.0168\n",
      "Epoch 17/200\n",
      "21635/21635 [==============================] - 6s 297us/step - loss: 0.5434 - mean_absolute_error: 0.5534 - val_loss: 1.8903 - val_mean_absolute_error: 0.9568\n",
      "Epoch 18/200\n",
      "21635/21635 [==============================] - 6s 291us/step - loss: 0.3712 - mean_absolute_error: 0.4589 - val_loss: 2.2882 - val_mean_absolute_error: 1.0775\n",
      "Epoch 19/200\n",
      "21635/21635 [==============================] - 6s 293us/step - loss: 0.3870 - mean_absolute_error: 0.4692 - val_loss: 1.8882 - val_mean_absolute_error: 1.0073\n",
      "Epoch 20/200\n",
      "21635/21635 [==============================] - 6s 296us/step - loss: 0.3951 - mean_absolute_error: 0.4719 - val_loss: 2.2719 - val_mean_absolute_error: 1.1235\n",
      "Epoch 21/200\n",
      "21635/21635 [==============================] - 6s 286us/step - loss: 0.3819 - mean_absolute_error: 0.4658 - val_loss: 1.7368 - val_mean_absolute_error: 0.9161\n",
      "Epoch 22/200\n",
      "21635/21635 [==============================] - 6s 298us/step - loss: 0.3297 - mean_absolute_error: 0.4289 - val_loss: 1.8269 - val_mean_absolute_error: 1.0117\n",
      "Epoch 23/200\n",
      "21635/21635 [==============================] - 6s 288us/step - loss: 0.3454 - mean_absolute_error: 0.4382 - val_loss: 1.6953 - val_mean_absolute_error: 0.9355\n",
      "Epoch 24/200\n",
      "21635/21635 [==============================] - 6s 288us/step - loss: 0.3485 - mean_absolute_error: 0.4347 - val_loss: 1.5203 - val_mean_absolute_error: 0.8517\n",
      "Epoch 25/200\n",
      "21635/21635 [==============================] - 6s 293us/step - loss: 0.2523 - mean_absolute_error: 0.3840 - val_loss: 1.7112 - val_mean_absolute_error: 0.9086\n",
      "Epoch 26/200\n",
      "21635/21635 [==============================] - 6s 300us/step - loss: 0.2732 - mean_absolute_error: 0.3893 - val_loss: 1.5756 - val_mean_absolute_error: 0.8504\n",
      "Epoch 27/200\n",
      "21635/21635 [==============================] - 6s 297us/step - loss: 0.2334 - mean_absolute_error: 0.3670 - val_loss: 1.6945 - val_mean_absolute_error: 0.9106\n",
      "Epoch 28/200\n",
      "21635/21635 [==============================] - 6s 294us/step - loss: 0.2508 - mean_absolute_error: 0.3801 - val_loss: 1.5635 - val_mean_absolute_error: 0.8727\n",
      "Epoch 29/200\n",
      "21635/21635 [==============================] - 6s 288us/step - loss: 0.2233 - mean_absolute_error: 0.3534 - val_loss: 1.4728 - val_mean_absolute_error: 0.8416\n",
      "Epoch 30/200\n",
      "21635/21635 [==============================] - 6s 296us/step - loss: 0.2241 - mean_absolute_error: 0.3540 - val_loss: 1.7203 - val_mean_absolute_error: 0.9405\n",
      "Epoch 31/200\n",
      "21635/21635 [==============================] - 6s 294us/step - loss: 0.2150 - mean_absolute_error: 0.3506 - val_loss: 1.3573 - val_mean_absolute_error: 0.8056\n",
      "Epoch 32/200\n",
      "21635/21635 [==============================] - 6s 291us/step - loss: 0.2153 - mean_absolute_error: 0.3519 - val_loss: 1.4605 - val_mean_absolute_error: 0.8401\n",
      "Epoch 33/200\n",
      "21635/21635 [==============================] - 6s 293us/step - loss: 0.2183 - mean_absolute_error: 0.3548 - val_loss: 1.3840 - val_mean_absolute_error: 0.7965\n",
      "Epoch 34/200\n",
      "21635/21635 [==============================] - 6s 297us/step - loss: 0.1733 - mean_absolute_error: 0.3148 - val_loss: 1.3275 - val_mean_absolute_error: 0.7926\n",
      "Epoch 35/200\n",
      "21635/21635 [==============================] - 6s 294us/step - loss: 0.1633 - mean_absolute_error: 0.3122 - val_loss: 1.3275 - val_mean_absolute_error: 0.7850\n",
      "Epoch 36/200\n",
      "21635/21635 [==============================] - 6s 286us/step - loss: 0.1756 - mean_absolute_error: 0.3192 - val_loss: 1.3840 - val_mean_absolute_error: 0.8388\n",
      "Epoch 37/200\n",
      "21635/21635 [==============================] - 6s 288us/step - loss: 0.1888 - mean_absolute_error: 0.3313 - val_loss: 1.2699 - val_mean_absolute_error: 0.7684\n",
      "Epoch 38/200\n",
      "21635/21635 [==============================] - 6s 286us/step - loss: 0.1504 - mean_absolute_error: 0.2985 - val_loss: 1.5223 - val_mean_absolute_error: 0.8636\n",
      "Epoch 39/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.1419 - mean_absolute_error: 0.2862 - val_loss: 1.2014 - val_mean_absolute_error: 0.7644\n",
      "Epoch 40/200\n",
      "21635/21635 [==============================] - 6s 288us/step - loss: 0.1315 - mean_absolute_error: 0.2788 - val_loss: 1.3571 - val_mean_absolute_error: 0.8636\n",
      "Epoch 41/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.1558 - mean_absolute_error: 0.3014 - val_loss: 1.7342 - val_mean_absolute_error: 0.9959\n",
      "Epoch 42/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.1420 - mean_absolute_error: 0.2879 - val_loss: 1.3075 - val_mean_absolute_error: 0.7958\n",
      "Epoch 43/200\n",
      "21635/21635 [==============================] - 6s 286us/step - loss: 0.1189 - mean_absolute_error: 0.2648 - val_loss: 1.2436 - val_mean_absolute_error: 0.7677\n",
      "Epoch 44/200\n",
      "21635/21635 [==============================] - 6s 288us/step - loss: 0.1307 - mean_absolute_error: 0.2773 - val_loss: 1.2980 - val_mean_absolute_error: 0.7945\n",
      "Epoch 45/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.1216 - mean_absolute_error: 0.2649 - val_loss: 1.1898 - val_mean_absolute_error: 0.7438\n",
      "Epoch 46/200\n",
      "21635/21635 [==============================] - 6s 286us/step - loss: 0.1212 - mean_absolute_error: 0.2683 - val_loss: 1.1423 - val_mean_absolute_error: 0.7357\n",
      "Epoch 47/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.1009 - mean_absolute_error: 0.2426 - val_loss: 1.0956 - val_mean_absolute_error: 0.7135\n",
      "Epoch 48/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21635/21635 [==============================] - 6s 279us/step - loss: 0.1005 - mean_absolute_error: 0.2419 - val_loss: 1.1475 - val_mean_absolute_error: 0.7304\n",
      "Epoch 49/200\n",
      "21635/21635 [==============================] - 6s 281us/step - loss: 0.1083 - mean_absolute_error: 0.2542 - val_loss: 1.1159 - val_mean_absolute_error: 0.7206\n",
      "Epoch 50/200\n",
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.1155 - mean_absolute_error: 0.2622 - val_loss: 1.0711 - val_mean_absolute_error: 0.7143\n",
      "Epoch 51/200\n",
      "21635/21635 [==============================] - 6s 279us/step - loss: 0.0982 - mean_absolute_error: 0.2412 - val_loss: 1.0252 - val_mean_absolute_error: 0.6965\n",
      "Epoch 52/200\n",
      "21635/21635 [==============================] - 6s 280us/step - loss: 0.0902 - mean_absolute_error: 0.2305 - val_loss: 1.0963 - val_mean_absolute_error: 0.7526\n",
      "Epoch 53/200\n",
      "21635/21635 [==============================] - 6s 280us/step - loss: 0.0987 - mean_absolute_error: 0.2424 - val_loss: 1.0511 - val_mean_absolute_error: 0.7023\n",
      "Epoch 54/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.0866 - mean_absolute_error: 0.2247 - val_loss: 0.9919 - val_mean_absolute_error: 0.6879\n",
      "Epoch 55/200\n",
      "21635/21635 [==============================] - 6s 281us/step - loss: 0.0949 - mean_absolute_error: 0.2341 - val_loss: 1.0067 - val_mean_absolute_error: 0.6871\n",
      "Epoch 56/200\n",
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.1141 - mean_absolute_error: 0.2224 - val_loss: 1.1233 - val_mean_absolute_error: 0.7292\n",
      "Epoch 57/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.0877 - mean_absolute_error: 0.2259 - val_loss: 0.9969 - val_mean_absolute_error: 0.6944\n",
      "Epoch 58/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0762 - mean_absolute_error: 0.2120 - val_loss: 1.2068 - val_mean_absolute_error: 0.7813\n",
      "Epoch 59/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0840 - mean_absolute_error: 0.2205 - val_loss: 1.3164 - val_mean_absolute_error: 0.8587\n",
      "Epoch 60/200\n",
      "21635/21635 [==============================] - 6s 281us/step - loss: 0.0935 - mean_absolute_error: 0.2243 - val_loss: 0.9748 - val_mean_absolute_error: 0.6837\n",
      "Epoch 61/200\n",
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.0864 - mean_absolute_error: 0.2250 - val_loss: 1.0605 - val_mean_absolute_error: 0.7108\n",
      "Epoch 62/200\n",
      "21635/21635 [==============================] - 6s 281us/step - loss: 0.0653 - mean_absolute_error: 0.1964 - val_loss: 0.9920 - val_mean_absolute_error: 0.6846\n",
      "Epoch 63/200\n",
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.0695 - mean_absolute_error: 0.2023 - val_loss: 0.9285 - val_mean_absolute_error: 0.6758\n",
      "Epoch 64/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.0624 - mean_absolute_error: 0.1931 - val_loss: 0.9959 - val_mean_absolute_error: 0.6821\n",
      "Epoch 65/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0699 - mean_absolute_error: 0.2043 - val_loss: 0.9313 - val_mean_absolute_error: 0.6575\n",
      "Epoch 66/200\n",
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.0748 - mean_absolute_error: 0.2051 - val_loss: 0.9386 - val_mean_absolute_error: 0.6633\n",
      "Epoch 67/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0644 - mean_absolute_error: 0.1923 - val_loss: 0.9045 - val_mean_absolute_error: 0.6496\n",
      "Epoch 68/200\n",
      "21635/21635 [==============================] - 6s 280us/step - loss: 0.0557 - mean_absolute_error: 0.1818 - val_loss: 1.0938 - val_mean_absolute_error: 0.7556\n",
      "Epoch 69/200\n",
      "21635/21635 [==============================] - 6s 280us/step - loss: 0.0713 - mean_absolute_error: 0.2028 - val_loss: 0.9048 - val_mean_absolute_error: 0.6557\n",
      "Epoch 70/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0608 - mean_absolute_error: 0.1902 - val_loss: 0.8852 - val_mean_absolute_error: 0.6554\n",
      "Epoch 71/200\n",
      "21635/21635 [==============================] - 6s 281us/step - loss: 0.0639 - mean_absolute_error: 0.1946 - val_loss: 0.8512 - val_mean_absolute_error: 0.6536\n",
      "Epoch 72/200\n",
      "21635/21635 [==============================] - 6s 280us/step - loss: 0.0589 - mean_absolute_error: 0.1884 - val_loss: 0.8788 - val_mean_absolute_error: 0.6660\n",
      "Epoch 73/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.0551 - mean_absolute_error: 0.1809 - val_loss: 0.8498 - val_mean_absolute_error: 0.6256\n",
      "Epoch 74/200\n",
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.0886 - mean_absolute_error: 0.1907 - val_loss: 0.8665 - val_mean_absolute_error: 0.6565\n",
      "Epoch 75/200\n",
      "21635/21635 [==============================] - 6s 281us/step - loss: 0.0652 - mean_absolute_error: 0.1739 - val_loss: 1.0980 - val_mean_absolute_error: 0.7386\n",
      "Epoch 76/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0848 - mean_absolute_error: 0.1978 - val_loss: 0.9044 - val_mean_absolute_error: 0.6416\n",
      "Epoch 77/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.0597 - mean_absolute_error: 0.1599 - val_loss: 1.0364 - val_mean_absolute_error: 0.7002\n",
      "Epoch 78/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.1075 - mean_absolute_error: 0.1930 - val_loss: 0.9635 - val_mean_absolute_error: 0.6847\n",
      "Epoch 79/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0645 - mean_absolute_error: 0.1647 - val_loss: 0.8970 - val_mean_absolute_error: 0.6468\n",
      "Epoch 80/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0614 - mean_absolute_error: 0.1624 - val_loss: 0.8687 - val_mean_absolute_error: 0.6355\n",
      "Epoch 81/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0649 - mean_absolute_error: 0.1701 - val_loss: 0.8569 - val_mean_absolute_error: 0.6309\n",
      "Epoch 82/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.0656 - mean_absolute_error: 0.1718 - val_loss: 0.8939 - val_mean_absolute_error: 0.6502\n",
      "Epoch 83/200\n",
      "21635/21635 [==============================] - 6s 286us/step - loss: 0.0632 - mean_absolute_error: 0.1677 - val_loss: 0.8420 - val_mean_absolute_error: 0.6463\n",
      "Epoch 84/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0687 - mean_absolute_error: 0.1763 - val_loss: 0.8496 - val_mean_absolute_error: 0.6300\n",
      "Epoch 85/200\n",
      "21635/21635 [==============================] - 6s 286us/step - loss: 0.0600 - mean_absolute_error: 0.1609 - val_loss: 0.8258 - val_mean_absolute_error: 0.6410\n",
      "Epoch 86/200\n",
      "21635/21635 [==============================] - 6s 286us/step - loss: 0.0701 - mean_absolute_error: 0.1780 - val_loss: 0.8023 - val_mean_absolute_error: 0.6132\n",
      "Epoch 87/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0626 - mean_absolute_error: 0.1636 - val_loss: 0.7979 - val_mean_absolute_error: 0.6059\n",
      "Epoch 88/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0631 - mean_absolute_error: 0.1625 - val_loss: 0.9294 - val_mean_absolute_error: 0.6805\n",
      "Epoch 89/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0692 - mean_absolute_error: 0.1779 - val_loss: 0.7941 - val_mean_absolute_error: 0.6068\n",
      "Epoch 90/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0571 - mean_absolute_error: 0.1516 - val_loss: 0.7830 - val_mean_absolute_error: 0.6045\n",
      "Epoch 91/200\n",
      "21635/21635 [==============================] - 6s 291us/step - loss: 0.0642 - mean_absolute_error: 0.1614 - val_loss: 0.7989 - val_mean_absolute_error: 0.6161\n",
      "Epoch 92/200\n",
      "21635/21635 [==============================] - 6s 286us/step - loss: 0.0804 - mean_absolute_error: 0.1628 - val_loss: 0.7973 - val_mean_absolute_error: 0.6103\n",
      "Epoch 93/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0378 - mean_absolute_error: 0.1480 - val_loss: 0.8492 - val_mean_absolute_error: 0.6691\n",
      "Epoch 94/200\n",
      "21635/21635 [==============================] - 6s 291us/step - loss: 0.0507 - mean_absolute_error: 0.1692 - val_loss: 0.7781 - val_mean_absolute_error: 0.6108\n",
      "Epoch 95/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0382 - mean_absolute_error: 0.1486 - val_loss: 0.7777 - val_mean_absolute_error: 0.6030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/200\n",
      "21635/21635 [==============================] - 6s 289us/step - loss: 0.0382 - mean_absolute_error: 0.1471 - val_loss: 0.8188 - val_mean_absolute_error: 0.6215\n",
      "Epoch 97/200\n",
      "21635/21635 [==============================] - 7s 311us/step - loss: 0.0509 - mean_absolute_error: 0.1491 - val_loss: 0.8312 - val_mean_absolute_error: 0.6355\n",
      "Epoch 98/200\n",
      "21635/21635 [==============================] - 6s 289us/step - loss: 0.0807 - mean_absolute_error: 0.1620 - val_loss: 0.7539 - val_mean_absolute_error: 0.5901\n",
      "Epoch 99/200\n",
      "21635/21635 [==============================] - 6s 293us/step - loss: 0.5009 - mean_absolute_error: 0.1774 - val_loss: 0.7897 - val_mean_absolute_error: 0.6073\n",
      "Epoch 100/200\n",
      "21635/21635 [==============================] - 6s 291us/step - loss: 0.0319 - mean_absolute_error: 0.1317 - val_loss: 0.7913 - val_mean_absolute_error: 0.6049\n",
      "Epoch 101/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0287 - mean_absolute_error: 0.1293 - val_loss: 0.7708 - val_mean_absolute_error: 0.6074\n",
      "Epoch 102/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0400 - mean_absolute_error: 0.1480 - val_loss: 0.7815 - val_mean_absolute_error: 0.6001\n",
      "Epoch 103/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0336 - mean_absolute_error: 0.1328 - val_loss: 0.8297 - val_mean_absolute_error: 0.6308\n",
      "Epoch 104/200\n",
      "21635/21635 [==============================] - 7s 316us/step - loss: 0.0321 - mean_absolute_error: 0.1362 - val_loss: 0.7962 - val_mean_absolute_error: 0.6139\n",
      "Epoch 105/200\n",
      "21635/21635 [==============================] - 6s 270us/step - loss: 0.0292 - mean_absolute_error: 0.1302 - val_loss: 0.7607 - val_mean_absolute_error: 0.5974\n",
      "Epoch 106/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.0381 - mean_absolute_error: 0.1493 - val_loss: 0.7873 - val_mean_absolute_error: 0.6069\n",
      "Epoch 107/200\n",
      "21635/21635 [==============================] - 7s 309us/step - loss: 0.0338 - mean_absolute_error: 0.1393 - val_loss: 0.7627 - val_mean_absolute_error: 0.5910\n",
      "Epoch 108/200\n",
      "21635/21635 [==============================] - 7s 305us/step - loss: 0.0305 - mean_absolute_error: 0.1325 - val_loss: 0.8464 - val_mean_absolute_error: 0.6493\n",
      "Epoch 109/200\n",
      "21635/21635 [==============================] - 6s 297us/step - loss: 0.0387 - mean_absolute_error: 0.1403 - val_loss: 0.8151 - val_mean_absolute_error: 0.6333\n",
      "Epoch 110/200\n",
      "21635/21635 [==============================] - 6s 291us/step - loss: 0.0482 - mean_absolute_error: 0.1479 - val_loss: 0.7315 - val_mean_absolute_error: 0.5941\n",
      "Epoch 111/200\n",
      "21635/21635 [==============================] - 6s 288us/step - loss: 0.0292 - mean_absolute_error: 0.1267 - val_loss: 0.7448 - val_mean_absolute_error: 0.5848\n",
      "Epoch 112/200\n",
      "21635/21635 [==============================] - 7s 314us/step - loss: 0.0334 - mean_absolute_error: 0.1354 - val_loss: 0.7303 - val_mean_absolute_error: 0.5902\n",
      "Epoch 113/200\n",
      "21635/21635 [==============================] - 6s 292us/step - loss: 0.0605 - mean_absolute_error: 0.1389 - val_loss: 0.8344 - val_mean_absolute_error: 0.6458\n",
      "Epoch 114/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0304 - mean_absolute_error: 0.1316 - val_loss: 0.7408 - val_mean_absolute_error: 0.5826\n",
      "Epoch 115/200\n",
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.0325 - mean_absolute_error: 0.1306 - val_loss: 0.7285 - val_mean_absolute_error: 0.5830\n",
      "Epoch 116/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0391 - mean_absolute_error: 0.1380 - val_loss: 0.7503 - val_mean_absolute_error: 0.5896\n",
      "Epoch 117/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.0472 - mean_absolute_error: 0.1312 - val_loss: 0.7238 - val_mean_absolute_error: 0.5876\n",
      "Epoch 118/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0221 - mean_absolute_error: 0.1121 - val_loss: 0.7321 - val_mean_absolute_error: 0.5830\n",
      "Epoch 119/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0317 - mean_absolute_error: 0.1315 - val_loss: 0.7210 - val_mean_absolute_error: 0.5828\n",
      "Epoch 120/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0359 - mean_absolute_error: 0.1380 - val_loss: 0.7203 - val_mean_absolute_error: 0.5810\n",
      "Epoch 121/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0282 - mean_absolute_error: 0.1224 - val_loss: 0.7050 - val_mean_absolute_error: 0.5738\n",
      "Epoch 122/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0347 - mean_absolute_error: 0.1193 - val_loss: 0.6870 - val_mean_absolute_error: 0.5702\n",
      "Epoch 123/200\n",
      "21635/21635 [==============================] - 6s 286us/step - loss: 0.0388 - mean_absolute_error: 0.1294 - val_loss: 0.7877 - val_mean_absolute_error: 0.6214\n",
      "Epoch 124/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0314 - mean_absolute_error: 0.1268 - val_loss: 0.7290 - val_mean_absolute_error: 0.5868\n",
      "Epoch 125/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0281 - mean_absolute_error: 0.1245 - val_loss: 0.7114 - val_mean_absolute_error: 0.5731\n",
      "Epoch 126/200\n",
      "21635/21635 [==============================] - 7s 304us/step - loss: 0.0292 - mean_absolute_error: 0.1294 - val_loss: 0.6949 - val_mean_absolute_error: 0.5708\n",
      "Epoch 127/200\n",
      "21635/21635 [==============================] - 7s 307us/step - loss: 0.0425 - mean_absolute_error: 0.1277 - val_loss: 0.6944 - val_mean_absolute_error: 0.5668\n",
      "Epoch 128/200\n",
      "21635/21635 [==============================] - 7s 304us/step - loss: 0.0414 - mean_absolute_error: 0.1162 - val_loss: 0.7183 - val_mean_absolute_error: 0.5789\n",
      "Epoch 129/200\n",
      "21635/21635 [==============================] - 7s 303us/step - loss: 0.0311 - mean_absolute_error: 0.1285 - val_loss: 0.7029 - val_mean_absolute_error: 0.5709\n",
      "Epoch 130/200\n",
      "21635/21635 [==============================] - 7s 321us/step - loss: 0.0259 - mean_absolute_error: 0.1127 - val_loss: 0.6891 - val_mean_absolute_error: 0.5745\n",
      "Epoch 131/200\n",
      "21635/21635 [==============================] - 7s 302us/step - loss: 0.0252 - mean_absolute_error: 0.1127 - val_loss: 0.7314 - val_mean_absolute_error: 0.5892\n",
      "Epoch 132/200\n",
      "21635/21635 [==============================] - 7s 302us/step - loss: 0.0234 - mean_absolute_error: 0.1155 - val_loss: 0.6968 - val_mean_absolute_error: 0.5663\n",
      "Epoch 133/200\n",
      "21635/21635 [==============================] - 6s 299us/step - loss: 0.0219 - mean_absolute_error: 0.1116 - val_loss: 0.7036 - val_mean_absolute_error: 0.5713\n",
      "Epoch 134/200\n",
      "21635/21635 [==============================] - 6s 300us/step - loss: 0.0286 - mean_absolute_error: 0.1247 - val_loss: 0.7130 - val_mean_absolute_error: 0.5823\n",
      "Epoch 135/200\n",
      "21635/21635 [==============================] - 7s 301us/step - loss: 0.0252 - mean_absolute_error: 0.1205 - val_loss: 0.6834 - val_mean_absolute_error: 0.5613\n",
      "Epoch 136/200\n",
      "21635/21635 [==============================] - 6s 299us/step - loss: 0.0217 - mean_absolute_error: 0.1110 - val_loss: 0.7004 - val_mean_absolute_error: 0.5768\n",
      "Epoch 137/200\n",
      "21635/21635 [==============================] - 6s 300us/step - loss: 0.0285 - mean_absolute_error: 0.1242 - val_loss: 0.6879 - val_mean_absolute_error: 0.5641\n",
      "Epoch 138/200\n",
      "21635/21635 [==============================] - 6s 299us/step - loss: 0.0221 - mean_absolute_error: 0.1108 - val_loss: 0.6766 - val_mean_absolute_error: 0.5588\n",
      "Epoch 139/200\n",
      "21635/21635 [==============================] - 7s 301us/step - loss: 0.0535 - mean_absolute_error: 0.1249 - val_loss: 0.6817 - val_mean_absolute_error: 0.5620\n",
      "Epoch 140/200\n",
      "21635/21635 [==============================] - 6s 298us/step - loss: 0.0283 - mean_absolute_error: 0.1165 - val_loss: 0.6712 - val_mean_absolute_error: 0.5557\n",
      "Epoch 141/200\n",
      "21635/21635 [==============================] - 7s 301us/step - loss: 0.0221 - mean_absolute_error: 0.1100 - val_loss: 0.6634 - val_mean_absolute_error: 0.5611\n",
      "Epoch 142/200\n",
      "21635/21635 [==============================] - 6s 299us/step - loss: 0.0243 - mean_absolute_error: 0.1134 - val_loss: 0.6843 - val_mean_absolute_error: 0.5633\n",
      "Epoch 143/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.0235 - mean_absolute_error: 0.1138 - val_loss: 0.6586 - val_mean_absolute_error: 0.5533\n",
      "Epoch 144/200\n",
      "21635/21635 [==============================] - 6s 281us/step - loss: 0.0216 - mean_absolute_error: 0.1054 - val_loss: 0.7154 - val_mean_absolute_error: 0.5833\n",
      "Epoch 145/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.0773 - mean_absolute_error: 0.1202 - val_loss: 0.6796 - val_mean_absolute_error: 0.5756\n",
      "Epoch 146/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0248 - mean_absolute_error: 0.1156 - val_loss: 0.6621 - val_mean_absolute_error: 0.5563\n",
      "Epoch 147/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.0228 - mean_absolute_error: 0.1096 - val_loss: 0.6612 - val_mean_absolute_error: 0.5531\n",
      "Epoch 148/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0292 - mean_absolute_error: 0.1127 - val_loss: 0.6585 - val_mean_absolute_error: 0.5569\n",
      "Epoch 149/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0215 - mean_absolute_error: 0.1104 - val_loss: 0.6712 - val_mean_absolute_error: 0.5575\n",
      "Epoch 150/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0198 - mean_absolute_error: 0.1053 - val_loss: 0.6869 - val_mean_absolute_error: 0.5692\n",
      "Epoch 151/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0219 - mean_absolute_error: 0.1123 - val_loss: 0.6648 - val_mean_absolute_error: 0.5572\n",
      "Epoch 152/200\n",
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.0236 - mean_absolute_error: 0.1118 - val_loss: 0.6632 - val_mean_absolute_error: 0.5532\n",
      "Epoch 153/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0241 - mean_absolute_error: 0.1158 - val_loss: 0.7035 - val_mean_absolute_error: 0.5773\n",
      "Epoch 154/200\n",
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.0193 - mean_absolute_error: 0.1035 - val_loss: 0.7790 - val_mean_absolute_error: 0.6331\n",
      "Epoch 155/200\n",
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.0272 - mean_absolute_error: 0.1211 - val_loss: 0.6603 - val_mean_absolute_error: 0.5659\n",
      "Epoch 156/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.0225 - mean_absolute_error: 0.1098 - val_loss: 0.6412 - val_mean_absolute_error: 0.5504\n",
      "Epoch 157/200\n",
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.0191 - mean_absolute_error: 0.1024 - val_loss: 0.6634 - val_mean_absolute_error: 0.5563\n",
      "Epoch 158/200\n",
      "21635/21635 [==============================] - 6s 281us/step - loss: 0.0213 - mean_absolute_error: 0.1080 - val_loss: 0.6475 - val_mean_absolute_error: 0.5466\n",
      "Epoch 159/200\n",
      "21635/21635 [==============================] - 6s 286us/step - loss: 0.0215 - mean_absolute_error: 0.1085 - val_loss: 0.6945 - val_mean_absolute_error: 0.6078\n",
      "Epoch 160/200\n",
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.0280 - mean_absolute_error: 0.1197 - val_loss: 0.6653 - val_mean_absolute_error: 0.5564\n",
      "Epoch 161/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0260 - mean_absolute_error: 0.1093 - val_loss: 0.6623 - val_mean_absolute_error: 0.5575\n",
      "Epoch 162/200\n",
      "21635/21635 [==============================] - 6s 286us/step - loss: 0.0227 - mean_absolute_error: 0.1069 - val_loss: 0.7050 - val_mean_absolute_error: 0.5842\n",
      "Epoch 163/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0179 - mean_absolute_error: 0.0968 - val_loss: 0.6445 - val_mean_absolute_error: 0.5462\n",
      "Epoch 164/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.0182 - mean_absolute_error: 0.0992 - val_loss: 0.6552 - val_mean_absolute_error: 0.5526\n",
      "Epoch 165/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0227 - mean_absolute_error: 0.1117 - val_loss: 0.6452 - val_mean_absolute_error: 0.5498\n",
      "Epoch 166/200\n",
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.0239 - mean_absolute_error: 0.1007 - val_loss: 0.6557 - val_mean_absolute_error: 0.5501\n",
      "Epoch 167/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.2240 - mean_absolute_error: 0.1080 - val_loss: 0.6579 - val_mean_absolute_error: 0.5522\n",
      "Epoch 168/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.0320 - mean_absolute_error: 0.1137 - val_loss: 0.6899 - val_mean_absolute_error: 0.5730\n",
      "Epoch 169/200\n",
      "21635/21635 [==============================] - 6s 289us/step - loss: 0.0166 - mean_absolute_error: 0.0933 - val_loss: 0.6461 - val_mean_absolute_error: 0.5484\n",
      "Epoch 170/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0206 - mean_absolute_error: 0.1059 - val_loss: 0.6488 - val_mean_absolute_error: 0.5460\n",
      "Epoch 171/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0205 - mean_absolute_error: 0.0995 - val_loss: 0.6738 - val_mean_absolute_error: 0.5633\n",
      "Epoch 172/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0196 - mean_absolute_error: 0.0997 - val_loss: 0.7079 - val_mean_absolute_error: 0.5873\n",
      "Epoch 173/200\n",
      "21635/21635 [==============================] - 6s 285us/step - loss: 0.0204 - mean_absolute_error: 0.1059 - val_loss: 0.7136 - val_mean_absolute_error: 0.5918\n",
      "Epoch 174/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0217 - mean_absolute_error: 0.1052 - val_loss: 0.6541 - val_mean_absolute_error: 0.5557\n",
      "Epoch 175/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0187 - mean_absolute_error: 0.1019 - val_loss: 0.6396 - val_mean_absolute_error: 0.5487\n",
      "Epoch 176/200\n",
      "21635/21635 [==============================] - 6s 289us/step - loss: 0.0269 - mean_absolute_error: 0.0995 - val_loss: 0.6482 - val_mean_absolute_error: 0.5482\n",
      "Epoch 177/200\n",
      "21635/21635 [==============================] - 6s 286us/step - loss: 0.0315 - mean_absolute_error: 0.1093 - val_loss: 0.6500 - val_mean_absolute_error: 0.5466\n",
      "Epoch 178/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0310 - mean_absolute_error: 0.1055 - val_loss: 0.6838 - val_mean_absolute_error: 0.5689\n",
      "Epoch 179/200\n",
      "21635/21635 [==============================] - 6s 288us/step - loss: 0.0256 - mean_absolute_error: 0.0958 - val_loss: 0.6554 - val_mean_absolute_error: 0.5696\n",
      "Epoch 180/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0276 - mean_absolute_error: 0.1035 - val_loss: 0.6404 - val_mean_absolute_error: 0.5473\n",
      "Epoch 181/200\n",
      "21635/21635 [==============================] - 6s 289us/step - loss: 0.0198 - mean_absolute_error: 0.0981 - val_loss: 0.6839 - val_mean_absolute_error: 0.5692\n",
      "Epoch 182/200\n",
      "21635/21635 [==============================] - 6s 289us/step - loss: 0.0209 - mean_absolute_error: 0.0991 - val_loss: 0.6918 - val_mean_absolute_error: 0.5760\n",
      "Epoch 183/200\n",
      "21635/21635 [==============================] - 6s 289us/step - loss: 0.0201 - mean_absolute_error: 0.1007 - val_loss: 0.6547 - val_mean_absolute_error: 0.5506\n",
      "Epoch 184/200\n",
      "21635/21635 [==============================] - 6s 287us/step - loss: 0.0193 - mean_absolute_error: 0.1001 - val_loss: 0.6502 - val_mean_absolute_error: 0.5502\n",
      "Epoch 185/200\n",
      "21635/21635 [==============================] - 6s 291us/step - loss: 0.0234 - mean_absolute_error: 0.1036 - val_loss: 0.6530 - val_mean_absolute_error: 0.5599\n",
      "Epoch 186/200\n",
      "21635/21635 [==============================] - 6s 288us/step - loss: 0.0170 - mean_absolute_error: 0.0961 - val_loss: 0.6775 - val_mean_absolute_error: 0.5661\n",
      "Epoch 187/200\n",
      "21635/21635 [==============================] - 6s 290us/step - loss: 0.0211 - mean_absolute_error: 0.1021 - val_loss: 0.6440 - val_mean_absolute_error: 0.5464\n",
      "Epoch 188/200\n",
      "21635/21635 [==============================] - 6s 292us/step - loss: 0.0207 - mean_absolute_error: 0.1014 - val_loss: 0.6416 - val_mean_absolute_error: 0.5447\n",
      "Epoch 189/200\n",
      "21635/21635 [==============================] - 6s 289us/step - loss: 0.0200 - mean_absolute_error: 0.1030 - val_loss: 0.7038 - val_mean_absolute_error: 0.5893\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.0164 - mean_absolute_error: 0.0940 - val_loss: 0.6385 - val_mean_absolute_error: 0.5495\n",
      "Epoch 191/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0158 - mean_absolute_error: 0.0924 - val_loss: 0.6596 - val_mean_absolute_error: 0.5761\n",
      "Epoch 192/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0169 - mean_absolute_error: 0.0963 - val_loss: 0.6482 - val_mean_absolute_error: 0.5624\n",
      "Epoch 193/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0182 - mean_absolute_error: 0.0995 - val_loss: 0.6390 - val_mean_absolute_error: 0.5430\n",
      "Epoch 194/200\n",
      "21635/21635 [==============================] - 6s 282us/step - loss: 0.0179 - mean_absolute_error: 0.0977 - val_loss: 0.6501 - val_mean_absolute_error: 0.5488\n",
      "Epoch 195/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0173 - mean_absolute_error: 0.0954 - val_loss: 0.6600 - val_mean_absolute_error: 0.5537\n",
      "Epoch 196/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0213 - mean_absolute_error: 0.1053 - val_loss: 0.6462 - val_mean_absolute_error: 0.5505\n",
      "Epoch 197/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0192 - mean_absolute_error: 0.1001 - val_loss: 0.6472 - val_mean_absolute_error: 0.5453\n",
      "Epoch 198/200\n",
      "21635/21635 [==============================] - 6s 286us/step - loss: 0.0141 - mean_absolute_error: 0.0871 - val_loss: 0.6411 - val_mean_absolute_error: 0.5511\n",
      "Epoch 199/200\n",
      "21635/21635 [==============================] - 6s 284us/step - loss: 0.0150 - mean_absolute_error: 0.0908 - val_loss: 0.6454 - val_mean_absolute_error: 0.5449\n",
      "Epoch 200/200\n",
      "21635/21635 [==============================] - 6s 283us/step - loss: 0.0176 - mean_absolute_error: 0.0997 - val_loss: 0.6423 - val_mean_absolute_error: 0.5527\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              multiple                  1021568   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              multiple                  65        \n",
      "=================================================================\n",
      "Total params: 1,034,113\n",
      "Trainable params: 1,034,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We train the model on our data\n",
    "# Number of epochs the network should run through\n",
    "EPOCHS = 200\n",
    "# Size of the batch for optimization\n",
    "BATCH_SIZE = 32\n",
    "# Set up validation split\n",
    "VALIDATION_SPLIT = 0.2\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "# This will avoid overfitting\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split = VALIDATION_SPLIT, \\\n",
    "          callbacks=[early_stop])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1305755100837624"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.predict(X_test, batch_size=32)\n",
    "rmse(result, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "For the following model :\n",
    "    - 4 hidden layers of 64 neurons - relu activation.\n",
    "    - A result layer of 1 neuron - relu activation.\n",
    "    - Early stopping callback\n",
    "    - Test split of 0.2\n",
    "    - Using AdamOptimizer gives best results.\n",
    "    - No cross validation.\n",
    "    \n",
    "Network size :\n",
    "- $4*64 + 1 = 257$ neurons (biases)\n",
    "- $30049*64 + 64*64 + 64*64 + 64 = 1931392$ weights\n",
    "- Total of $1931649$ learnable parameters (almost 2 millions)\n",
    "\n",
    "With the above, we reached a RMSE of 0.5. But this is without separating the data into train and test set. This can lead to overfitting. Now we need to ensure that we are not overfitting.\n",
    "\n",
    "For this we will test 2 solutions :\n",
    "\n",
    "### Change our implementation to add regulizer that will avoid this overfitting\n",
    "\n",
    "### Cross validation over multiple models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
