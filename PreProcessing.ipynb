{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to find good techniques to apply to our original data to improve the precision of our algorithms and to reduce the size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from regressions import *\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data/\"\n",
    "X = np.load(DATA_FOLDER + \"feature_mat_radial_compression.npy\")\n",
    "y = np.load(DATA_FOLDER + \"CSD500-r_train-H_total.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (30049, 15961)\n",
      "y: (30049,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \" + str(X.shape))\n",
    "print(\"y: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (30049, 15961)\n"
     ]
    }
   ],
   "source": [
    "x_df = x_df.drop_duplicates()\n",
    "print(\"X: \" + str(x_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split function is used to split the data set into train and test sets with a 75/25 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x_df, y, perc):\n",
    "    train_set_size = int(x_df.shape[0] * 0.75)\n",
    "    x_tr = x_df.head(train_set_size)\n",
    "    x_te = x_df.tail(int(x_df.shape[0] - train_set_size))\n",
    "    y_tr = y[: train_set_size]\n",
    "    y_te = y[train_set_size :]\n",
    "    return x_tr, y_tr, x_te, y_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that a polynomial expansion really improves the results. This expansion is equivalent to add a column of 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cte_col(df):\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp[df_tmp.shape[1]] = pd.Series(np.ones(df_tmp.shape[0]), index=df_tmp.index)\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can create our test function that splits the data, adds a column of constant term and performs ridge regression for a few lambdas. Only the lowest error is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_quality(x_df, y):\n",
    "    x_df = add_cte_col(x_df)\n",
    "    test_perc = 0.75\n",
    "    x_train, y_train, x_test, y_test = split(x_df, y, test_perc)\n",
    "    best = 100\n",
    "    for lambda_ in [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]:\n",
    "        err = rmse(y_test, x_test, ridge_regression(y_train, x_train, lambda_))\n",
    "        best = err if err < best else best\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7373143070214498"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_quality(x_df, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a simple ridge regression with polynomial expansion gives us an RMSE of 0.73. This will be the value that we consider as the reference for the upcoming tests on the techniques we try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encountered in our readings that deep learning can get only better with more data. So we try to artificially generate some samples based on what we have. We use jittering whhich consists in adding a little bit of noise to an existing, small enough to consider that our evaluation y will stay the same. Basically, for a given number of x_i, we create new samples by replacing x_ij by x_ij + 1% of the mean of the j th feature of the samples. We made it such that we can choose how many samples should we add and how many features should we modify per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to have more samples\n",
    "def add_jitter(df, y, perc_sample, perc_col):\n",
    "    means = [df[j].mean() for j in df]\n",
    "    df_tmp = df.copy()\n",
    "    y_tmp = y.copy()\n",
    "    ids = random.sample(range(df_tmp.shape[0]), int(df_tmp.shape[0]*perc_sample))\n",
    "    for id_ in ids:\n",
    "        new_sample = df_tmp.iloc[[id_]].copy()\n",
    "        col = random.sample(range(df_tmp.shape[1]), int(df_tmp.shape[1]*perc_col))\n",
    "        for j in col:\n",
    "            new_sample[j] = new_sample[j] + 0.01*means[j]\n",
    "        df_tmp = df_tmp.append(new_sample, ignore_index=True)\n",
    "        y_tmp = np.append(y_tmp, y_tmp[id_])\n",
    "    return df_tmp, y_tmp        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df_aug, y_aug = add_jitter(x_df, y, 0.01, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_quality(x_df_aug, y_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adding 1% of samples and modifying 1% of the features lowers the RMSE to 0.72. This is some improvement !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df=(x_df-x_df.mean())/x_df.std()\n",
    "x_df = x_df.drop(15960, axis=1)\n",
    "x_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is a linear transformation and thus should have any influence on the predictions. We choose to normalize not for results but for smoothness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_rm = []\n",
    "for i in [i for i in x_df if i < 50]:\n",
    "    for j in [j for j in x_df if j > i]:\n",
    "        if x_df[i].corr(x_df[j]) > 0.95:\n",
    "            to_rm.append(j)\n",
    "x_df_uncorr = x_df.drop(to_rm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_df_uncorr.shape)\n",
    "test_quality(x_df_uncorr, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those results are surprising as we were expecting the error to go down by removing some noise, but we can see that correlation actually make the results worse. We won't use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses = []\n",
    "ys = range(500, 5501, 500)\n",
    "for i in ys:\n",
    "    pca = PCA(n_components=i, whiten=True)\n",
    "    principalComponents = pca.fit_transform(x_df, x_df.shape)\n",
    "    principalDf = pd.DataFrame(data = principalComponents\n",
    "                 , columns = range(principalComponents.shape[1]))\n",
    "    err = test_quality(principalDf, y)\n",
    "    rmses.append(err)\n",
    "    print(i, err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.plot(ys, rmses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error plotted is relatively high when reducing the number of components below 6000. We are not sure of the conlusion to give here, as we were expecting results less good then the full matrix but not to that extend. We can argue that our `test_quality` function is absolutely not optimized, but after a look at the scientific side, 1000 features should be enough to get good predictions. We then have to choose by default the best one of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_star = 3003\n",
    "pca = PCA(n_components=i_star, whiten=True)\n",
    "principalComponents = pca.fit_transform(x_df, x_df.shape)\n",
    "principalDf_star = pd.DataFrame(data = principalComponents\n",
    "             , columns = range(principalComponents.shape[1]))\n",
    "test_quality(principalDf_star, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_star = 3003\n",
    "pca = PCA(n_components=i_star)\n",
    "principalComponents_no_w = pca.fit_transform(x_df, x_df.shape)\n",
    "principalDf_star_no_w = pd.DataFrame(data = principalComponents_no_w\n",
    "             , columns = range(principalComponents_no_w.shape[1]))\n",
    "test_quality(principalDf_star_no_w, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whitening does not improve anything, even worsen the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ValueError: math domain error is known bug : https://github.com/scikit-learn/scikit-learn/issues/10217: Cannot use MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disapointement is real not to be able to use Maximum Likelyhood Estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data/\"\n",
    "X = np.load(DATA_FOLDER + \"feature_mat_radial_compression.npy\")\n",
    "y = np.load(DATA_FOLDER + \"CSD500-r_train-H_total.npy\")\n",
    "x_df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply PCA\n",
    "i_star = 4500\n",
    "pca = PCA(n_components=i_star)\n",
    "principalComponents = pca.fit_transform(x_df, x_df.shape)\n",
    "x_pca_df = pd.DataFrame(data = principalComponents\n",
    "             , columns = range(principalComponents.shape[1]))\n",
    "x_pca_df.columns = range(x_pca_df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add jitter\n",
    "x_with_jitter_df, y_with_jitter = add_jitter(x_pca_df, y, 0.01, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize\n",
    "x_with_jitter_df=(x_with_jitter_df-x_with_jitter_df.mean())/x_with_jitter_df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/ML/x_train.npy\", x_with_jitter_df)\n",
    "np.save(\"data/ML/y.npy\", y_with_jitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data/\"\n",
    "X = np.load(DATA_FOLDER + \"feature_mat_radial_compression.npy\")\n",
    "y = np.load(DATA_FOLDER + \"CSD500-r_train-H_total.npy\")\n",
    "x_df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply PCA\n",
    "i_star = 3004\n",
    "pca = PCA(n_components=i_star)\n",
    "principalComponents = pca.fit_transform(x_df, x_df.shape)\n",
    "x_pca_df = pd.DataFrame(data = principalComponents\n",
    "             , columns = range(principalComponents.shape[1]))\n",
    "x_pca_df.columns = range(x_pca_df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize\n",
    "x_pca_df=(x_pca_df-x_pca_df.mean())/x_pca_df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/feature_mat_radial_compression_normalized_red.npy\", x_pca_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
