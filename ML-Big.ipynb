{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSMO Project - Regressions\n",
    "By Mathilde Raynal, Etienne Bonvin and Xavier Pantet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.regressions import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data/\"\n",
    "X = np.load(DATA_FOLDER + \"ML/x_train.npy\")\n",
    "y = np.load(DATA_FOLDER + \"ML/y.npy\")\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (30349, 4500)\n",
      "y: (30349,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \" + str(X.shape))\n",
    "print(\"y: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 1: Good ol' least squares (MSE loss without regularizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try a standard and naive implementation of `least_squares` on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 26.16344670274864\n"
     ]
    }
   ],
   "source": [
    "def run_least_squares():\n",
    "    w_star = least_squares(y, X)\n",
    "    loss = rmse(y, X, w_star)\n",
    "    print(\"Loss = \" + str(loss))\n",
    "\n",
    "run_least_squares()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss is quite large! We hope to do better using polynomial expansion using a smaller dataset composed only of a smaller number of features so that we don't need a cluster. We use 4-fold cross-validation to find the best `degree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 4\n",
    "k_indices = build_k_indices(y, k_fold)\n",
    "\n",
    "def run_least_squares_poly():\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "\n",
    "    for degree in range(4):\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            try:\n",
    "                loss_tr, loss_te, _ = cross_validation(y, X, k_indices, k, degree, least_squares)\n",
    "                rmse_tr_tmp.append(loss_tr)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            except:\n",
    "                print(\"Could not inverse the matrix\")\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))\n",
    "    pd.DataFrame([rmse_tr, rmse_te]).add_prefix(\"Degree \").rename({0: \"Train error\", 1: \"Test error\"}).head()\n",
    "    return rmse_tr, rmse_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3.213171631581826,\n",
       "  0.40145325242460417,\n",
       "  0.2659636431823623,\n",
       "  0.1652126444248406],\n",
       " [3.2132414414047386,\n",
       "  0.5357537373798815,\n",
       "  0.6076663576371552,\n",
       "  0.853874297553556])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_least_squares_poly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, polynomial expansion provides better results. Moreover, we see that the best `degree` is 1.\n",
    "\n",
    "__By adding a single feature of constant values, we manage to decrease the RMSE from 26 to 0.7!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 2: Ridge regression (MSE loss with $\\mathcal{L}_2$-regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 4\n",
    "k_indices = build_k_indices(y, k_fold)\n",
    "\n",
    "def run_ridge_regression():\n",
    "    for lambda_ in np.logspace(-5, 1, 7):\n",
    "        print(\"Lambda = \" + str(lambda_))\n",
    "        for degree in range(1, 3):\n",
    "            print(\"    Degree = \" + str(degree))\n",
    "            rmse_tr_tmp = []\n",
    "            rmse_te_tmp = []\n",
    "            for k in range(k_fold):\n",
    "                ridge_lambda = lambda y, X: ridge_regression(y, X, lambda_)\n",
    "                loss_tr, loss_te, _ = cross_validation(y, X, k_indices, k, degree, ridge_lambda)\n",
    "                rmse_tr_tmp.append(loss_tr)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            print(\"        \" + str(np.mean(rmse_tr_tmp)))\n",
    "            print(\"        \" + str(np.mean(rmse_te_tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 1e-05\n",
      "    Degree = 1\n",
      "        0.40145325242460456\n",
      "        0.5357537372320587\n",
      "    Degree = 2\n",
      "        0.26596380103399736\n",
      "        0.607666163440107\n",
      "Lambda = 0.0001\n",
      "    Degree = 1\n",
      "        0.4014532524246221\n",
      "        0.5357537359016747\n",
      "    Degree = 2\n",
      "        0.2659683198847004\n",
      "        0.6076760153324785\n",
      "Lambda = 0.001\n",
      "    Degree = 1\n",
      "        0.4014532524263871\n",
      "        0.535753722599882\n",
      "    Degree = 2\n",
      "        0.26597880149891057\n",
      "        0.6077023419396217\n",
      "Lambda = 0.01\n",
      "    Degree = 1\n",
      "        0.40145325260288683\n",
      "        0.5357535897864518\n",
      "    Degree = 2\n",
      "        0.26598240518404337\n",
      "        0.60770831648428\n",
      "Lambda = 0.1\n",
      "    Degree = 1\n",
      "        0.40145327025267574\n",
      "        0.535752282101457\n",
      "    Degree = 2\n",
      "        0.26605512986311564\n",
      "        0.6078706202481085\n",
      "Lambda = 1.0\n",
      "    Degree = 1\n",
      "        0.40145503506146574\n",
      "        0.5357412499056584\n",
      "    Degree = 2\n",
      "        0.26837515562810677\n",
      "        0.6137603953993617\n",
      "Lambda = 10.0\n",
      "    Degree = 1\n",
      "        0.4016313108003009\n",
      "        0.535835073442734\n",
      "    Degree = 2\n",
      "        0.28025537342649093\n",
      "        0.6431991791448415\n"
     ]
    }
   ],
   "source": [
    "run_ridge_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the best test error is achieved with a regulizer of $\\lambda = 10^{-5}$ for `degree 1` and $\\lambda = 10^{-2}$ for `degree 2`. In all cases, `degree 1` remains our best degree value.\n",
    "\n",
    "Since the results are almost equal with and without a regulizer and provided that the test error increases for larger values of $\\lambda$, we deduce that the model is not overfitted and that a $\\mathcal{L}_2$ regulizer is useless..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 3: Lasso (MSE loss with $\\mathcal{L}_1$-regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lasso():\n",
    "    for lambda_ in np.concatenate([[0], np.logspace(-5, 2, 8)]):\n",
    "        print(\"lambda = \" + str(lambda_))\n",
    "        for degree in range(1, 3):\n",
    "            print(\"    degree = \" + str(degree))\n",
    "            rmse_tr_tmp = []\n",
    "            rmse_te_tmp = []\n",
    "    \n",
    "            for k in range(k_fold):\n",
    "                lasso_lambda = lambda y, X, w: lasso(y, X, w, lambda_)\n",
    "                lasso_stoch_grad_lambda = lambda y, X, w: lasso_stoch_grad(y, X, w, lambda_)\n",
    "                loss_lambda = lambda y, X: stochastic_gradient_descent(y, X, np.zeros(X.shape[1]), 100, 1e-2, lasso_lambda, lasso_stoch_grad_lambda, batch_size = 100)\n",
    "                loss_tr, loss_te, _ = cross_validation(y, X, k_indices, k, degree, loss_lambda)\n",
    "                rmse_tr_tmp.append(loss_tr)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            print(\"        \" + str(np.mean(rmse_tr_tmp)))\n",
    "            print(\"        \" + str(np.mean(rmse_te_tmp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run it, we try a `stochastic_gradient_descent` using a `lasso`, just to see what we can expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "lasso_lambda = lambda y, X, w: lasso(y, X, w, lambda_)\n",
    "lasso_stoch_grad_lambda = lambda y, X, w: lasso_stoch_grad(y, X, w, lambda_)\n",
    "_, losses = stochastic_gradient_descent(y, X, np.zeros(X.shape[1]), 80, 1e-3, lasso_lambda, lasso_stoch_grad_lambda, batch_size = 100, detail = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"SGD iterations\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmin(losses))\n",
    "print(losses[np.argmin(losses)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the result, we see that convergence is really slow. Indeed, since $\\lambda = 0$, we are basically optimizing a standard MSE for which we know (from above) that the optimal solution is 16. We deduce it is unlikely to do better than `ridge_regression`, and we drop this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 4: MAE loss (with SGD) with $\\mathcal{L}_2$-regularizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mae_sgd():\n",
    "    for lambda_ in [0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]:\n",
    "        print(\"lambda = \" + str(lambda_))\n",
    "        for degree in range(1, 3):\n",
    "            print(\"    degree = \" + str(degree))\n",
    "            rmse_tr_tmp = []\n",
    "            rmse_te_tmp = []\n",
    "            \n",
    "            lambda_mae = lambda y, X, w: mae(y, X, w, lambda_)\n",
    "            lambda_mae_stoch_grad = lambda y, X, w: mae_stoch_grad(y, X, w, lambda_)\n",
    "            for k in range(k_fold):\n",
    "                mae_lambda = lambda y, X: stochastic_gradient_descent(y, X, np.zeros(X.shape[1]), 50, 1, lambda_mae, lambda_mae_stoch_grad, batch_size = 100)\n",
    "                loss_tr, loss_te, _ = cross_validation(y, X, k_indices, k, degree, mae_lambda)\n",
    "                rmse_tr_tmp.append(loss_tr)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            print(\"        \" + str(np.mean(rmse_tr_tmp)))\n",
    "            print(\"        \" + str(np.mean(rmse_te_tmp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we first try to validate the method before we run that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree 1\n",
      "    loss = 26.159135164285743\n",
      "Degree 2\n"
     ]
    }
   ],
   "source": [
    "for i in [1, 2]:\n",
    "    lambda_ = 0\n",
    "    print(\"Degree \" + str(i))\n",
    "    X_ = build_poly(X, i)\n",
    "    lambda_mae = lambda y, X, w: mae(y, X, w, lambda_)\n",
    "    lambda_grad_mae = lambda y, X, w: mae_stoch_grad(y, X, w, lambda_)\n",
    "    ws, losses = stochastic_gradient_descent(y, X_, np.zeros(X_.shape[1]), 50, 1, lambda_mae, lambda_grad_mae, batch_size = 100, detail = True)\n",
    "    plt.xlabel(\"SGD iterations\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.semilogy(losses, label = \"degree =\" + str(i))\n",
    "    plt.legend()\n",
    "    print(\"    loss = \" + str(losses[np.argmin(losses)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss obtained reaches RMSE of approx. 15 for degree 2 polynomial expansion. We think that we probably won't be able to do better than ridge regression, but the effort is worth trying. Degree 3 has been tested separately and gives RMSE > 1000, hence we exclude it from the possible degree candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mae_sgd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we see that the results are not as good as ridge regression..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
