{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSMO Project - Regressions\n",
    "By Mathilde Raynal, Etienne Bonvin and Xavier Pantet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from regressions import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data/\"\n",
    "X = np.load(DATA_FOLDER + \"pca_x_4500.npy\")\n",
    "y = np.load(DATA_FOLDER + \"CSD500-r_train-H_total.npy\")\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (30049, 4501)\n",
      "y: (30049,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \" + str(X.shape))\n",
    "print(\"y: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 1: Good ol' least squares (MSE loss without regularizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try a standard and naive implementation of `least_squares` on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.4170623015791401\n"
     ]
    }
   ],
   "source": [
    "def run_least_squares():\n",
    "    w_star = least_squares(y, X)\n",
    "    loss = rmse(y, X, w_star)\n",
    "    print(\"Loss = \" + str(loss))\n",
    "\n",
    "run_least_squares()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss is quite large! We hope to do better using polynomial expansion using a smaller dataset composed only of a smaller number of features so that we don't need a cluster. We use 4-fold cross-validation to find the best `degree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 4\n",
    "k_indices = build_k_indices(y, k_fold)\n",
    "\n",
    "def run_least_squares_poly():\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "\n",
    "    for degree in range(4):\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            try:\n",
    "                loss_tr, loss_te, _ = cross_validation(y, X, k_indices, k, degree, least_squares)\n",
    "                rmse_tr_tmp.append(loss_tr)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            except:\n",
    "                print(\"Could not inverse the matrix\")\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))\n",
    "    pd.DataFrame([rmse_tr, rmse_te]).add_prefix(\"Degree \").rename({0: \"Train error\", 1: \"Test error\"}).head()\n",
    "    return rmse_tr, rmse_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not inverse the matrix\n",
      "Could not inverse the matrix\n",
      "Could not inverse the matrix\n",
      "Could not inverse the matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not inverse the matrix\n",
      "Could not inverse the matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([3.214363737341961, nan, 1827471.5245389498, 26.359518509500585],\n",
       " [3.214384303727872, nan, 1871489.1269204326, 26.346462575458478])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_least_squares_poly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, polynomial expansion provides better results. Moreover, we see that the best `degree` is 1.\n",
    "\n",
    "__By adding a single feature of constant values, we manage to decrease the RMSE from 26 to 0.7!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 2: Ridge regression (MSE loss with $\\mathcal{L}_2$-regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 4\n",
    "k_indices = build_k_indices(y, k_fold)\n",
    "\n",
    "def run_ridge_regression():\n",
    "    for lambda_ in np.logspace(-5, 1, 7):\n",
    "        print(\"Lambda = \" + str(lambda_))\n",
    "        for degree in range(1, 3):\n",
    "            print(\"    Degree = \" + str(degree))\n",
    "            rmse_tr_tmp = []\n",
    "            rmse_te_tmp = []\n",
    "            for k in range(k_fold):\n",
    "                ridge_lambda = lambda y, X: ridge_regression(y, X, lambda_)\n",
    "                loss_tr, loss_te, _ = cross_validation(y, X, k_indices, k, degree, ridge_lambda)\n",
    "                rmse_tr_tmp.append(loss_tr)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            print(\"        \" + str(np.mean(rmse_tr_tmp)))\n",
    "            print(\"        \" + str(np.mean(rmse_te_tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 1e-05\n",
      "    Degree = 1\n",
      "        0.40200686409147257\n",
      "        0.5377947190508019\n",
      "    Degree = 2\n",
      "        0.265177181844077\n",
      "        0.610148048690785\n",
      "Lambda = 0.0001\n",
      "    Degree = 1\n",
      "        0.4020068640914771\n",
      "        0.5377947176813502\n",
      "    Degree = 2\n",
      "        0.26518073554549637\n",
      "        0.6100546833352817\n",
      "Lambda = 0.001\n",
      "    Degree = 1\n",
      "        0.40200686409194614\n",
      "        0.537794703987376\n",
      "    Degree = 2\n",
      "        0.26519714428236285\n",
      "        0.6099322031597625\n",
      "Lambda = 0.01\n",
      "    Degree = 1\n",
      "        0.4020068641388381\n",
      "        0.5377945671017491\n",
      "    Degree = 2\n",
      "        0.26520419336929624\n",
      "        0.6098626350150099\n",
      "Lambda = 0.1\n",
      "    Degree = 1\n",
      "        0.40200686882801384\n",
      "        0.5377932036573185\n",
      "    Degree = 2\n",
      "        0.2652325536911477\n",
      "        0.6095144804320468\n",
      "Lambda = 1.0\n",
      "    Degree = 1\n",
      "        0.4020073377198474\n",
      "        0.5377801103501418\n",
      "    Degree = 2\n",
      "        0.2662579522744218\n",
      "        0.6095262299174047\n",
      "Lambda = 10.0\n",
      "    Degree = 1\n",
      "        0.4020541987132068\n",
      "        0.5377032411796367\n",
      "    Degree = 2\n",
      "        0.2728815713409477\n",
      "        0.618366696718677\n"
     ]
    }
   ],
   "source": [
    "run_ridge_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the best test error is achieved with a regulizer of $\\lambda = 10^{-5}$ for `degree 1` and $\\lambda = 10^{-2}$ for `degree 2`. In all cases, `degree 1` remains our best degree value.\n",
    "\n",
    "Since the results are almost equal with and without a regulizer and provided that the test error increases for larger values of $\\lambda$, we deduce that the model is not overfitted and that a $\\mathcal{L}_2$ regulizer is useless..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 3: Lasso (MSE loss with $\\mathcal{L}_1$-regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lasso():\n",
    "    for lambda_ in np.concatenate([[0], np.logspace(-5, 2, 8)]):\n",
    "        print(\"lambda = \" + str(lambda_))\n",
    "        for degree in range(1, 3):\n",
    "            print(\"    degree = \" + str(degree))\n",
    "            rmse_tr_tmp = []\n",
    "            rmse_te_tmp = []\n",
    "    \n",
    "            for k in range(k_fold):\n",
    "                lasso_lambda = lambda y, X, w: lasso(y, X, w, lambda_)\n",
    "                lasso_stoch_grad_lambda = lambda y, X, w: lasso_stoch_grad(y, X, w, lambda_)\n",
    "                loss_lambda = lambda y, X: stochastic_gradient_descent(y, X, np.zeros(X.shape[1]), 100, 1e-2, lasso_lambda, lasso_stoch_grad_lambda, batch_size = 100)\n",
    "                loss_tr, loss_te, _ = cross_validation(y, X, k_indices, k, degree, loss_lambda)\n",
    "                rmse_tr_tmp.append(loss_tr)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            print(\"        \" + str(np.mean(rmse_tr_tmp)))\n",
    "            print(\"        \" + str(np.mean(rmse_te_tmp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run it, we try a `stochastic_gradient_descent` using a `lasso`, just to see what we can expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "lasso_lambda = lambda y, X, w: lasso(y, X, w, lambda_)\n",
    "lasso_stoch_grad_lambda = lambda y, X, w: lasso_stoch_grad(y, X, w, lambda_)\n",
    "_, losses = stochastic_gradient_descent(y, X, np.zeros(X.shape[1]), 500, 1e-3, lasso_lambda, lasso_stoch_grad_lambda, batch_size = 100, detail = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"SGD iterations\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmin(losses))\n",
    "print(losses[np.argmin(losses)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the result, we see that convergence is really slow. Indeed, since $\\lambda = 0$, we are basically optimizing a standard MSE for which we know (from above) that the optimal solution is 16. We deduce it is unlikely to do better than `ridge_regression`, and we drop this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 4: MAE loss (with SGD) with $\\mathcal{L}_2$-regularizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mae_sgd():\n",
    "    for lambda_ in [0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]:\n",
    "        print(\"lambda = \" + str(lambda_))\n",
    "        for degree in range(1, 3):\n",
    "            print(\"    degree = \" + str(degree))\n",
    "            rmse_tr_tmp = []\n",
    "            rmse_te_tmp = []\n",
    "            \n",
    "            lambda_mae = lambda y, X, w: mae(y, X, w, lambda_)\n",
    "            lambda_mae_stoch_grad = lambda y, X, w: mae_stoch_grad(y, X, w, lambda_)\n",
    "            for k in range(k_fold):\n",
    "                mae_lambda = lambda y, X: stochastic_gradient_descent(y, X, np.zeros(X.shape[1]), 500, 1, lambda_mae, lambda_mae_stoch_grad, batch_size = 100)\n",
    "                loss_tr, loss_te, _ = cross_validation(y, X, k_indices, k, degree, mae_lambda)\n",
    "                rmse_tr_tmp.append(loss_tr)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            print(\"        \" + str(np.mean(rmse_tr_tmp)))\n",
    "            print(\"        \" + str(np.mean(rmse_te_tmp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we first try to validate the method before we run that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree 1\n",
      "    loss = 26.157477430411543\n",
      "Degree 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3):\n",
    "    lambda_ = 0\n",
    "    print(\"Degree \" + str(i))\n",
    "    X_ = build_poly(X, i)\n",
    "    lambda_mae = lambda y, X, w: mae(y, X, w, lambda_)\n",
    "    lambda_grad_mae = lambda y, X, w: mae_stoch_grad(y, X, w, lambda_)\n",
    "    ws, losses = stochastic_gradient_descent(y, X_, np.zeros(X_.shape[1]), 50, 1, lambda_mae, lambda_grad_mae, batch_size = 100, detail = True)\n",
    "    plt.xlabel(\"SGD iterations\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.semilogy(losses, label = \"degree =\" + str(i))\n",
    "    plt.legend()\n",
    "    print(\"    loss = \" + str(losses[np.argmin(losses)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss obtained reaches RMSE of approx. 15 for degree 2 polynomial expansion. We think that we probably won't be able to do better than ridge regression, but the effort is worth trying. Degree 3 has been tested separately and gives RMSE > 1000, hence we exclude it from the possible degree candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mae_sgd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we see that the results are not as good as ridge regression..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
