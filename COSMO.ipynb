{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSMO Project\n",
    "By Mathilde Raynal, Etienne Bonvin and Xavier Pantet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from regressions import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data/\"\n",
    "#X = np.load(DATA_FOLDER + \"feature_mat_radial_compression.npy\")\n",
    "X = np.load(DATA_FOLDER + \"pca_x.npy\")\n",
    "y = np.load(DATA_FOLDER + \"CSD500-r_train-H_total.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (30049, 3004)\n",
      "y: (30049,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \" + str(X.shape))\n",
    "print(\"y: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 1: Good ol' least squares (MSE loss without regularizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try a standard and naive implementation of `least_squares` on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 26.166553759575127\n"
     ]
    }
   ],
   "source": [
    "w_star = least_squares(y, X)\n",
    "loss = rmse(y, X, w_star)\n",
    "print(\"Loss = \" + str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss is quite large! We hope to do better using polynomial expansion using a smaller dataset composed only of a smaller number of features so that we don't need a cluster. We use 4-fold cross-validation to find the best `degree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k_fold = 4\n",
    "#k_indices = build_k_indices(y, k_fold)\n",
    "\n",
    "#rmse_tr = []\n",
    "#rmse_te = []\n",
    "\n",
    "#for degree in range(4):\n",
    "#    rmse_tr_tmp = []\n",
    "#    rmse_te_tmp = []\n",
    "#    for k in range(k_fold):\n",
    "#        loss_tr, loss_te, _ = cross_validation(y, X, k_indices, k, degree, least_squares)\n",
    "#        rmse_tr_tmp.append(loss_tr)\n",
    "#        rmse_te_tmp.append(loss_te)\n",
    "#    rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "#    rmse_te.append(np.mean(rmse_te_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame([rmse_tr, rmse_te]).add_prefix(\"Degree \").rename({0: \"Train error\", 1: \"Test error\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, polynomial expansion provides better results. Moreover, we see that the best `degree` is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 2: Ridge regression (MSE loss with $\\mathcal{L}_2$-regulizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 4\n",
    "k_indices = build_k_indices(y, k_fold)\n",
    "\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "\n",
    "#for lambda_ in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]:\n",
    "for lambda_ in [1e-13, 1e-12, 1e-11]:\n",
    "    for degree in range(1, 3):\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            ridge_lambda = lambda y, X: ridge_regression(y, X, lambda_)\n",
    "            loss_tr, loss_te, _ = cross_validation(y, X, k_indices, k, degree, ridge_lambda)\n",
    "            rmse_tr_tmp.append(loss_tr)\n",
    "            rmse_te_tmp.append(loss_te)\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7107496454416272,\n",
       " 0.7489895213565038,\n",
       " 0.7107496454416272,\n",
       " 0.7489895213565052,\n",
       " 0.7107496454416271,\n",
       " 0.748989521356491]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_te"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
