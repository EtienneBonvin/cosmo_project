{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSMO Project\n",
    "By Mathilde Raynal, Etienne Bonvin and Xavier Pantet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from regressions import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data/\"\n",
    "#X = np.load(DATA_FOLDER + \"feature_mat_radial_compression.npy\")\n",
    "X = np.load(DATA_FOLDER + \"pca_x.npy\")\n",
    "y = np.load(DATA_FOLDER + \"CSD500-r_train-H_total.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (30049, 3004)\n",
      "y: (30049,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \" + str(X.shape))\n",
    "print(\"y: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 1: Good ol' least squares (MSE loss without regularizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try a standard and naive implementation of `least_squares` on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_least_squares():\n",
    "    w_star = least_squares(y, X)\n",
    "    loss = rmse(y, X, w_star)\n",
    "    print(\"Loss = \" + str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss is quite large! We hope to do better using polynomial expansion using a smaller dataset composed only of a smaller number of features so that we don't need a cluster. We use 4-fold cross-validation to find the best `degree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 4\n",
    "k_indices = build_k_indices(y, k_fold)\n",
    "\n",
    "def run_least_squares_poly():\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "\n",
    "    for degree in range(4):\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te, _ = cross_validation(y, X, k_indices, k, degree, least_squares)\n",
    "            rmse_tr_tmp.append(loss_tr)\n",
    "            rmse_te_tmp.append(loss_te)\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))\n",
    "    pd.DataFrame([rmse_tr, rmse_te]).add_prefix(\"Degree \").rename({0: \"Train error\", 1: \"Test error\"}).head()\n",
    "    return rmse_tr, rmse_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, polynomial expansion provides better results. Moreover, we see that the best `degree` is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 2: Ridge regression (MSE loss with $\\mathcal{L}_2$-regulizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 4\n",
    "k_indices = build_k_indices(y, k_fold)\n",
    "\n",
    "def run_ridge_regression():\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    for lambda_ in [1e-10]:\n",
    "        for degree in range(1, 3):\n",
    "            rmse_tr_tmp = []\n",
    "            rmse_te_tmp = []\n",
    "            for k in range(k_fold):\n",
    "                ridge_lambda = lambda y, X: ridge_regression(y, X, lambda_)\n",
    "                loss_tr, loss_te, _ = cross_validation(y, X, k_indices, k, degree, ridge_lambda)\n",
    "                rmse_tr_tmp.append(loss_tr)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "            rmse_te.append(np.mean(rmse_te_tmp))\n",
    "    return rmse_tr, rmse_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 3: Lasso (MSE loss with $\\mathcal{L}_1$-regulizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lasso():\n",
    "    for lambda_ in [0, 1e-4, 1e-3, 1e-2, 1e-1]:\n",
    "        for degree in range(1, 3):\n",
    "            rmse_tr_tmp = []\n",
    "            rmse_te_tmp = []\n",
    "    \n",
    "            for k in range(k_fold):\n",
    "                lasso_lambda = lambda y, X, w: lasso(y, X, w, lambda_)\n",
    "                lasso_stoch_grad_lambda = lambda y, X, w: lasso_stoch_grad(y, X, w, lambda_)\n",
    "                loss_lambda = lambda y, X: stochastic_gradient_descent(y, X, np.zeros(X.shape[1]), 100, 1e-4, lasso_lambda, lasso_stoch_grad_lambda)\n",
    "                loss_tr, loss_te, w = cross_validation(y, X, k_indices, k, degree, loss_lambda)\n",
    "                rmse_tr_tmp.append(loss_tr)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            print(np.mean(rmse_tr_tmp), np.mean(rmse_te_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "lasso_lambda = lambda y, X, w: lasso(y, X, w, lambda_)\n",
    "lasso_stoch_grad_lambda = lambda y, X, w: lasso_stoch_grad(y, X, w, lambda_)\n",
    "# lasso: gamma = 1e-15\n",
    "ws, losses = stochastic_gradient_descent(y, X, np.zeros(X.shape[1]), 1000, 1e-10, mae, mae_stoch_grad, detail = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "#list(map(lambda w: rmse(y, X, w), ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 4: MAE loss (with SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mae_sgd():\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "\n",
    "    #for lambda_ in [0, 1e-4, 1e-3, 1e-2, 1e-1]:\n",
    "    for _ in [0]:\n",
    "        for degree in range(1, 3):\n",
    "            rmse_tr_tmp = []\n",
    "            rmse_te_tmp = []\n",
    "    \n",
    "            for k in range(k_fold):\n",
    "                mae_lambda = lambda y, X: stochastic_gradient_descent(y, X, np.zeros(X.shape[1]), 100, 1e-4, mae, mae_stoch_grad)\n",
    "                loss_tr, loss_te, _ = cross_validation(y, X, k_indices, k, degree, mae_lambda)\n",
    "                rmse_tr_tmp.append(loss_tr)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "            rmse_te.append(np.mean(rmse_te_tmp))\n",
    "        print(rmse_tr, rmse_te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
